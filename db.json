{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/README.md","path":"README.md","modified":0,"renderable":0},{"_id":"source/robots.txt","path":"robots.txt","modified":0,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/android-chrome-192x192.png","path":"images/android-chrome-192x192.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/browserconfig.xml","path":"images/browserconfig.xml","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16.png","path":"images/favicon-16x16.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32.png","path":"images/favicon-32x32.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/mstile-150x150.png","path":"images/mstile-150x150.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/safari-pinned-tab.svg","path":"images/safari-pinned-tab.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/site.webmanifest","path":"images/site.webmanifest","modified":0,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/wechatpay.jpg","path":"images/wechatpay.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/alipay.jpg","path":"images/alipay.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/android-chrome-512x512.png","path":"images/android-chrome-512x512.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.png","path":"images/avatar.png","modified":0,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/CV_Columbia_University_Jason_Ding.pdf","path":"CV_Columbia_University_Jason_Ding.pdf","modified":1,"renderable":0},{"_id":"source/Resume_Columbia_University_Zhicheng Ding.pdf","path":"Resume_Columbia_University_Zhicheng Ding.pdf","modified":1,"renderable":0}],"Cache":[{"_id":"source/README.md","hash":"2979a6aba846c0a215183c709fe5e5cd20df6810","modified":1523734926480},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1523737798764},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1523737798764},{"_id":"source/robots.txt","hash":"65968de5ef3fae4f6947d369c1f819dc628ec038","modified":1523734926511},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1523737798766},{"_id":"themes/next/.gitignore","hash":"0378adb9c2dc4855b3198184df4863cb30e4059c","modified":1523737798774},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1523737798776},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1523737798779},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1523737798777},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1523737798780},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1523737798781},{"_id":"themes/next/LICENSE.md","hash":"f0190c7d83a98464549a6b3a51bb206148d88e1b","modified":1523737798782},{"_id":"themes/next/README.md","hash":"4803a2fc3101612ecad7f59182427350700ddbc6","modified":1523737798783},{"_id":"themes/next/bower.json","hash":"d0bd2495c886e1f12f9e6913d8220f92c0791688","modified":1523737798786},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1523737798787},{"_id":"themes/next/_config.yml","hash":"2cfed3c77f3c870cb15095f25a22c43d1e086126","modified":1523745255591},{"_id":"themes/next/gulpfile.coffee","hash":"67eaf2515100971f6195b60eeebbfe5e8de895ab","modified":1523737798817},{"_id":"themes/next/package.json","hash":"9970079cd151a10b45717d701f9adb0d7e5225d5","modified":1523737799000},{"_id":"source/_posts/2017-09-15-Why Keep Writing Blog.md","hash":"bd6febd890dfa5590c0201924bb619b5523dd0ab","modified":1523746995594},{"_id":"source/about/index.md","hash":"bc01e46dbcb7db65e786685c5b0e8285730d3406","modified":1523734926510},{"_id":"source/categories/index.md","hash":"4bdc80d2f7520d76ac1e48d8f71bb16b14d82574","modified":1523734926510},{"_id":"source/projects/index.md","hash":"314b693c55b6e05694d5ddeecb1362a3287d0d67","modified":1523746842459},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1523737798731},{"_id":"themes/next/.git/config","hash":"e65784464d551b062da55cd14a9e44eda811e748","modified":1523737798747},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1523737794300},{"_id":"themes/next/.git/index","hash":"490b69de3600a190cff578f3c8b9385e1a08eab7","modified":1523737800273},{"_id":"themes/next/.git/packed-refs","hash":"da647cc6415fd60706a20c0e8b11f87e3b69ef50","modified":1523737798717},{"_id":"source/tags/index.md","hash":"9b7445e9555af7df32ac89873bf0bcabc3d0d8e5","modified":1523734926512},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"22f2ccc5522563b67c8663849fc1d6cbae93a8ff","modified":1523737798768},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"869dcd36524e2c61ddd2315c1266edca7f0da6c9","modified":1523737798769},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"1e212fe229bd659726b4a3bcf4b5b14e0310ba3a","modified":1523737798770},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"36201119490a04107c8179b10202548a9d0e5e60","modified":1523737798771},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1523737798773},{"_id":"themes/next/.github/stale.yml","hash":"dbd5e6bf89b76ad1f2b081578b239c7ae32755af","modified":1523737798774},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"1dada3c3404445a00367882b8f97cdf092b7943d","modified":1523737798790},{"_id":"themes/next/docs/DATA-FILES.md","hash":"9a1895c0a0db705c4c48f512e86917f9af1ec3fb","modified":1523737798793},{"_id":"themes/next/docs/AUTHORS.md","hash":"51a0a13da55ff3d596970b2f9ab4531c6b2211f2","modified":1523737798792},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1523737798789},{"_id":"themes/next/docs/INSTALLATION.md","hash":"b74ef6fedf76cdb156e2265759ee0a789ddd49cc","modified":1523737798794},{"_id":"themes/next/docs/LICENSE","hash":"5b702310012d480b40529fd10cf1872f687277a0","modified":1523737798797},{"_id":"themes/next/docs/MATH.md","hash":"bdbbcaf88f86de3d762fd1459fd5a0893cf4bce6","modified":1523737798798},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"d0bc29c02b61e4433108987412685d991afd1a95","modified":1523737798795},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"c9f2ed8e15c137b1885d9ca8b7197d9f457971e9","modified":1523737798799},{"_id":"themes/next/layout/_layout.swig","hash":"ea8f422bf6f28718ee38842df2a2f2251fc626ad","modified":1523737798900},{"_id":"themes/next/layout/archive.swig","hash":"678a6cab739b54da9dfb2d6634848cffdc7aab6a","modified":1523737798992},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1523737798993},{"_id":"themes/next/layout/page.swig","hash":"fbf2c3d14c3e5730fad75d7d106bf7f070ccd69c","modified":1523737798995},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1523737798994},{"_id":"themes/next/layout/projects.ejs","hash":"ee0ce22327cc85802933788f212824f3bcdef219","modified":1523745179154},{"_id":"themes/next/layout/post.swig","hash":"ceba7287574c429a235aa7a62bf7a3dc1efb265a","modified":1523737798996},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1523737798997},{"_id":"themes/next/languages/ar.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798819},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1523737798999},{"_id":"themes/next/languages/bn.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798821},{"_id":"themes/next/languages/bg.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798820},{"_id":"themes/next/languages/cs.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798822},{"_id":"themes/next/languages/da.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798823},{"_id":"themes/next/languages/el.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798827},{"_id":"themes/next/languages/de.yml","hash":"bfd74445c7f17fdc859551d648b6a03e2eed2561","modified":1523737798824},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1523737798826},{"_id":"themes/next/languages/en.yml","hash":"7601a793fc87d275e733d0cb85d2ab79a965e2ed","modified":1523747770475},{"_id":"themes/next/languages/es.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798829},{"_id":"themes/next/languages/fa.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798831},{"_id":"themes/next/languages/et.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798830},{"_id":"themes/next/languages/fi.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798832},{"_id":"themes/next/languages/fr.yml","hash":"1bca7176ff846db5b063f4a90a653eed377b6909","modified":1523737798833},{"_id":"themes/next/languages/he.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798834},{"_id":"themes/next/languages/hi.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798835},{"_id":"themes/next/languages/hr.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798863},{"_id":"themes/next/languages/hu.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798865},{"_id":"themes/next/languages/id.yml","hash":"4cda8f32dacebda3479e3a0c9d688050388897f2","modified":1523737798865},{"_id":"themes/next/languages/it.yml","hash":"6199268268f4b460b5d193c9a5a30622fc97fcdc","modified":1523737798867},{"_id":"themes/next/languages/ja.yml","hash":"36732b432ac39cff022bca5700b6fef1330be0d3","modified":1523737798868},{"_id":"themes/next/languages/jv.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798869},{"_id":"themes/next/languages/ko.yml","hash":"0753fbef09df65937586760996535bf00197c548","modified":1523737798870},{"_id":"themes/next/languages/lt.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798871},{"_id":"themes/next/languages/lv.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798872},{"_id":"themes/next/languages/ms.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798873},{"_id":"themes/next/languages/nl.yml","hash":"2692731e1e9887b390bc6241f38a234bdb621947","modified":1523737798874},{"_id":"themes/next/languages/no.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798875},{"_id":"themes/next/languages/pa.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798877},{"_id":"themes/next/languages/pl.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798878},{"_id":"themes/next/languages/pt-BR.yml","hash":"3542d4ecafb6f91427128df8b75b1824e0255405","modified":1523737798879},{"_id":"themes/next/languages/pt.yml","hash":"681cde6ddbc998d6a36ffe857aaf6f0252ecfb29","modified":1523737798880},{"_id":"themes/next/languages/ru.yml","hash":"a4811c66a84b5aae2c414aac5e43b359cac2d4d6","modified":1523737798882},{"_id":"themes/next/languages/ro.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798881},{"_id":"themes/next/languages/sl.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798883},{"_id":"themes/next/languages/sr.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798884},{"_id":"themes/next/languages/sv.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798885},{"_id":"themes/next/languages/th.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798886},{"_id":"themes/next/languages/tr.yml","hash":"2545ff1d60b51047f33d7080ea983a9731bf59bb","modified":1523737798888},{"_id":"themes/next/languages/uk.yml","hash":"1409bf8d19c9d094f37add2435eca054a6e82b44","modified":1523737798889},{"_id":"themes/next/languages/vi.yml","hash":"b9295a00301ce5db00e2aeaa6c5d90ff0295efcc","modified":1523737798891},{"_id":"themes/next/languages/zh-CN.yml","hash":"4bb91d89cd876b5f27d77c319d6ee6a550f7c721","modified":1523737798892},{"_id":"themes/next/languages/zh-TW.yml","hash":"89caf0c4f51d807ec5082692418de74d8d080c30","modified":1523737798895},{"_id":"themes/next/languages/zh-HK.yml","hash":"279fa94d8ef33818c3f537e718131ea214dafdf1","modified":1523737798894},{"_id":"themes/next/scripts/helpers.js","hash":"85811e77311b7b8255ef2c124a59fa4e6b6ac819","modified":1523737799002},{"_id":"themes/next/scripts/merge-configs.js","hash":"176952dfe3605c2ab57f3f7cdbac4f2487825c41","modified":1523737799003},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1523737800266},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1523737799004},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1523737800268},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1523737800270},{"_id":"source/_posts/2018-02-26-Resume-Zhicheng Ding.pdf","hash":"46514ecb84f1d621614b51c9464093c8fd943a81","modified":1519657848274},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523737799336},{"_id":"source/_posts/Latex/2017-12-05-Latex常见错误整理.md","hash":"5258af2332034b0d30b923ed73b983d1731e9cb8","modified":1523734926485},{"_id":"source/_posts/ML System Tips/2017-11-30-Get More Data.md","hash":"caff39a0610302375cc9507ebab58a17baff5a54","modified":1523734926486},{"_id":"source/_posts/Mathematics/2017-10-29-tanh Function.md","hash":"27352c79ec8b0fc49561545c6984291e01cf3a36","modified":1523734926487},{"_id":"source/_posts/Mathematics/2017-10-28-MatrixToVector.md","hash":"c38f9d648d79210345229d92bf6033428e13bde5","modified":1523734926487},{"_id":"source/_posts/Mathematics/2017-10-29-数据预处理-数据归一化和数据规范化.md","hash":"2ac2c80f6fd488b0f6fe2810ead43c584fe86a39","modified":1523734926488},{"_id":"source/_posts/Mathematics/2017-10-30-Sigmoid Function.md","hash":"71bdafe1b83c639e6cc6276802afba70ab8a9d67","modified":1523734926488},{"_id":"source/_posts/Mathematics/2017-10-31-数据分割.md","hash":"eb410a07127cf16971ee488246c305e87caa7e51","modified":1523734926489},{"_id":"source/_posts/Mathematics/2017-11-01-线性模型性能分析--混淆矩阵(Confusion Matrix).md","hash":"05d540b123792a4bc2b6ae8d2e17eefd9479fa65","modified":1523734926489},{"_id":"source/_posts/Mathematics/2017-11-30-Concave and Convex Function.md","hash":"50564dc2d937c2831b05c2454a07f9d7549d684d","modified":1523734926490},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-01-0.Machine Learning Foundation - Table of Contents.md","hash":"b6c3d96af66ad4205d11c43e2c606bb5f3ea4f7a","modified":1523734926501},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-02-1.When can Machine Learn - The Learning Problem.md","hash":"87390fd4565d7649affda2f59795926f20d78440","modified":1523734926501},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-03-2.When can Machine Learn - Learning to Answer Yes or No.md","hash":"b46441a031b4106666ed118a594e22690e1f6382","modified":1523734926502},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-06-4.When can Machine Learn - Feasible of Learning.md","hash":"c78446288feb2574377024b338cfe095f81dd7fc","modified":1523734926503},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-08-5.Why Can Machine Learn.md","hash":"c6875914e5e0d5bff95f5fb3d8f0c4abe1ad5bba","modified":1523734926504},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-04-3.When can Machine Learn - Types of Learning.md","hash":"f295f528127420cfb5f19022916863a2b70a7fce","modified":1523734926503},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-11-7.How can Machine Learn - Linear Regression.md","hash":"b6d73698e36304aab7800c56b2617fbd2b3842f8","modified":1523734926505},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-09-6.How can Machine Learn - Noice and Error.md","hash":"524173380c6dac511af63a8b425fac9d80a29ed5","modified":1523734926504},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-13-8.How can Machine Learn - Logistic Regression.md","hash":"6428aaa9539aa9b5164ac3a2923fd8d0c9a5f6ab","modified":1523734926505},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-14-9.How can Machine Learn - Linear Model for Classification.md","hash":"5190971796b979b05a4a7f417327a359c1ba2ac0","modified":1523734926506},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-15-10.How can Machine Learn - Nonlinear Transformation.md","hash":"6b4ac3fe30a8749b79b34c7b376e01b3a04dc43d","modified":1523734926506},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-18-13.How can Machine Learn Better - Validation.md","hash":"5275867da61dd28bd9ac904cbf34096ad0954b95","modified":1523734926508},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-16-11.How can Machine Learn Better - Overfitting and Solution.md","hash":"f69846cf0a38aeeb6c2a024bce6ab9a7982cc7d8","modified":1523734926507},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-17-12.How can Machine Learn Better - Regularization.md","hash":"d4d018fc3e50c43989daf1aa07ef4bf5bf365759","modified":1523734926508},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-21-14.How can Machine Learn Better - Three Learning Principles.md","hash":"2a371781e639a5e7fd2d4c098dd44cdbf6537635","modified":1523734926509},{"_id":"source/_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-22-15.Summary - Power of Three.md","hash":"91586d47407a7d4212611485bb2d9e78c6546cd2","modified":1523734926509},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1523737794302},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1523737794303},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1523737794306},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1523737794305},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1523737794308},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1523737794309},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1523737794312},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1523737794311},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1523737794314},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1523737794315},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1523737794318},{"_id":"themes/next/.git/logs/HEAD","hash":"de1c886fba9dda561e3fb1cb3ab0f38c462c9887","modified":1523737798737},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"a51de08657f5946f4028b11373280ddc04639525","modified":1523737798801},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"7b2963daac19b0c14f98ebef375d5fbce8fc3f44","modified":1523737798802},{"_id":"themes/next/docs/ru/README.md","hash":"f80fc5b6678a104de086856a25dcfded73f07bc6","modified":1523737798803},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"1a4e41adcf5831057f3f7b3025ed4a5ef7c442b4","modified":1523737798804},{"_id":"themes/next/layout/_custom/head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1523737798897},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1523737798898},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1523737798899},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"9b512cb820627fcc45c9f77c7a122aa99e021bd5","modified":1523737798807},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"e771c5b745608c6fb5ae2fa1c06c61b3699627ec","modified":1523737798806},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"f2a2d6e68c5f65e27303b40f5285a8a4a5adae4c","modified":1523737798808},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"67f4a987e7db0ab1ce1ea4c311f2961df07b6681","modified":1523737798809},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"baca12cc24be082f1db28c7f283493569666321c","modified":1523737798810},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"684dffabee3132055df10f66ff65c482a9b57a66","modified":1523737798813},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"1ec7a0e9ac64b2c84aa1fd7ba2c662be1dbcb407","modified":1523737798816},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"08c630dc7bd479e49d7d5aa813df628e7726bd7f","modified":1523737798814},{"_id":"themes/next/docs/zh-CN/README.md","hash":"15a61c2eb19d3eceac479db415ca4a30587456c7","modified":1523737798815},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"949bf640c011689e518ad480299ab19785df4b80","modified":1523737798906},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1523737798904},{"_id":"themes/next/layout/_macro/post-related.swig","hash":"e8dfb86eb62b9c2bc1435d6d1afa95d3b4c7b931","modified":1523737798907},{"_id":"themes/next/layout/_macro/post.swig","hash":"49d2858363596b85ead4556dcfe9a9bf392d8431","modified":1523737798908},{"_id":"themes/next/layout/_partials/breadcrumb.swig","hash":"4b48fdbfe3bf41ddeda4ff74c1ff17ab9f15c14e","modified":1523737798913},{"_id":"themes/next/layout/_macro/reward.swig","hash":"8dc3b6ba76c389aaa0810cbd0df206f7096d76a0","modified":1523737798909},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"2d8e3b4a211d55f4861eb2375b6fdd786484bf71","modified":1523737798910},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"616ffee93e84958e72ee0f996808f866c4ccdbf7","modified":1523737798912},{"_id":"themes/next/layout/_partials/comments.swig","hash":"2e8c3df265e9ae38003d1321fb80fb3b12cb6a97","modified":1523737798914},{"_id":"themes/next/layout/_partials/footer.swig","hash":"96feda6cf096297a17f07ee81254cc41768d2184","modified":1523737798915},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"206cbd6ac9ca6a219a8516f59beae25b3c770199","modified":1523737798925},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"914155d5d758306cff405beefd4a07973fd8fc77","modified":1523737798925},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"ed62ea83d3f2c9db2ea57bf23a7d765ed82504c2","modified":1523737798962},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1523737798972},{"_id":"themes/next/layout/_third-party/github-banner.swig","hash":"736cb278fa09d3b4ed6f305b56353941ea918793","modified":1523737798974},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"debba9b7110f635204a15df148194d4c2fd2668b","modified":1523737798979},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1523737798981},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"ccf0035086e14dcefa24c2907301edf4c37d5448","modified":1523737798980},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1523737798982},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1523737798983},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1523737798937},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1523737798938},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"f8c7e729ad6e72b4c705a2c5d5041589c2b4cc52","modified":1523737798939},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"f134aeb8d5bee351e5277edb92ac694af314b75f","modified":1523737798946},{"_id":"themes/next/scripts/tags/button.js","hash":"496e3846f353d253dad944b2ed8fdc4e02dcc28a","modified":1523737799006},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"5c87817385986748617fa87dee9cba654566adcd","modified":1523737799007},{"_id":"themes/next/scripts/tags/exturl.js","hash":"4f928ef9ad0ae8e3d766352b8861bc22f96de5d8","modified":1523737799008},{"_id":"themes/next/scripts/tags/full-image.js","hash":"2118895d2d5e379240b27399a6c988a74f25a427","modified":1523737799009},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"696157d4e4382fdbe29c674b0f4b433d47db449b","modified":1523737799011},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"1f7bafba9cf3c94f5735873bd52e93ddcf37f8a2","modified":1523737799012},{"_id":"themes/next/scripts/tags/label.js","hash":"d8bfb2974c6afef2c085642857b70b6569b4a097","modified":1523737799013},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"92c0ead5f503cfc4ee7c254ebfb00a2623fa1890","modified":1523737799014},{"_id":"themes/next/scripts/tags/note.js","hash":"1cec218c9fcaa8fb4144bf9d0b39babcaafac8d4","modified":1523737799015},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1523737799017},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1523734926709},{"_id":"themes/next/source/images/android-chrome-192x192.png","hash":"1c4e77121a942fce2a122f56ad4c8120444f681b","modified":1523762130000},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1523737799339},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1523734926712},{"_id":"themes/next/source/images/apple-touch-icon.png","hash":"457f72c374756faeb4b375d4413b2c4eeb921093","modified":1523762130000},{"_id":"themes/next/source/images/browserconfig.xml","hash":"974aea18bda5a95802c06b80126ab1d96d91d708","modified":1523762130000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1523734926716},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1523734926757},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1523734926760},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1523734926762},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1523734926763},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1523734926772},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1523734926774},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1523737799351},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1523737799352},{"_id":"themes/next/source/images/favicon-16x16.png","hash":"729e9550a1465cbeffdcbea5b70bb27883da981d","modified":1523762130000},{"_id":"themes/next/source/images/favicon-32x32.png","hash":"b398dd3db0a29507602e6945c4e9606d188d523e","modified":1523762130000},{"_id":"themes/next/source/images/favicon.ico","hash":"9c756e07b09d87bd03c30dc4315ac7818fd6c400","modified":1523762132000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1523734926787},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1523734926789},{"_id":"themes/next/source/images/mstile-150x150.png","hash":"3b50bc2c1c01c5dc42e4da0ecec5129201612581","modified":1523762132000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1523734926791},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1523734926799},{"_id":"themes/next/source/images/safari-pinned-tab.svg","hash":"e2ca3df89ab832d15933d3ea9b71226f662e168d","modified":1523762132000},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1523734926801},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1523734926804},{"_id":"themes/next/source/images/site.webmanifest","hash":"13e5aa58eb2182d8ace63266856c8cc29c47c083","modified":1523762132000},{"_id":"themes/next/source/css/main.styl","hash":"2a62e2a11e9cdcc69e538d856d6f9ce228a07c93","modified":1523737799335},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523737798944},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523737798943},{"_id":"themes/next/source/images/wechatpay.jpg","hash":"652b5c1055cfa52ae6005cba00c751b1ec55b8f5","modified":1523734926815},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523737799290},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523737799291},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523737799294},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523737799330},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523737799334},{"_id":"source/_posts/Algorithm/Optimize Algorithm/2017-12-12-Gradient Descent.md","hash":"a0e73fdbdaf82f78c339d52b260250250fae08ec","modified":1523734926483},{"_id":"source/_posts/Algorithm/Optimize Algorithm/2017-12-11-Stochastic Gradient Descent.md","hash":"55259a4046886c533d9c2536aff3b6b650b168b9","modified":1523734926483},{"_id":"source/_posts/Algorithm/Optimize Algorithm/2017-11-30-Batch Gradient Descent.md","hash":"efe391d4343dc10ceb5e8f53a37de3f1d8cca4c2","modified":1523734926483},{"_id":"source/_posts/Algorithm/Optimize Algorithm/2017-12-12-Maximum Likelihood Estimation.md","hash":"22efb9c1bfd1fafb7d66897a1e601f2a577e357a","modified":1523734926484},{"_id":"source/_posts/Algorithm/Optimize Algorithm/2017-12-12-Mini-Batch Gradient Descent.md","hash":"77804ef0c28b847f7fe473b39e8089c75082d052","modified":1523734926484},{"_id":"source/_posts/Mathematics/Calculus/2017-11-02-Differentiation Rules.md","hash":"9d3945ec716526da3ce64bd8550d10e642369a52","modified":1523734926491},{"_id":"source/_posts/Others/Hexos/2017-11-23-Hexo Next Theme Beautification.md","hash":"353111153e47292c7cdab947a04a07f7d288c4eb","modified":1523734926499},{"_id":"source/_posts/Others/Hexos/2017-11-22-Deploy Blog With Hexo And Github Page.md","hash":"2130082b17719dec882cbb2eab03e06fd9aa76eb","modified":1523734926498},{"_id":"source/_posts/Mathematics/Calculus/2017-11-02-What is a one-sided limit.md","hash":"17b855503fb791a49f45074d90fae5b303ad5e57","modified":1523734926493},{"_id":"source/_posts/Mathematics/Calculus/2017-11-02-Function and limit.md","hash":"81d76b0be19031e3bf7900f724a7f7f7280c791f","modified":1523734926492},{"_id":"source/_posts/Mathematics/Calculus/2017-11-03-Practical Derivatives.md","hash":"f2de84acc13012917e2e7bf92e7eebdf9eba3495","modified":1523734926493},{"_id":"source/_posts/Mathematics/Calculus/2017-11-04-Typlor Expansion Example.md","hash":"9d76dca2aeb0c6cff7504d676cfd7272e9029781","modified":1523734926494},{"_id":"source/_posts/Others/Markdown/2017-11-30-How to use MathJax in Markdown.md","hash":"6f253c91630d2d25ed9d6cf11b4d42e8486aa315","modified":1523734926500},{"_id":"source/_posts/Others/Markdown/2017-12-03-How to Render the Hyperlink with Braces.md","hash":"5441c5bd5bd5e11f86b1a4f8f18abb36a87086d2","modified":1523734926500},{"_id":"themes/next/.git/refs/heads/master","hash":"6eb674c4fe05fcb1a66e819006ac6de9eea88708","modified":1523737798736},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"4eb8e222dc337211efb0d3bbdb5e29af3e6ecdb8","modified":1523737798903},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"aab518204d3125e948796a9ba6b56b09cade2d92","modified":1523737798904},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1523737798917},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"82e4a5e469d6f39b3e503ed879e4f32db31ec4f1","modified":1523737798921},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"2048f9a47bfcc14b689d776028626f46d2865732","modified":1523737798918},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"60555b19a3b4bf203baa9bc0ea6267674f95235c","modified":1523737798919},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"83f6256070bad5f56f9298b046b00d4bfc562ffd","modified":1523737798922},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"f3ae3168801304af3d80ec3b84264e1d4201cb89","modified":1523737798923},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"3f11ae8e9084f39628cd2006931d39a2069b9dd6","modified":1523737798924},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1523737798948},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1523737798950},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1523737798951},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"c43f41b7f3c0743d1e673af2e787ea34a67fc9cb","modified":1523737798952},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1523737798954},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1523737798953},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"d67d9a176a276cdab6f2bcb7eb3650fbca5459c4","modified":1523737798955},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"cfc932c5db04fef64cc56d3ba0b8ddf3a15a63bd","modified":1523737798956},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"7973b4741863362cd9d023216de4a43eb23dc227","modified":1523737798958},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1523737798957},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1523737798959},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1523737798960},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1523737798961},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"f6454c452b2e90a8c760321bce7e3dc6119b71fa","modified":1523737798928},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1523737798929},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1523737798930},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1523737798932},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1523737798931},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1523737798933},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1523737798934},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1523737798935},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1523737798964},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"292cdd1059b76d3d10486b71c99d9afb3e59ea44","modified":1523737798966},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"a693a2c00497471d06113443f548b676ee2e527d","modified":1523737798964},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"787d482d797c38e77d69629ebb88d45f972216b7","modified":1523737798967},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"18a58db53f1eef50462ee0065d4445b7fbb04fb5","modified":1523737798968},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"7cf0c589d95a2d1848266ffe2357e91c95d2e7f8","modified":1523737798969},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"de1fac9bb3f0cab88b1ed1950f76c7eb0037aeda","modified":1523737798970},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"ab181c036cda9656f87bb1a749e3e7cbc437f012","modified":1523737798971},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"30e9e55d9af2ced6e6b156a042026a8b480f0ab0","modified":1523737798975},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"860de4ce6fccc516d2f779a4b600a4214d8c18e2","modified":1523737798976},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"9a865d013b6d273bbbfc3999e8657844b0c48752","modified":1523737798977},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1523737798987},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"71c897f9b107dd0de1b7f649633cf583c206a9db","modified":1523737798988},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1523737798989},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1523737798991},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1523737798941},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1523737798942},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1523737798945},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1523737799287},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1523737799289},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"8aa98ae349908736ba43196c42498fd5bdeb780a","modified":1523737799292},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"4d6dd32dae6f0ea93d43c5cfa0106b6cb46c3b00","modified":1523737799293},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"8b5fc9e6621a5de04ae7d9ef117683c69965fb00","modified":1523737799328},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1523737799329},{"_id":"themes/next/source/css/_variables/base.styl","hash":"534b9dec93f3e235c217b97314d4bf513fa3e5ba","modified":1523737799333},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"b89e68789e6dac24e00250528f4fc9ae8aa3599b","modified":1523737799331},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1523737799362},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1523737799363},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1523737799365},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"8e9ed6392620ba68726c0ade4868100d831daba8","modified":1523737799364},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1523737799366},{"_id":"themes/next/source/js/src/motion.js","hash":"bea49fc6392bd38fb6f80d041d95d23051423aa2","modified":1523737799367},{"_id":"themes/next/source/js/src/post-details.js","hash":"02acfa1862d5e4345d6a7cee08841cc9e2fb0259","modified":1523737799369},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1523737799372},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1523737799373},{"_id":"themes/next/source/js/src/utils.js","hash":"d8d288bc41af85fc72882239529131cbd6a13f9d","modified":1523737799374},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1523737799380},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1523737799377},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1523737799379},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1523737799378},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1523737799381},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1523737800258},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1523737800260},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1523737800263},{"_id":"themes/next/.git/objects/pack/pack-4d2a9a2684fb8c6aa7d66216def528c9aaac6309.idx","hash":"e90e408756ca8e6ce84228fb6fa606ecb7d7cfe9","modified":1523737798171},{"_id":"themes/next/source/images/alipay.jpg","hash":"e34cd1e8fa17b157c0391bb897f3c10d80cd08f7","modified":1523734926711},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1523737800243},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"de1c886fba9dda561e3fb1cb3ab0f38c462c9887","modified":1523737798740},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1523737798728},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1523737798985},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1523737798986},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1523737799021},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1523737799022},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1523737799023},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1523737799028},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1523737799024},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1523737799195},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1523737799252},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1523737799278},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"97bb39756e85f5b27bba7f43270105ad01d736c9","modified":1523737799280},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1523737799281},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1523737799283},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1523737799284},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"52bc8ba71b91d954530b35dfc63b402a02b1321d","modified":1523737799286},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1523737799285},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"c08eb379718fa5af267fd8c5fe9b926003c0c7c7","modified":1523737799297},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1523737799301},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1523737799300},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1523737799303},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"d0e9065b0dbbc01811259f0597d1790268b4881b","modified":1523737799298},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"f3aa863adf972569b72f2df6bc6a914e7daace99","modified":1523737799302},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"2ccb9bdc309b7c1ef183a3dbb0a4621bec54a328","modified":1523737799305},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1523737799311},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1523737799310},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"b26f8a3394d8357a5bfd24d9f8bf62d7b4063ebb","modified":1523737799315},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"3cc7646583218d16925ced7b70865e63a901d4a7","modified":1523737799313},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1523737799304},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1523737799314},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1523737799318},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"ab7eb1d66e300264a225feb6f8bed55683c96cdf","modified":1523737799319},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"cc961108b12ab97d9216606ceb1cd1cd31ab20f0","modified":1523737799320},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1523737799321},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"df16dc995eb9ad498df2edcfc3e20528fc9aa133","modified":1523737799324},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"be72740313a9a0477b8a22f62e4c8ffa6d23a2e5","modified":1523737799325},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"a558803ca81cceae2bdc22c18ef638fcc023681b","modified":1523737799326},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1523737799371},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1523737800203},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1523737800228},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1523737800230},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1523737800248},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1523737800250},{"_id":"source/_posts/Mathematics/Calculus/images/1. one-sided limits.jpg","hash":"1598fbf08396e45a5b5ded0d5806fd89738f61f5","modified":1523734926497},{"_id":"themes/next/source/images/android-chrome-512x512.png","hash":"668da720cbcb765a3baf9a026abc8b0a65d4d0cd","modified":1523762130000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1523737800238},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1523737800241},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1523737800255},{"_id":"source/_posts/Mathematics/Calculus/images/1. one-sided limit graph.jpg","hash":"e9b6f7c3011b7a26c6f53e19d35fd3b77057ef99","modified":1523734926496},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"de1c886fba9dda561e3fb1cb3ab0f38c462c9887","modified":1523737798727},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7dd247c8869fdefb5a007045d00f3ef8ceecf300","modified":1523737799121},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"3f3d2a43d1a326bad25b633c8ec9ddd87867224c","modified":1523737799128},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1523737799137},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"34f5ac3c1ed2dd31e9297cc4c0733e71bc2e252f","modified":1523737799132},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1523737799141},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1523737799145},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1523737799149},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1523737799172},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1523737799156},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"835c1340571bd6c4ec263c482cf13283fb047e49","modified":1523737799160},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1523737799165},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"630be616447a982413030e561bbd3a80ac14b120","modified":1523737799176},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"ad4cae23c8e383f4fabc9a2a95bca6055020d22e","modified":1523737799184},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1523737799180},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1523737799189},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1523737799192},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1523737799199},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1523737799204},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1523737799207},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1523737799202},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"317c9ceda655e9dc373ce8e7b71d20b794fce9a4","modified":1523737799209},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1523737799212},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"eed81f136f9e52032e2fa901df4fb82a0f7f5872","modified":1523737799215},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"82bc7fa5d38d98e98cc25f9a73189024fda25e63","modified":1523737799220},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1523737799218},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1523737799223},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1523737799224},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1523737799225},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"fa57ec9a6f1943c0558856dfba2d6b8faca0cd4d","modified":1523737799235},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"adfd6d2d3b34adc4b476a0ea91e19020456a3b1a","modified":1523737799227},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1523737799229},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1523737799230},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"1faeb3fca899df9d033e4e6ad9a4867cdce7ef9d","modified":1523737799232},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1523737799238},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1523737799240},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1523737799241},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1523737799245},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1523737799243},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"9e05a2232faabb41bcebb51d545d897a76f077da","modified":1523737799246},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"94d76e6da600a36d80e2470326ebb6b3be447ccb","modified":1523737799249},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1523737799254},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1523737799248},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1523737799251},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1523737799255},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"d79c051abb8ab7b5ee5da29a28587704fd5108f5","modified":1523737799258},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1523737799259},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1523737799260},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1523737799261},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1523737799263},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"f4d8144c22544bdb89787c14ab9d39578dae4b7c","modified":1523737799267},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1523737799264},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1523737799265},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1523737799268},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1523737799269},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1523737799270},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1523737799271},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1523737799272},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1523737799273},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1523737799274},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"0b9c4140d7bc526553553552c3ed92da7c81e0b6","modified":1523737799277},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"acfdd76b1c90d2e384affb3d0006a39b524609d2","modified":1523737799276},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1523737799307},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1523737799309},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1523737799316},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1523737800236},{"_id":"themes/next/source/images/avatar.png","hash":"29cd9d85ea4ba7e7eb1be1328b3f708b3b619624","modified":1523734926715},{"_id":"themes/next/.git/objects/pack/pack-4d2a9a2684fb8c6aa7d66216def528c9aaac6309.pack","hash":"6df7df61ab59b5fc832120be02e5f5ea8e4b6a19","modified":1523737798315},{"_id":"public/atom.xml","hash":"b954980b3331829a1f48347539f184718f4f12b4","modified":1523747921123},{"_id":"public/sitemap.xml","hash":"2146a988167c7a7939fb60cb96e46efa4015bdbf","modified":1523747317002},{"_id":"public/search.xml","hash":"373d3a4ca99f9c935e1dedcd809b0ad1d2f9cfc6","modified":1524630276906},{"_id":"public/categories/index.html","hash":"4fcfff9c7c934a7cfe0068d633f1eaaa5ffedefd","modified":1523747812180},{"_id":"public/tags/index.html","hash":"ed3028bc5e51b04fb638ee609c06060342f1ee9f","modified":1523747812180},{"_id":"public/about/index.html","hash":"8302ba8737dbe3aa5d3b0f731c41c1287f7c7994","modified":1523747812180},{"_id":"public/projects/index.html","hash":"e2f17640e03d95f99db14e0cc2dcd1ddc3477506","modified":1523747812180},{"_id":"public/2017/12/12/Algorithm/Optimize Algorithm/2017-12-12-Maximum Likelihood Estimation/index.html","hash":"bf7b6e0d8f91b19f6a33cc88ca2b9151d3e3ede8","modified":1523747812180},{"_id":"public/2017/12/12/Algorithm/Optimize Algorithm/2017-12-12-Gradient Descent/index.html","hash":"4d177d7785ca1faf202d322f18b3722c302f9b43","modified":1523747812180},{"_id":"public/2017/12/12/Algorithm/Optimize Algorithm/2017-12-12-Mini-Batch Gradient Descent/index.html","hash":"53f6837c690b3b1b401a3b08a162981963bbbe9d","modified":1523747812181},{"_id":"public/2017/12/11/Algorithm/Optimize Algorithm/2017-12-11-Stochastic Gradient Descent/index.html","hash":"0acfa431748bb11602fcaf7bc97718670f1c613d","modified":1523747812181},{"_id":"public/2017/12/04/Latex/2017-12-05-Latex常见错误整理/index.html","hash":"23292fc6e2fe4d90f83a9b6e62d2b80f219dd999","modified":1523747812181},{"_id":"public/2017/12/02/Others/Markdown/2017-12-03-How to Render the Hyperlink with Braces/index.html","hash":"22bb1f40497d8dc88246cd30619b03a7eae33e8c","modified":1523747812181},{"_id":"public/2017/11/30/Mathematics/2017-11-30-Concave and Convex Function/index.html","hash":"071991a5e795594ce4ce7c09893c52328c4f2861","modified":1523747812181},{"_id":"public/2017/11/30/Algorithm/Optimize Algorithm/2017-11-30-Batch Gradient Descent/index.html","hash":"2132d54fce678f307273cbcefbf25099619d6d10","modified":1523747812181},{"_id":"public/2017/11/29/ML System Tips/2017-11-30-Get More Data/index.html","hash":"308051547f476f39b10cde75f79571cd1f5254af","modified":1523747812181},{"_id":"public/2017/11/29/Others/Markdown/2017-11-30-How to use MathJax in Markdown/index.html","hash":"9c65c3e0bc681ad4487254bb82f852c0d5edd232","modified":1523747812181},{"_id":"public/2017/11/22/Others/Hexos/2017-11-23-Hexo Next Theme Beautification/index.html","hash":"1f954e856fcc434f33779de344ef4f5cd81359f6","modified":1523747812181},{"_id":"public/2017/11/21/Others/Hexos/2017-11-22-Deploy Blog With Hexo And Github Page/index.html","hash":"9769296ed32c07dd1787b28cd5c1e6f95eee8d25","modified":1523747812181},{"_id":"public/2017/11/04/Mathematics/Calculus/2017-11-04-Typlor Expansion Example/index.html","hash":"955479671944c2ee2d1e75fe22c9684bcc9c5e79","modified":1523747812181},{"_id":"public/2017/11/03/Mathematics/Calculus/2017-11-03-Practical Derivatives/index.html","hash":"b6a807f42ccf5b4f25487ecf78de4471ccfb2318","modified":1523747812182},{"_id":"public/2017/11/02/Mathematics/Calculus/2017-11-02-Differentiation Rules/index.html","hash":"775153bd21b5d0ea944b3857bc16db091b1e9aee","modified":1523747812182},{"_id":"public/2017/11/01/Mathematics/Calculus/2017-11-02-What is a one-sided limit/index.html","hash":"5fae3a3cb62edbfa5f6063d1e70117bed32e4a14","modified":1523747812182},{"_id":"public/2017/11/01/Mathematics/Calculus/2017-11-02-Function and limit/index.html","hash":"05bdb3200c12c7347821eb3f4a23f1f61f237c8c","modified":1523747812182},{"_id":"public/2017/11/01/Mathematics/2017-11-01-线性模型性能分析--混淆矩阵(Confusion Matrix)/index.html","hash":"4a90e7f658ad08ee15a5a5f90d524d1b3774ecda","modified":1523747812182},{"_id":"public/2017/10/31/Mathematics/2017-10-31-数据分割/index.html","hash":"ca6a60064d4188ef5f421265332fbcfa4a7eb95f","modified":1523747812182},{"_id":"public/2017/10/30/Mathematics/2017-10-30-Sigmoid Function/index.html","hash":"2c3e2bc2377312740f35a418b4cac5e59a231d5e","modified":1523747812182},{"_id":"public/2017/10/29/Mathematics/2017-10-29-数据预处理-数据归一化和数据规范化/index.html","hash":"df70a6c549223042deac24524f6148712557d3fe","modified":1523747812182},{"_id":"public/2017/10/28/Mathematics/2017-10-29-tanh Function/index.html","hash":"093468c4a992277f0e435744aa7eb198217eb34a","modified":1523747812182},{"_id":"public/2017/10/28/Mathematics/2017-10-28-MatrixToVector/index.html","hash":"86a3709d4d0e73fc45d00db70c1fb224db58020e","modified":1523747812182},{"_id":"public/2017/10/22/ReadNote-Machine Learning Foudantion -NTU/2017-10-22-15.Summary - Power of Three/index.html","hash":"962bed75e265ec9effe045921fb520c3d2d07890","modified":1523747812182},{"_id":"public/2017/10/20/ReadNote-Machine Learning Foudantion -NTU/2017-10-21-14.How can Machine Learn Better - Three Learning Principles/index.html","hash":"03fd402356aaf44e155e09a18843c9fbea6a60dd","modified":1523747812182},{"_id":"public/2017/10/18/ReadNote-Machine Learning Foudantion -NTU/2017-10-18-13.How can Machine Learn Better - Validation/index.html","hash":"b49a6732c100d6f0f9f09b0276389f939e366824","modified":1523747812182},{"_id":"public/2017/10/16/ReadNote-Machine Learning Foudantion -NTU/2017-10-17-12.How can Machine Learn Better - Regularization/index.html","hash":"a59177244601bfacac211a800a85168c8241e982","modified":1523747812183},{"_id":"public/2017/10/16/ReadNote-Machine Learning Foudantion -NTU/2017-10-16-11.How can Machine Learn Better - Overfitting and Solution/index.html","hash":"9ab9a043ed2e525a74c66870cd1fa6f2e23998df","modified":1523747812183},{"_id":"public/2017/10/15/ReadNote-Machine Learning Foudantion -NTU/2017-10-15-10.How can Machine Learn - Nonlinear Transformation/index.html","hash":"25dab32f12a96f392793647a317b4cf6b057a887","modified":1523747812183},{"_id":"public/2017/10/14/ReadNote-Machine Learning Foudantion -NTU/2017-10-14-9.How can Machine Learn - Linear Model for Classification/index.html","hash":"95da136ecce362c81e43f15c1b36443d1c459a73","modified":1523747812183},{"_id":"public/2017/10/13/ReadNote-Machine Learning Foudantion -NTU/2017-10-13-8.How can Machine Learn - Logistic Regression/index.html","hash":"c48f9baf931b04e4132acdad21729ac733be8579","modified":1523747812183},{"_id":"public/2017/10/11/ReadNote-Machine Learning Foudantion -NTU/2017-10-11-7.How can Machine Learn - Linear Regression/index.html","hash":"bca1e7f735bcf2ee2beb316749a90b21f6bda348","modified":1523747812183},{"_id":"public/2017/10/09/ReadNote-Machine Learning Foudantion -NTU/2017-10-09-6.How can Machine Learn - Noice and Error/index.html","hash":"ceec593de1f1f4911ed061f6b403ab5899241ac7","modified":1523747812183},{"_id":"public/2017/10/08/ReadNote-Machine Learning Foudantion -NTU/2017-10-08-5.Why Can Machine Learn/index.html","hash":"599d8cb439bd9753d5d278aa51eaa8f6d8de1244","modified":1523747812183},{"_id":"public/2017/10/06/ReadNote-Machine Learning Foudantion -NTU/2017-10-06-4.When can Machine Learn - Feasible of Learning/index.html","hash":"5fdcf77961f40f56671ba6a72c2289b4d53328ca","modified":1523747812183},{"_id":"public/2017/10/04/ReadNote-Machine Learning Foudantion -NTU/2017-10-04-3.When can Machine Learn - Types of Learning/index.html","hash":"84c327b9f5a8b0bf933bf48f33545ad2cc3ffd30","modified":1523747812183},{"_id":"public/2017/10/02/ReadNote-Machine Learning Foudantion -NTU/2017-10-03-2.When can Machine Learn - Learning to Answer Yes or No/index.html","hash":"f72496b42fc31329317d5c7b9297eb674e1f6512","modified":1523747812184},{"_id":"public/2017/10/02/ReadNote-Machine Learning Foudantion -NTU/2017-10-02-1.When can Machine Learn - The Learning Problem/index.html","hash":"13a0fdf14dc123ad2f7d8ec527ddffa9d2b0743a","modified":1523747812184},{"_id":"public/2017/09/30/ReadNote-Machine Learning Foudantion -NTU/2017-10-01-0.Machine Learning Foundation - Table of Contents/index.html","hash":"2b6c2f560cbaab910d9540165d85462075aa5fae","modified":1523747812184},{"_id":"public/2017/09/15/2017-09-15-Why Keep Writing Blog/index.html","hash":"6907b31cfcbb40910900772fe4289c0a997bb81f","modified":1523747812184},{"_id":"public/archives/index.html","hash":"617b82c3f2468142ec62f3e7e0147acfb95e4ed5","modified":1523747812184},{"_id":"public/archives/page/2/index.html","hash":"a1959627a1a899187a60d4f1a9717a5c6da11daf","modified":1523747812184},{"_id":"public/archives/2017/index.html","hash":"46af6065c10b462e249179edd52fb1224a85e97a","modified":1523747812184},{"_id":"public/archives/2017/page/2/index.html","hash":"6ce74ffecf5c84e84b654f023af9db306d4f9db9","modified":1523747812184},{"_id":"public/archives/2017/09/index.html","hash":"cdd1b15d05bd34b1578bf243c13216968cd90a69","modified":1523747812184},{"_id":"public/archives/2017/10/index.html","hash":"f10fc6ef79aae79f15916ff9c296772328c132b1","modified":1523747812185},{"_id":"public/archives/2017/11/index.html","hash":"f2d5accd94b5875261d79c90ec806aba5f21eb7e","modified":1523747812185},{"_id":"public/archives/2017/12/index.html","hash":"5c7342bd3a88b938cc1a840f77febe2d73bea99d","modified":1523747812185},{"_id":"public/categories/ReadNote/index.html","hash":"e5295242b2f8f43790a9e04b2189b147180536b5","modified":1523747812185},{"_id":"public/categories/Latex/index.html","hash":"6cf76c41070f90d9dd4f4dcf05529e3e0243fca0","modified":1523747812185},{"_id":"public/categories/Machine-Learning-Mathematics/index.html","hash":"1108c7af53c7e8eedcbb91d42a4333b5394d3fcd","modified":1523747812185},{"_id":"public/categories/Machine-Learning-System-Tips/index.html","hash":"68713d4382cd182de82c77f5f6f1b5a8713e8fff","modified":1523747812185},{"_id":"public/categories/Machine-Learning-Algorithm/index.html","hash":"8c459f8de6c8d101517b384c5d49f3b3c44489ce","modified":1523747812185},{"_id":"public/categories/Hexo/index.html","hash":"8bc7737edd4d4374eaeac3cb3d8e92d7d57cd416","modified":1523747812185},{"_id":"public/categories/Markdown/index.html","hash":"e45d37e5221fe43836c70051ffa87776611f45c3","modified":1523747812185},{"_id":"public/index.html","hash":"cb989adbfd6a7de6b6d1d8f3b4f0faf9f736479d","modified":1523747812185},{"_id":"public/page/2/index.html","hash":"1499f63b93e51a5950346757104352c45c9eea37","modified":1523747812185},{"_id":"public/tags/ReadNote-Machine-Learning-Foundation/index.html","hash":"2d8ea451544cefbca7690be8fee998144df89bc2","modified":1523747812185},{"_id":"public/tags/Latex/index.html","hash":"cefb6e187feb4ddb9f0fae80b12c55ded9beafb4","modified":1523747812185},{"_id":"public/tags/Machine-Learning/index.html","hash":"5ed15c1748ca9849892432e59f8a7e0dc18f418a","modified":1523747812185},{"_id":"public/tags/Mathematics/index.html","hash":"2547a7fa28d40a79cca202b8e094ad0e26f70e9e","modified":1523747812185},{"_id":"public/tags/Machine-Learning-System-Tips/index.html","hash":"7a054364c0d6ccbcdc59ef8dd6bb776f1b795953","modified":1523747812185},{"_id":"public/tags/Machine-Learning-Mathematics/index.html","hash":"f0f8777d42c40b9a245e705c2bbd388d7f52fe69","modified":1523747812185},{"_id":"public/tags/Machine-Learning-Algorithm/index.html","hash":"8180226c4ae7abae585003d5e067e50e401b94db","modified":1523747812185},{"_id":"public/tags/Calculus/index.html","hash":"eda03102fc9ef31188ed95e65399d04bda852b38","modified":1523747812185},{"_id":"public/tags/Hexo/index.html","hash":"5a8c1b3e9ae7dd44e4f30436982d5265c62aedf6","modified":1523747812186},{"_id":"public/tags/Markdown/index.html","hash":"1f6111589b3e2c5e2e072f4afcc811e89aea16c5","modified":1523747812186},{"_id":"public/tags/Mathjax/index.html","hash":"6b0ec5eeb0780ed355258e4a7640457598dcba50","modified":1523747812186},{"_id":"public/README.md","hash":"2979a6aba846c0a215183c709fe5e5cd20df6810","modified":1523747317022},{"_id":"public/robots.txt","hash":"65968de5ef3fae4f6947d369c1f819dc628ec038","modified":1523747317022},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1523747317022},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1523747317022},{"_id":"public/images/android-chrome-192x192.png","hash":"1c4e77121a942fce2a122f56ad4c8120444f681b","modified":1523747317022},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1523747317022},{"_id":"public/images/apple-touch-icon.png","hash":"457f72c374756faeb4b375d4413b2c4eeb921093","modified":1523747317022},{"_id":"public/images/browserconfig.xml","hash":"974aea18bda5a95802c06b80126ab1d96d91d708","modified":1523747317022},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1523747317022},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1523747317022},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1523747317022},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1523747317023},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1523747317023},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1523747317023},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1523747317023},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1523747317023},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1523747317023},{"_id":"public/images/favicon-16x16.png","hash":"729e9550a1465cbeffdcbea5b70bb27883da981d","modified":1523747317023},{"_id":"public/images/favicon-32x32.png","hash":"b398dd3db0a29507602e6945c4e9606d188d523e","modified":1523747317023},{"_id":"public/images/favicon.ico","hash":"9c756e07b09d87bd03c30dc4315ac7818fd6c400","modified":1523747317023},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1523747317023},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1523747317024},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1523747317024},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1523747317024},{"_id":"public/images/mstile-150x150.png","hash":"3b50bc2c1c01c5dc42e4da0ecec5129201612581","modified":1523747317024},{"_id":"public/images/safari-pinned-tab.svg","hash":"e2ca3df89ab832d15933d3ea9b71226f662e168d","modified":1523747317024},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1523747317024},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1523747317024},{"_id":"public/images/site.webmanifest","hash":"13e5aa58eb2182d8ace63266856c8cc29c47c083","modified":1523747317024},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1523747317024},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1523747317025},{"_id":"public/images/wechatpay.jpg","hash":"652b5c1055cfa52ae6005cba00c751b1ec55b8f5","modified":1523747317657},{"_id":"public/css/main.css","hash":"38fd8e44eafe369e4d4490c3b18c8436ec22543f","modified":1523747317669},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1523747317669},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1523747317671},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1523747317673},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1523747317673},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1523747317673},{"_id":"public/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1523747317673},{"_id":"public/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1523747317673},{"_id":"public/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1523747317673},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1523747317673},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1523747317673},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1523747317673},{"_id":"public/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1523747317673},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1523747317673},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1523747317673},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1523747317673},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1523747317673},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1523747317673},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1523747317673},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1523747317674},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1523747317674},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1523747317674},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1523747317674},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1523747317674},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1523747317674},{"_id":"public/images/alipay.jpg","hash":"e34cd1e8fa17b157c0391bb897f3c10d80cd08f7","modified":1523747320033},{"_id":"public/images/android-chrome-512x512.png","hash":"668da720cbcb765a3baf9a026abc8b0a65d4d0cd","modified":1523747320060},{"_id":"public/images/avatar.png","hash":"29cd9d85ea4ba7e7eb1be1328b3f708b3b619624","modified":1523747320079},{"_id":"source/CNAME","hash":"3851bce1e3d8971d4c441975a4fd99ba737f077e","modified":1523243402325},{"_id":"public/CNAME","hash":"3851bce1e3d8971d4c441975a4fd99ba737f077e","modified":1523747644575},{"_id":"source/2018-02-26-Resume-Zhicheng Ding.pdf","hash":"46514ecb84f1d621614b51c9464093c8fd943a81","modified":1519657848274},{"_id":"public/2018-02-26-Resume-Zhicheng Ding.pdf","hash":"46514ecb84f1d621614b51c9464093c8fd943a81","modified":1523747921156},{"_id":"source/CV_Columbia_University_Jason_Ding.pdf","hash":"94f73fcd67dabf1a3e41f6f389fffd6cfb28c32a","modified":1524497028179},{"_id":"source/Resume_Columbia_University_Zhicheng Ding.pdf","hash":"2be7b20ff8587c8b419d9f06790bc2357e6e25d5","modified":1524072659230},{"_id":"public/CV_Columbia_University_Jason_Ding.pdf","hash":"94f73fcd67dabf1a3e41f6f389fffd6cfb28c32a","modified":1524630276948},{"_id":"public/Resume_Columbia_University_Zhicheng Ding.pdf","hash":"2be7b20ff8587c8b419d9f06790bc2357e6e25d5","modified":1524630276949}],"Category":[{"name":"ReadNote","_id":"cjfzzt2a60003rwtjjwkvte4w"},{"name":"Latex","_id":"cjfzzt2dc000brwtjkv2l5qyb"},{"name":"Machine-Learning-Mathematics","_id":"cjfzzt2dg000grwtjqs6k22x0"},{"name":"Machine-Learning-System-Tips","_id":"cjfzzt2dj000mrwtjo6mq3qwi"},{"name":"Machine-Learning-Algorithm","_id":"cjfzzt2hb0030rwtj9vzmnilq"},{"name":"Hexo","_id":"cjfzzt2hn003krwtjl7puyvj0"},{"name":"Markdown","_id":"cjfzzt2hv003zrwtjt9w6jq0c"}],"Data":[],"Page":[{"title":"Categories","date":"2017-11-01T06:59:40.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2017-11-01 14:59:40\ntype: \"categories\"\ncomments: false\n---\n","updated":"2018-04-14T19:42:06.510Z","path":"categories/index.html","layout":"page","_id":"cjfzzt2a20001rwtjr6qi6gd3","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"About","date":"2017-11-01T06:08:23.000Z","comments":0,"_content":"\n\n<p>\n<span class=\"hello\">\n  <em>Where</em>\n  <small>there is a will, there is a way! \n</span>\n</p>\n\n<blockquote>\n<p>\nThere is something inside, that they can't get to, that they can't touch. That's yours. --The Shawshank Redemption\n</p>\n</blockquote>\n\n\n<br>\n<p>\nThis is Zhicheng Ding, currrent a machine learning master student in University. This website aims to collect my blogs about life experience and technic understanding.\n</p>\n\n<p>\nI am interested in Natural Language Processing. My ultimate goal is to create a real-time high accuracy translator. I have this sense of idea for the reason that my father is afraid to travel aboard because of language problem.\n</p>\n\n<p>\nI support open source (@<a href=\"https://github.com/zhichengML\">zhichengML<i class=\"icon-github\"></i>)</a>. The most frequently language is Python and Octave. \nI love fitness, tennis and hiking.\n</p>\n\n<p>\nI am fond of sharing knowledge. That's definitely the reason for my to keep writing blog. Meanwhile, I enjoy to overcome challenges. That's the why I choose to learn science. My dream is to be a professor in college since I could both sharing knowledge and overcomming challenges.\n</p>\n<p>\nFor more details about me, click here to access my <a href = \"http://www.linkedin.com/in/zhicheng-ding-a80595b3/\">linkedin<i class=\"icon-linkedin\"></i></a>.\n</p>","source":"about/index.md","raw":"---\ntitle: About\ndate: 2017-11-01 14:08:23\ncomments: false\n---\n\n\n<p>\n<span class=\"hello\">\n  <em>Where</em>\n  <small>there is a will, there is a way! \n</span>\n</p>\n\n<blockquote>\n<p>\nThere is something inside, that they can't get to, that they can't touch. That's yours. --The Shawshank Redemption\n</p>\n</blockquote>\n\n\n<br>\n<p>\nThis is Zhicheng Ding, currrent a machine learning master student in University. This website aims to collect my blogs about life experience and technic understanding.\n</p>\n\n<p>\nI am interested in Natural Language Processing. My ultimate goal is to create a real-time high accuracy translator. I have this sense of idea for the reason that my father is afraid to travel aboard because of language problem.\n</p>\n\n<p>\nI support open source (@<a href=\"https://github.com/zhichengML\">zhichengML<i class=\"icon-github\"></i>)</a>. The most frequently language is Python and Octave. \nI love fitness, tennis and hiking.\n</p>\n\n<p>\nI am fond of sharing knowledge. That's definitely the reason for my to keep writing blog. Meanwhile, I enjoy to overcome challenges. That's the why I choose to learn science. My dream is to be a professor in college since I could both sharing knowledge and overcomming challenges.\n</p>\n<p>\nFor more details about me, click here to access my <a href = \"http://www.linkedin.com/in/zhicheng-ding-a80595b3/\">linkedin<i class=\"icon-linkedin\"></i></a>.\n</p>","updated":"2018-04-14T19:42:06.510Z","path":"about/index.html","layout":"page","_id":"cjfzzt2a40002rwtju22c7omm","content":"<p>\n<span class=\"hello\">\n  <em>Where</em>\n  <small>there is a will, there is a way! \n</small></span>\n</p>\n\n<blockquote>\n<p>\nThere is something inside, that they can't get to, that they can't touch. That's yours. --The Shawshank Redemption\n</p>\n</blockquote>\n\n\n<p><br></p>\n<p>\nThis is Zhicheng Ding, currrent a machine learning master student in University. This website aims to collect my blogs about life experience and technic understanding.\n</p>\n\n<p>\nI am interested in Natural Language Processing. My ultimate goal is to create a real-time high accuracy translator. I have this sense of idea for the reason that my father is afraid to travel aboard because of language problem.\n</p>\n\n<p>\nI support open source (@<a href=\"https://github.com/zhichengML\" target=\"_blank\" rel=\"external\">zhichengML<i class=\"icon-github\"></i>)</a>. The most frequently language is Python and Octave. \nI love fitness, tennis and hiking.\n</p>\n\n<p>\nI am fond of sharing knowledge. That's definitely the reason for my to keep writing blog. Meanwhile, I enjoy to overcome challenges. That's the why I choose to learn science. My dream is to be a professor in college since I could both sharing knowledge and overcomming challenges.\n</p>\n<p>\nFor more details about me, click here to access my <a href=\"http://www.linkedin.com/in/zhicheng-ding-a80595b3/\" target=\"_blank\" rel=\"external\">linkedin<i class=\"icon-linkedin\"></i></a>.\n</p>","site":{"data":{}},"excerpt":"","more":"<p>\n<span class=\"hello\">\n  <em>Where</em>\n  <small>there is a will, there is a way! \n</small></span>\n</p>\n\n<blockquote>\n<p>\nThere is something inside, that they can't get to, that they can't touch. That's yours. --The Shawshank Redemption\n</p>\n</blockquote>\n\n\n<p><br></p>\n<p>\nThis is Zhicheng Ding, currrent a machine learning master student in University. This website aims to collect my blogs about life experience and technic understanding.\n</p>\n\n<p>\nI am interested in Natural Language Processing. My ultimate goal is to create a real-time high accuracy translator. I have this sense of idea for the reason that my father is afraid to travel aboard because of language problem.\n</p>\n\n<p>\nI support open source (@<a href=\"https://github.com/zhichengML\" target=\"_blank\" rel=\"external\">zhichengML<i class=\"icon-github\"></i>)</a>. The most frequently language is Python and Octave. \nI love fitness, tennis and hiking.\n</p>\n\n<p>\nI am fond of sharing knowledge. That's definitely the reason for my to keep writing blog. Meanwhile, I enjoy to overcome challenges. That's the why I choose to learn science. My dream is to be a professor in college since I could both sharing knowledge and overcomming challenges.\n</p>\n<p>\nFor more details about me, click here to access my <a href=\"http://www.linkedin.com/in/zhicheng-ding-a80595b3/\" target=\"_blank\" rel=\"external\">linkedin<i class=\"icon-linkedin\"></i></a>.\n</p>"},{"title":"Tags","date":"2017-11-01T05:59:40.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2017-11-01 13:59:40\ntype: \"tags\"\ncomments: false\n---\n","updated":"2018-04-14T19:42:06.512Z","path":"tags/index.html","layout":"page","_id":"cjfzzt2a80005rwtjow0tmonx","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Projects","date":"2017-11-01T06:59:40.000Z","comments":0,"_content":"\n\n1. The Online PDF Reader based on Gesture Movement [[Demo]](https://www.youtube.com/watch?v=IsIhWa0TZrA&feature=youtu.be)\n\n2. More ...","source":"projects/index.md","raw":"---\ntitle: Projects\ndate: 2017-11-01 14:59:40\ncomments: false\n---\n\n\n1. The Online PDF Reader based on Gesture Movement [[Demo]](https://www.youtube.com/watch?v=IsIhWa0TZrA&feature=youtu.be)\n\n2. More ...","updated":"2018-04-14T23:00:42.459Z","path":"projects/index.html","layout":"page","_id":"cjfzzt2a80006rwtjjmu5svg5","content":"<ol>\n<li><p>The Online PDF Reader based on Gesture Movement <a href=\"https://www.youtube.com/watch?v=IsIhWa0TZrA&amp;feature=youtu.be\" target=\"_blank\" rel=\"external\">[Demo]</a></p>\n</li>\n<li><p>More …</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<ol>\n<li><p>The Online PDF Reader based on Gesture Movement <a href=\"https://www.youtube.com/watch?v=IsIhWa0TZrA&amp;feature=youtu.be\" target=\"_blank\" rel=\"external\">[Demo]</a></p>\n</li>\n<li><p>More …</p>\n</li>\n</ol>\n"}],"Post":[{"title":"Why Keep Writing Blog?","date":"2017-09-15T09:23:19.000Z","mathjax":false,"copyright":true,"top":200,"_content":"\n\n## 1. Personal Need\n\nBy writing blog, I could able to document my life experience. For so many time, I failed to recall expriences and what I have learned clearly.\n\nMemories are so fragile! Keeping a note is a good way to keep my life lively.\n\n## 2. Systematically Learn Kownledge\n\nWriting blog, especially technic blog is a good way to review theose knowledge systematically. When begin writing blog, I tried to build up the connection among the knowledge I have learned.\n\n## 3. Sharing is Happy\n\nSharing knowledge is a really a happy thing to me. My ultimate goal is to achieve something really pracitical for this world. Also, I like sharing and challenging difficulties. That's why I aims to become a professor in college. Start to write a blog is one of the great way to prepare for my goal.\n\n## 4. Knowledge Interaction\n\nWriting blog is also an another way to listen to people, for not only technic correction, but also life. As for technic blog, for sure I could not ensure all the things I write is 100% right. I will try my best to leave as least wrong idea as possible, but blog is a good way for reader to help me correct it. As for life experience, I would like to hear from different opinions. And I will try to best to do the best.\n\n\n\n\nFinally, hope everyone could find something useful in this blog. Good luck!\n\nThanks,\n\nZhichengMLE\n","source":"_posts/2017-09-15-Why Keep Writing Blog.md","raw":"---\ntitle: Why Keep Writing Blog?\ndate: 2017-09-15 17:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: false\ncopyright: true\ntop: 200\n---\n\n\n## 1. Personal Need\n\nBy writing blog, I could able to document my life experience. For so many time, I failed to recall expriences and what I have learned clearly.\n\nMemories are so fragile! Keeping a note is a good way to keep my life lively.\n\n## 2. Systematically Learn Kownledge\n\nWriting blog, especially technic blog is a good way to review theose knowledge systematically. When begin writing blog, I tried to build up the connection among the knowledge I have learned.\n\n## 3. Sharing is Happy\n\nSharing knowledge is a really a happy thing to me. My ultimate goal is to achieve something really pracitical for this world. Also, I like sharing and challenging difficulties. That's why I aims to become a professor in college. Start to write a blog is one of the great way to prepare for my goal.\n\n## 4. Knowledge Interaction\n\nWriting blog is also an another way to listen to people, for not only technic correction, but also life. As for technic blog, for sure I could not ensure all the things I write is 100% right. I will try my best to leave as least wrong idea as possible, but blog is a good way for reader to help me correct it. As for life experience, I would like to hear from different opinions. And I will try to best to do the best.\n\n\n\n\nFinally, hope everyone could find something useful in this blog. Good luck!\n\nThanks,\n\nZhichengMLE\n","slug":"2017-09-15-Why Keep Writing Blog","published":1,"updated":"2018-04-14T23:03:15.594Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt29x0000rwtjth7kqrys","content":"<h2 id=\"1-Personal-Need\"><a href=\"#1-Personal-Need\" class=\"headerlink\" title=\"1. Personal Need\"></a>1. Personal Need</h2><p>By writing blog, I could able to document my life experience. For so many time, I failed to recall expriences and what I have learned clearly.</p>\n<p>Memories are so fragile! Keeping a note is a good way to keep my life lively.</p>\n<h2 id=\"2-Systematically-Learn-Kownledge\"><a href=\"#2-Systematically-Learn-Kownledge\" class=\"headerlink\" title=\"2. Systematically Learn Kownledge\"></a>2. Systematically Learn Kownledge</h2><p>Writing blog, especially technic blog is a good way to review theose knowledge systematically. When begin writing blog, I tried to build up the connection among the knowledge I have learned.</p>\n<h2 id=\"3-Sharing-is-Happy\"><a href=\"#3-Sharing-is-Happy\" class=\"headerlink\" title=\"3. Sharing is Happy\"></a>3. Sharing is Happy</h2><p>Sharing knowledge is a really a happy thing to me. My ultimate goal is to achieve something really pracitical for this world. Also, I like sharing and challenging difficulties. That’s why I aims to become a professor in college. Start to write a blog is one of the great way to prepare for my goal.</p>\n<h2 id=\"4-Knowledge-Interaction\"><a href=\"#4-Knowledge-Interaction\" class=\"headerlink\" title=\"4. Knowledge Interaction\"></a>4. Knowledge Interaction</h2><p>Writing blog is also an another way to listen to people, for not only technic correction, but also life. As for technic blog, for sure I could not ensure all the things I write is 100% right. I will try my best to leave as least wrong idea as possible, but blog is a good way for reader to help me correct it. As for life experience, I would like to hear from different opinions. And I will try to best to do the best.</p>\n<p>Finally, hope everyone could find something useful in this blog. Good luck!</p>\n<p>Thanks,</p>\n<p>ZhichengMLE</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-Personal-Need\"><a href=\"#1-Personal-Need\" class=\"headerlink\" title=\"1. Personal Need\"></a>1. Personal Need</h2><p>By writing blog, I could able to document my life experience. For so many time, I failed to recall expriences and what I have learned clearly.</p>\n<p>Memories are so fragile! Keeping a note is a good way to keep my life lively.</p>\n<h2 id=\"2-Systematically-Learn-Kownledge\"><a href=\"#2-Systematically-Learn-Kownledge\" class=\"headerlink\" title=\"2. Systematically Learn Kownledge\"></a>2. Systematically Learn Kownledge</h2><p>Writing blog, especially technic blog is a good way to review theose knowledge systematically. When begin writing blog, I tried to build up the connection among the knowledge I have learned.</p>\n<h2 id=\"3-Sharing-is-Happy\"><a href=\"#3-Sharing-is-Happy\" class=\"headerlink\" title=\"3. Sharing is Happy\"></a>3. Sharing is Happy</h2><p>Sharing knowledge is a really a happy thing to me. My ultimate goal is to achieve something really pracitical for this world. Also, I like sharing and challenging difficulties. That’s why I aims to become a professor in college. Start to write a blog is one of the great way to prepare for my goal.</p>\n<h2 id=\"4-Knowledge-Interaction\"><a href=\"#4-Knowledge-Interaction\" class=\"headerlink\" title=\"4. Knowledge Interaction\"></a>4. Knowledge Interaction</h2><p>Writing blog is also an another way to listen to people, for not only technic correction, but also life. As for technic blog, for sure I could not ensure all the things I write is 100% right. I will try my best to leave as least wrong idea as possible, but blog is a good way for reader to help me correct it. As for life experience, I would like to hear from different opinions. And I will try to best to do the best.</p>\n<p>Finally, hope everyone could find something useful in this blog. Good luck!</p>\n<p>Thanks,</p>\n<p>ZhichengMLE</p>\n"},{"title":"Latex 常见错误整理","date":"2017-12-05T04:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Latex 常见错误整理\n\n## 1. 求导\n\n```\n\\sideset{^*}{'}\\sum_{1\\le i\\le 100} A(i)\n\\qquad\n\\sum_{1\\le i\\le 100}\\vphantom{\\sum}^{'} A(i)\n\\qquad\n\\mathop{{\\sum}'}_{1\\le i\\le 100} A(i)\n```\n\n$$\n\\sideset{^*}{'}\\sum_{1\\le i\\le 100} A(i)\n\\qquad\n\\sum_{1\\le i\\le 100}\\vphantom{\\sum}^{'} A(i)\n$$\n\n## 2. 下标\n\n```\nC_{1} + {C_2} \\\\\nC_{m,n}\n```\n\n$$\nC_{1} + {C_2} \\\\\nC_{m,n}\n$$\n\n\n## 3. 前缀\n\n```\n{\\int_{0}^{\\frac{\\pi}{2}}} \\\\\n\\sum_{i=1}^{n}\\\\\n\\prod_\\epsilon\n```\n\n$$\n{\\int_{0}^{\\frac{\\pi}{2}}} \\\\\n\\sum_{i=1}^{n}\\\\\n\\prod_\\epsilon\n$$\n\n## 4. 占位符\n\n```\nC_{1} \\qquad {C_2} \\\\\nC_{1} \\quad {C_2} \\\\\nC_{1} \\ {C_2}\n```\n\n$$\nC_{1} \\qquad {C_2} \\\\\nC_{1} \\quad {C_2} \\\\\nC_{1} \\ {C_2}\n$$\n\n## 5. 公式对齐\n\n```\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}\n$$\n```\n\n\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}\n$$\n\n\n\n------------------------------------------\n<br>\n<br>","source":"_posts/Latex/2017-12-05-Latex常见错误整理.md","raw":"---\ntitle: Latex 常见错误整理\ndate: 2017-12-05 12:23:19\ncategories: [Latex]\ntags: [Latex]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Latex 常见错误整理\n\n## 1. 求导\n\n```\n\\sideset{^*}{'}\\sum_{1\\le i\\le 100} A(i)\n\\qquad\n\\sum_{1\\le i\\le 100}\\vphantom{\\sum}^{'} A(i)\n\\qquad\n\\mathop{{\\sum}'}_{1\\le i\\le 100} A(i)\n```\n\n$$\n\\sideset{^*}{'}\\sum_{1\\le i\\le 100} A(i)\n\\qquad\n\\sum_{1\\le i\\le 100}\\vphantom{\\sum}^{'} A(i)\n$$\n\n## 2. 下标\n\n```\nC_{1} + {C_2} \\\\\nC_{m,n}\n```\n\n$$\nC_{1} + {C_2} \\\\\nC_{m,n}\n$$\n\n\n## 3. 前缀\n\n```\n{\\int_{0}^{\\frac{\\pi}{2}}} \\\\\n\\sum_{i=1}^{n}\\\\\n\\prod_\\epsilon\n```\n\n$$\n{\\int_{0}^{\\frac{\\pi}{2}}} \\\\\n\\sum_{i=1}^{n}\\\\\n\\prod_\\epsilon\n$$\n\n## 4. 占位符\n\n```\nC_{1} \\qquad {C_2} \\\\\nC_{1} \\quad {C_2} \\\\\nC_{1} \\ {C_2}\n```\n\n$$\nC_{1} \\qquad {C_2} \\\\\nC_{1} \\quad {C_2} \\\\\nC_{1} \\ {C_2}\n$$\n\n## 5. 公式对齐\n\n```\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}\n$$\n```\n\n\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}\n$$\n\n\n\n------------------------------------------\n<br>\n<br>","slug":"Latex/2017-12-05-Latex常见错误整理","published":1,"updated":"2018-04-14T19:42:06.485Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2da0009rwtjod1qx9sb","content":"<h1 id=\"Latex-常见错误整理\"><a href=\"#Latex-常见错误整理\" class=\"headerlink\" title=\"Latex 常见错误整理\"></a>Latex 常见错误整理</h1><h2 id=\"1-求导\"><a href=\"#1-求导\" class=\"headerlink\" title=\"1. 求导\"></a>1. 求导</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\sideset&#123;^*&#125;&#123;&apos;&#125;\\sum_&#123;1\\le i\\le 100&#125; A(i)</div><div class=\"line\">\\qquad</div><div class=\"line\">\\sum_&#123;1\\le i\\le 100&#125;\\vphantom&#123;\\sum&#125;^&#123;&apos;&#125; A(i)</div><div class=\"line\">\\qquad</div><div class=\"line\">\\mathop&#123;&#123;\\sum&#125;&apos;&#125;_&#123;1\\le i\\le 100&#125; A(i)</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\sideset{^*}{'}\\sum_{1\\le i\\le 100} A(i)\n\\qquad\n\\sum_{1\\le i\\le 100}\\vphantom{\\sum}^{'} A(i)</script><h2 id=\"2-下标\"><a href=\"#2-下标\" class=\"headerlink\" title=\"2. 下标\"></a>2. 下标</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">C_&#123;1&#125; + &#123;C_2&#125; \\\\</div><div class=\"line\">C_&#123;m,n&#125;</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\nC_{1} + {C_2} \\\\\nC_{m,n}</script><h2 id=\"3-前缀\"><a href=\"#3-前缀\" class=\"headerlink\" title=\"3. 前缀\"></a>3. 前缀</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;\\int_&#123;0&#125;^&#123;\\frac&#123;\\pi&#125;&#123;2&#125;&#125;&#125; \\\\</div><div class=\"line\">\\sum_&#123;i=1&#125;^&#123;n&#125;\\\\</div><div class=\"line\">\\prod_\\epsilon</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n{\\int_{0}^{\\frac{\\pi}{2}}} \\\\\n\\sum_{i=1}^{n}\\\\\n\\prod_\\epsilon</script><h2 id=\"4-占位符\"><a href=\"#4-占位符\" class=\"headerlink\" title=\"4. 占位符\"></a>4. 占位符</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">C_&#123;1&#125; \\qquad &#123;C_2&#125; \\\\</div><div class=\"line\">C_&#123;1&#125; \\quad &#123;C_2&#125; \\\\</div><div class=\"line\">C_&#123;1&#125; \\ &#123;C_2&#125;</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\nC_{1} \\qquad {C_2} \\\\\nC_{1} \\quad {C_2} \\\\\nC_{1} \\ {C_2}</script><h2 id=\"5-公式对齐\"><a href=\"#5-公式对齐\" class=\"headerlink\" title=\"5. 公式对齐\"></a>5. 公式对齐</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">$$</div><div class=\"line\">\\begin&#123;align&#125;</div><div class=\"line\">\\lim\\limits_&#123;x \\rightarrow a&#125; f(x)</div><div class=\"line\">&amp;= \\frac&#123;f\\left( a \\right)&#125;&#123; 0 !&#125;\\left( x - a \\right) ^ 0 + \\frac&#123;f&apos;\\left( a \\right)&#125;&#123; 1 !&#125;\\left( x - a \\right) ^ 1 + \\frac&#123;f&apos;&apos;\\left( a \\right)&#125;&#123; 2 !&#125;\\left( x - a \\right) ^ 2  + \\cdots + \\frac&#123;f^&#123;(n)&#125;\\left( a \\right)&#125;&#123; n !&#125;\\left( x - a \\right) ^ n \\\\</div><div class=\"line\">&amp;= \\sum\\limits_&#123;n=0&#125;^&#123;\\infty&#125; \\frac&#123;f^&#123;(n)&#125;\\left( a \\right)&#125;&#123; n !&#125;\\left( x - a \\right) ^ n</div><div class=\"line\">\\end&#123;align&#125;</div><div class=\"line\">$$</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}</script><hr>\n<p><br><br><br></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Latex-常见错误整理\"><a href=\"#Latex-常见错误整理\" class=\"headerlink\" title=\"Latex 常见错误整理\"></a>Latex 常见错误整理</h1><h2 id=\"1-求导\"><a href=\"#1-求导\" class=\"headerlink\" title=\"1. 求导\"></a>1. 求导</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">\\sideset&#123;^*&#125;&#123;&apos;&#125;\\sum_&#123;1\\le i\\le 100&#125; A(i)</div><div class=\"line\">\\qquad</div><div class=\"line\">\\sum_&#123;1\\le i\\le 100&#125;\\vphantom&#123;\\sum&#125;^&#123;&apos;&#125; A(i)</div><div class=\"line\">\\qquad</div><div class=\"line\">\\mathop&#123;&#123;\\sum&#125;&apos;&#125;_&#123;1\\le i\\le 100&#125; A(i)</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\sideset{^*}{'}\\sum_{1\\le i\\le 100} A(i)\n\\qquad\n\\sum_{1\\le i\\le 100}\\vphantom{\\sum}^{'} A(i)</script><h2 id=\"2-下标\"><a href=\"#2-下标\" class=\"headerlink\" title=\"2. 下标\"></a>2. 下标</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">C_&#123;1&#125; + &#123;C_2&#125; \\\\</div><div class=\"line\">C_&#123;m,n&#125;</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\nC_{1} + {C_2} \\\\\nC_{m,n}</script><h2 id=\"3-前缀\"><a href=\"#3-前缀\" class=\"headerlink\" title=\"3. 前缀\"></a>3. 前缀</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;\\int_&#123;0&#125;^&#123;\\frac&#123;\\pi&#125;&#123;2&#125;&#125;&#125; \\\\</div><div class=\"line\">\\sum_&#123;i=1&#125;^&#123;n&#125;\\\\</div><div class=\"line\">\\prod_\\epsilon</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n{\\int_{0}^{\\frac{\\pi}{2}}} \\\\\n\\sum_{i=1}^{n}\\\\\n\\prod_\\epsilon</script><h2 id=\"4-占位符\"><a href=\"#4-占位符\" class=\"headerlink\" title=\"4. 占位符\"></a>4. 占位符</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">C_&#123;1&#125; \\qquad &#123;C_2&#125; \\\\</div><div class=\"line\">C_&#123;1&#125; \\quad &#123;C_2&#125; \\\\</div><div class=\"line\">C_&#123;1&#125; \\ &#123;C_2&#125;</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\nC_{1} \\qquad {C_2} \\\\\nC_{1} \\quad {C_2} \\\\\nC_{1} \\ {C_2}</script><h2 id=\"5-公式对齐\"><a href=\"#5-公式对齐\" class=\"headerlink\" title=\"5. 公式对齐\"></a>5. 公式对齐</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">$$</div><div class=\"line\">\\begin&#123;align&#125;</div><div class=\"line\">\\lim\\limits_&#123;x \\rightarrow a&#125; f(x)</div><div class=\"line\">&amp;= \\frac&#123;f\\left( a \\right)&#125;&#123; 0 !&#125;\\left( x - a \\right) ^ 0 + \\frac&#123;f&apos;\\left( a \\right)&#125;&#123; 1 !&#125;\\left( x - a \\right) ^ 1 + \\frac&#123;f&apos;&apos;\\left( a \\right)&#125;&#123; 2 !&#125;\\left( x - a \\right) ^ 2  + \\cdots + \\frac&#123;f^&#123;(n)&#125;\\left( a \\right)&#125;&#123; n !&#125;\\left( x - a \\right) ^ n \\\\</div><div class=\"line\">&amp;= \\sum\\limits_&#123;n=0&#125;^&#123;\\infty&#125; \\frac&#123;f^&#123;(n)&#125;\\left( a \\right)&#125;&#123; n !&#125;\\left( x - a \\right) ^ n</div><div class=\"line\">\\end&#123;align&#125;</div><div class=\"line\">$$</div></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}</script><hr>\n<p><br><br><br></p>\n"},{"title":"tanh Function","date":"2017-10-29T03:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# tanh Function\n\n\n## 1. Introduction\n> To limit all data within the range of -1 to 1. Comparing to Sigmoid Function which output range is [0,1]\n\n## 2. Formula\nThe formula and derivative of tanh is:\n$$\n\\begin{align}\nf(z)  &= tanh(z) = \\frac{e^z - e^{-z}}{e^z+e^{-z}} \\\\\nf'(z) &= 1 - (f(z))^2\n\\end{align}\n$$\nwhere as the sigmoid function is pretty close\n$$\n\\begin{align}\nf(z)  &= sigmoid(z) = \\frac{1}{1+e^{-z}} \\\\\nf'(z) &= 1 - f(z)\n\\end{align}\n$$\n\nSee the figure of tanh and sigmoid below.\n![tanh](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c0f683d49f1d3f2d5383d25566d4f490c552645d/__Blog/__Personal%20Understanding/Mathematics/images/tanh.png)\n<center>tanh Function</center>\n\n\n![sigmoid](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/2f13c175c89fc02793cef19d6cb2223d548479d3/__Blog/__Personal%20Understanding/Mathematics/images/sigmoid.png)\n<center>sigmoid Function</center>\n\n\n\n\n## 3. Implementation\n### 3.1 Octave\n```\n x = linspace(-10, 10 ,10000);\n y = zeros( size(x, 1), size(x, 2));\n\n for i = 1:length(x)\n   y(i) = 1/(1+e^(-x(i)));\n endfor\n\n figure();\n plot( x,y);\n grid on;\n xlabel(\"x\");\n ylabel(\"y=1/(1+e^-x)\");\n title(\"Sigmoid Function\");\n```\n\nOutput figure\n![tanh](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c0f683d49f1d3f2d5383d25566d4f490c552645d/__Blog/__Personal%20Understanding/Mathematics/images/tanh.png)\n<center>tanh Function</center>\n\n\n\n# Relative\n[Sigmoid Function](https://zhichengmle.github.io/2017/10/30/Mathematics/2017-10-30-Sigmoid%20Function/)\n\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/2017-10-29-tanh Function.md","raw":"---\ntitle: tanh Function\ndate: 2017-10-29 11:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# tanh Function\n\n\n## 1. Introduction\n> To limit all data within the range of -1 to 1. Comparing to Sigmoid Function which output range is [0,1]\n\n## 2. Formula\nThe formula and derivative of tanh is:\n$$\n\\begin{align}\nf(z)  &= tanh(z) = \\frac{e^z - e^{-z}}{e^z+e^{-z}} \\\\\nf'(z) &= 1 - (f(z))^2\n\\end{align}\n$$\nwhere as the sigmoid function is pretty close\n$$\n\\begin{align}\nf(z)  &= sigmoid(z) = \\frac{1}{1+e^{-z}} \\\\\nf'(z) &= 1 - f(z)\n\\end{align}\n$$\n\nSee the figure of tanh and sigmoid below.\n![tanh](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c0f683d49f1d3f2d5383d25566d4f490c552645d/__Blog/__Personal%20Understanding/Mathematics/images/tanh.png)\n<center>tanh Function</center>\n\n\n![sigmoid](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/2f13c175c89fc02793cef19d6cb2223d548479d3/__Blog/__Personal%20Understanding/Mathematics/images/sigmoid.png)\n<center>sigmoid Function</center>\n\n\n\n\n## 3. Implementation\n### 3.1 Octave\n```\n x = linspace(-10, 10 ,10000);\n y = zeros( size(x, 1), size(x, 2));\n\n for i = 1:length(x)\n   y(i) = 1/(1+e^(-x(i)));\n endfor\n\n figure();\n plot( x,y);\n grid on;\n xlabel(\"x\");\n ylabel(\"y=1/(1+e^-x)\");\n title(\"Sigmoid Function\");\n```\n\nOutput figure\n![tanh](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c0f683d49f1d3f2d5383d25566d4f490c552645d/__Blog/__Personal%20Understanding/Mathematics/images/tanh.png)\n<center>tanh Function</center>\n\n\n\n# Relative\n[Sigmoid Function](https://zhichengmle.github.io/2017/10/30/Mathematics/2017-10-30-Sigmoid%20Function/)\n\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/2017-10-29-tanh Function","published":1,"updated":"2018-04-14T19:42:06.487Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2db000arwtjjeaj2bzy","content":"<h1 id=\"tanh-Function\"><a href=\"#tanh-Function\" class=\"headerlink\" title=\"tanh Function\"></a>tanh Function</h1><h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><blockquote>\n<p>To limit all data within the range of -1 to 1. Comparing to Sigmoid Function which output range is [0,1]</p>\n</blockquote>\n<h2 id=\"2-Formula\"><a href=\"#2-Formula\" class=\"headerlink\" title=\"2. Formula\"></a>2. Formula</h2><p>The formula and derivative of tanh is:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf(z)  &= tanh(z) = \\frac{e^z - e^{-z}}{e^z+e^{-z}} \\\\\nf'(z) &= 1 - (f(z))^2\n\\end{align}</script><p>where as the sigmoid function is pretty close</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf(z)  &= sigmoid(z) = \\frac{1}{1+e^{-z}} \\\\\nf'(z) &= 1 - f(z)\n\\end{align}</script><p>See the figure of tanh and sigmoid below.<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c0f683d49f1d3f2d5383d25566d4f490c552645d/__Blog/__Personal%20Understanding/Mathematics/images/tanh.png\" alt=\"tanh\"></p>\n<center>tanh Function</center>\n\n\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/2f13c175c89fc02793cef19d6cb2223d548479d3/__Blog/__Personal%20Understanding/Mathematics/images/sigmoid.png\" alt=\"sigmoid\"></p>\n<center>sigmoid Function</center>\n\n\n\n\n<h2 id=\"3-Implementation\"><a href=\"#3-Implementation\" class=\"headerlink\" title=\"3. Implementation\"></a>3. Implementation</h2><h3 id=\"3-1-Octave\"><a href=\"#3-1-Octave\" class=\"headerlink\" title=\"3.1 Octave\"></a>3.1 Octave</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">x = linspace(-10, 10 ,10000);</div><div class=\"line\">y = zeros( size(x, 1), size(x, 2));</div><div class=\"line\"></div><div class=\"line\">for i = 1:length(x)</div><div class=\"line\">  y(i) = 1/(1+e^(-x(i)));</div><div class=\"line\">endfor</div><div class=\"line\"></div><div class=\"line\">figure();</div><div class=\"line\">plot( x,y);</div><div class=\"line\">grid on;</div><div class=\"line\">xlabel(&quot;x&quot;);</div><div class=\"line\">ylabel(&quot;y=1/(1+e^-x)&quot;);</div><div class=\"line\">title(&quot;Sigmoid Function&quot;);</div></pre></td></tr></table></figure>\n<p>Output figure<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c0f683d49f1d3f2d5383d25566d4f490c552645d/__Blog/__Personal%20Understanding/Mathematics/images/tanh.png\" alt=\"tanh\"></p>\n<center>tanh Function</center>\n\n\n\n<h1 id=\"Relative\"><a href=\"#Relative\" class=\"headerlink\" title=\"Relative\"></a>Relative</h1><p><a href=\"https://zhichengmle.github.io/2017/10/30/Mathematics/2017-10-30-Sigmoid%20Function/\">Sigmoid Function</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"tanh-Function\"><a href=\"#tanh-Function\" class=\"headerlink\" title=\"tanh Function\"></a>tanh Function</h1><h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><blockquote>\n<p>To limit all data within the range of -1 to 1. Comparing to Sigmoid Function which output range is [0,1]</p>\n</blockquote>\n<h2 id=\"2-Formula\"><a href=\"#2-Formula\" class=\"headerlink\" title=\"2. Formula\"></a>2. Formula</h2><p>The formula and derivative of tanh is:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf(z)  &= tanh(z) = \\frac{e^z - e^{-z}}{e^z+e^{-z}} \\\\\nf'(z) &= 1 - (f(z))^2\n\\end{align}</script><p>where as the sigmoid function is pretty close</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf(z)  &= sigmoid(z) = \\frac{1}{1+e^{-z}} \\\\\nf'(z) &= 1 - f(z)\n\\end{align}</script><p>See the figure of tanh and sigmoid below.<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c0f683d49f1d3f2d5383d25566d4f490c552645d/__Blog/__Personal%20Understanding/Mathematics/images/tanh.png\" alt=\"tanh\"></p>\n<center>tanh Function</center>\n\n\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/2f13c175c89fc02793cef19d6cb2223d548479d3/__Blog/__Personal%20Understanding/Mathematics/images/sigmoid.png\" alt=\"sigmoid\"></p>\n<center>sigmoid Function</center>\n\n\n\n\n<h2 id=\"3-Implementation\"><a href=\"#3-Implementation\" class=\"headerlink\" title=\"3. Implementation\"></a>3. Implementation</h2><h3 id=\"3-1-Octave\"><a href=\"#3-1-Octave\" class=\"headerlink\" title=\"3.1 Octave\"></a>3.1 Octave</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">x = linspace(-10, 10 ,10000);</div><div class=\"line\">y = zeros( size(x, 1), size(x, 2));</div><div class=\"line\"></div><div class=\"line\">for i = 1:length(x)</div><div class=\"line\">  y(i) = 1/(1+e^(-x(i)));</div><div class=\"line\">endfor</div><div class=\"line\"></div><div class=\"line\">figure();</div><div class=\"line\">plot( x,y);</div><div class=\"line\">grid on;</div><div class=\"line\">xlabel(&quot;x&quot;);</div><div class=\"line\">ylabel(&quot;y=1/(1+e^-x)&quot;);</div><div class=\"line\">title(&quot;Sigmoid Function&quot;);</div></pre></td></tr></table></figure>\n<p>Output figure<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c0f683d49f1d3f2d5383d25566d4f490c552645d/__Blog/__Personal%20Understanding/Mathematics/images/tanh.png\" alt=\"tanh\"></p>\n<center>tanh Function</center>\n\n\n\n<h1 id=\"Relative\"><a href=\"#Relative\" class=\"headerlink\" title=\"Relative\"></a>Relative</h1><p><a href=\"https://zhichengmle.github.io/2017/10/30/Mathematics/2017-10-30-Sigmoid%20Function/\">Sigmoid Function</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"Get More Data","date":"2017-11-30T03:23:19.000Z","mathjax":false,"copyright":true,"top":100,"_content":"\n# Get More Data\n\n## 1. Why We Need More Data?\n\nIn many situations (low bias learning model), more data usually means better performance of the model.\n\n## 2. When We Need More Data?\n\nUsually, we should plot the learning curve by using part of the training data (1/10). If we have low bias curve, then we are safely increase the training data to get better machine learning model.\n\n## 3. How to Get More Data?\n\n- Artificial data synthesis (e.g., rotation, crop, change background, etc)\n- Collect and label the data manually\n- Hire other company to label (e.g., Amazon Mechanical Turk)\n\nUsually to make the original data 10 times larger won't take so much effort, but it will make the performance of the model much better.\n","source":"_posts/ML System Tips/2017-11-30-Get More Data.md","raw":"---\ntitle: Get More Data\ndate: 2017-11-30 11:23:19\ncategories: [Machine-Learning-System-Tips]\ntags: [Machine-Learning-System-Tips]\nmathjax: false\ncopyright: true\ntop: 100\n---\n\n# Get More Data\n\n## 1. Why We Need More Data?\n\nIn many situations (low bias learning model), more data usually means better performance of the model.\n\n## 2. When We Need More Data?\n\nUsually, we should plot the learning curve by using part of the training data (1/10). If we have low bias curve, then we are safely increase the training data to get better machine learning model.\n\n## 3. How to Get More Data?\n\n- Artificial data synthesis (e.g., rotation, crop, change background, etc)\n- Collect and label the data manually\n- Hire other company to label (e.g., Amazon Mechanical Turk)\n\nUsually to make the original data 10 times larger won't take so much effort, but it will make the performance of the model much better.\n","slug":"ML System Tips/2017-11-30-Get More Data","published":1,"updated":"2018-04-14T19:42:06.486Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2dd000drwtjrredpr2a","content":"<h1 id=\"Get-More-Data\"><a href=\"#Get-More-Data\" class=\"headerlink\" title=\"Get More Data\"></a>Get More Data</h1><h2 id=\"1-Why-We-Need-More-Data\"><a href=\"#1-Why-We-Need-More-Data\" class=\"headerlink\" title=\"1. Why We Need More Data?\"></a>1. Why We Need More Data?</h2><p>In many situations (low bias learning model), more data usually means better performance of the model.</p>\n<h2 id=\"2-When-We-Need-More-Data\"><a href=\"#2-When-We-Need-More-Data\" class=\"headerlink\" title=\"2. When We Need More Data?\"></a>2. When We Need More Data?</h2><p>Usually, we should plot the learning curve by using part of the training data (1/10). If we have low bias curve, then we are safely increase the training data to get better machine learning model.</p>\n<h2 id=\"3-How-to-Get-More-Data\"><a href=\"#3-How-to-Get-More-Data\" class=\"headerlink\" title=\"3. How to Get More Data?\"></a>3. How to Get More Data?</h2><ul>\n<li>Artificial data synthesis (e.g., rotation, crop, change background, etc)</li>\n<li>Collect and label the data manually</li>\n<li>Hire other company to label (e.g., Amazon Mechanical Turk)</li>\n</ul>\n<p>Usually to make the original data 10 times larger won’t take so much effort, but it will make the performance of the model much better.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Get-More-Data\"><a href=\"#Get-More-Data\" class=\"headerlink\" title=\"Get More Data\"></a>Get More Data</h1><h2 id=\"1-Why-We-Need-More-Data\"><a href=\"#1-Why-We-Need-More-Data\" class=\"headerlink\" title=\"1. Why We Need More Data?\"></a>1. Why We Need More Data?</h2><p>In many situations (low bias learning model), more data usually means better performance of the model.</p>\n<h2 id=\"2-When-We-Need-More-Data\"><a href=\"#2-When-We-Need-More-Data\" class=\"headerlink\" title=\"2. When We Need More Data?\"></a>2. When We Need More Data?</h2><p>Usually, we should plot the learning curve by using part of the training data (1/10). If we have low bias curve, then we are safely increase the training data to get better machine learning model.</p>\n<h2 id=\"3-How-to-Get-More-Data\"><a href=\"#3-How-to-Get-More-Data\" class=\"headerlink\" title=\"3. How to Get More Data?\"></a>3. How to Get More Data?</h2><ul>\n<li>Artificial data synthesis (e.g., rotation, crop, change background, etc)</li>\n<li>Collect and label the data manually</li>\n<li>Hire other company to label (e.g., Amazon Mechanical Turk)</li>\n</ul>\n<p>Usually to make the original data 10 times larger won’t take so much effort, but it will make the performance of the model much better.</p>\n"},{"title":"数据预处理-数据归一化和数据规范化","date":"2017-10-29T07:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n\n# 1. 数据归一化\n### 1.1. 作用\n- 把数据映射到[0,1]的区间中\n- 把有量纲形式变成无量纲形式\n<br>\n<br>\n### 1.2. 算法\n\n#### 1.2.1. 最小-最大归一化\n$Y = \\frac{X - Xmin}{Xmax - Xmin}$\n> 把X的值映射到[0, 1] 的区域中，因为必有X &lt; Xmax，所以分子(X - Xmin) &lt; 分母(Xmax - Xmin)，所以Y在[0,1]之间\n\n<br>\n<br>\n#### 1.2.2. 对数函数归一化\n$Y = log10( X )$\n<br>\n<br>\n\n#### 1.2.3. 反余切函数归一化\n$Y = arctan(x- \\frac{2}{\\pi})$\n\n-----------------------------------\n\n<br>\n<br>\n<br>\n\n# 2. 数据规范化\n### 1.1. 作用\n> 使数据按照一定的比例进行缩放，通过缩放，使得数据映射到特定的空间里面。\n\n### 1.2. 算法\n\n#### 1.2.1. 最小-最大规范化\n$Y = \\frac{X-Xmin}{Xmax - Xmin} - (Xnewmax - Xnewmin) + Xnewmin$\n> 把X的值映射到[Xnewmin, Xnewmax] 的区域中。\n\n <br>\n<br>\n#### 1.2.2. Z分数(z-score)规范化（零均值规范化）\n${Y}=\\frac{X-\\mu }{\\sigma }$\n> 这种方法最大的优势在于，不需要知道数据集的最大值，最小值。离群点对结果影响较低。\n\n<br>\n<br>\n\n#### 1.2.3. Sigmoid函数\n${Y}=\\frac{1}{1+{e}^{-x}}$\n> Sigmoid函数是一个具有S形曲线的函数，当数据趋向于正无穷和负无穷的时候，映射出来的值就会无限趋向于1和0，\n\n<br>\n<br>\n\n#### 1.2.4. 小数定标规范化\n$Y = \\frac{X}{10^n} (n 为使得Ymax < 1 的最小整数)$\n> 通过移动小数点的位置，进行规范化\n\n<br>\n<br>\n\n#### 1.2.5. 模糊量化规范化\n$Y = \\frac{1}{2} + \\frac{1}{2} - sin[\\frac{\\pi}{Xmax - Xmin}-(X - \\frac{Xmax - Xmin}{2})]$\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/2017-10-29-数据预处理-数据归一化和数据规范化.md","raw":"---\ntitle: 数据预处理-数据归一化和数据规范化\ndate: 2017-10-29 15:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n\n# 1. 数据归一化\n### 1.1. 作用\n- 把数据映射到[0,1]的区间中\n- 把有量纲形式变成无量纲形式\n<br>\n<br>\n### 1.2. 算法\n\n#### 1.2.1. 最小-最大归一化\n$Y = \\frac{X - Xmin}{Xmax - Xmin}$\n> 把X的值映射到[0, 1] 的区域中，因为必有X &lt; Xmax，所以分子(X - Xmin) &lt; 分母(Xmax - Xmin)，所以Y在[0,1]之间\n\n<br>\n<br>\n#### 1.2.2. 对数函数归一化\n$Y = log10( X )$\n<br>\n<br>\n\n#### 1.2.3. 反余切函数归一化\n$Y = arctan(x- \\frac{2}{\\pi})$\n\n-----------------------------------\n\n<br>\n<br>\n<br>\n\n# 2. 数据规范化\n### 1.1. 作用\n> 使数据按照一定的比例进行缩放，通过缩放，使得数据映射到特定的空间里面。\n\n### 1.2. 算法\n\n#### 1.2.1. 最小-最大规范化\n$Y = \\frac{X-Xmin}{Xmax - Xmin} - (Xnewmax - Xnewmin) + Xnewmin$\n> 把X的值映射到[Xnewmin, Xnewmax] 的区域中。\n\n <br>\n<br>\n#### 1.2.2. Z分数(z-score)规范化（零均值规范化）\n${Y}=\\frac{X-\\mu }{\\sigma }$\n> 这种方法最大的优势在于，不需要知道数据集的最大值，最小值。离群点对结果影响较低。\n\n<br>\n<br>\n\n#### 1.2.3. Sigmoid函数\n${Y}=\\frac{1}{1+{e}^{-x}}$\n> Sigmoid函数是一个具有S形曲线的函数，当数据趋向于正无穷和负无穷的时候，映射出来的值就会无限趋向于1和0，\n\n<br>\n<br>\n\n#### 1.2.4. 小数定标规范化\n$Y = \\frac{X}{10^n} (n 为使得Ymax < 1 的最小整数)$\n> 通过移动小数点的位置，进行规范化\n\n<br>\n<br>\n\n#### 1.2.5. 模糊量化规范化\n$Y = \\frac{1}{2} + \\frac{1}{2} - sin[\\frac{\\pi}{Xmax - Xmin}-(X - \\frac{Xmax - Xmin}{2})]$\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/2017-10-29-数据预处理-数据归一化和数据规范化","published":1,"updated":"2018-04-14T19:42:06.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2de000erwtj1whgwb7c","content":"<h1 id=\"1-数据归一化\"><a href=\"#1-数据归一化\" class=\"headerlink\" title=\"1. 数据归一化\"></a>1. 数据归一化</h1><h3 id=\"1-1-作用\"><a href=\"#1-1-作用\" class=\"headerlink\" title=\"1.1. 作用\"></a>1.1. 作用</h3><ul>\n<li>把数据映射到[0,1]的区间中</li>\n<li>把有量纲形式变成无量纲形式<br><br><br><br><h3 id=\"1-2-算法\"><a href=\"#1-2-算法\" class=\"headerlink\" title=\"1.2. 算法\"></a>1.2. 算法</h3></li>\n</ul>\n<h4 id=\"1-2-1-最小-最大归一化\"><a href=\"#1-2-1-最小-最大归一化\" class=\"headerlink\" title=\"1.2.1. 最小-最大归一化\"></a>1.2.1. 最小-最大归一化</h4><p>$Y = \\frac{X - Xmin}{Xmax - Xmin}$</p>\n<blockquote>\n<p>把X的值映射到[0, 1] 的区域中，因为必有X &lt; Xmax，所以分子(X - Xmin) &lt; 分母(Xmax - Xmin)，所以Y在[0,1]之间</p>\n</blockquote>\n<p><br><br><br></p>\n<h4 id=\"1-2-2-对数函数归一化\"><a href=\"#1-2-2-对数函数归一化\" class=\"headerlink\" title=\"1.2.2. 对数函数归一化\"></a>1.2.2. 对数函数归一化</h4><p>$Y = log10( X )$<br><br><br><br></p>\n<h4 id=\"1-2-3-反余切函数归一化\"><a href=\"#1-2-3-反余切函数归一化\" class=\"headerlink\" title=\"1.2.3. 反余切函数归一化\"></a>1.2.3. 反余切函数归一化</h4><p>$Y = arctan(x- \\frac{2}{\\pi})$</p>\n<hr>\n<p><br><br><br><br><br></p>\n<h1 id=\"2-数据规范化\"><a href=\"#2-数据规范化\" class=\"headerlink\" title=\"2. 数据规范化\"></a>2. 数据规范化</h1><h3 id=\"1-1-作用-1\"><a href=\"#1-1-作用-1\" class=\"headerlink\" title=\"1.1. 作用\"></a>1.1. 作用</h3><blockquote>\n<p>使数据按照一定的比例进行缩放，通过缩放，使得数据映射到特定的空间里面。</p>\n</blockquote>\n<h3 id=\"1-2-算法-1\"><a href=\"#1-2-算法-1\" class=\"headerlink\" title=\"1.2. 算法\"></a>1.2. 算法</h3><h4 id=\"1-2-1-最小-最大规范化\"><a href=\"#1-2-1-最小-最大规范化\" class=\"headerlink\" title=\"1.2.1. 最小-最大规范化\"></a>1.2.1. 最小-最大规范化</h4><p>$Y = \\frac{X-Xmin}{Xmax - Xmin} - (Xnewmax - Xnewmin) + Xnewmin$</p>\n<blockquote>\n<p>把X的值映射到[Xnewmin, Xnewmax] 的区域中。</p>\n</blockquote>\n<p> <br><br><br></p>\n<h4 id=\"1-2-2-Z分数-z-score-规范化（零均值规范化）\"><a href=\"#1-2-2-Z分数-z-score-规范化（零均值规范化）\" class=\"headerlink\" title=\"1.2.2. Z分数(z-score)规范化（零均值规范化）\"></a>1.2.2. Z分数(z-score)规范化（零均值规范化）</h4><p>${Y}=\\frac{X-\\mu }{\\sigma }$</p>\n<blockquote>\n<p>这种方法最大的优势在于，不需要知道数据集的最大值，最小值。离群点对结果影响较低。</p>\n</blockquote>\n<p><br><br><br></p>\n<h4 id=\"1-2-3-Sigmoid函数\"><a href=\"#1-2-3-Sigmoid函数\" class=\"headerlink\" title=\"1.2.3. Sigmoid函数\"></a>1.2.3. Sigmoid函数</h4><p>${Y}=\\frac{1}{1+{e}^{-x}}$</p>\n<blockquote>\n<p>Sigmoid函数是一个具有S形曲线的函数，当数据趋向于正无穷和负无穷的时候，映射出来的值就会无限趋向于1和0，</p>\n</blockquote>\n<p><br><br><br></p>\n<h4 id=\"1-2-4-小数定标规范化\"><a href=\"#1-2-4-小数定标规范化\" class=\"headerlink\" title=\"1.2.4. 小数定标规范化\"></a>1.2.4. 小数定标规范化</h4><p>$Y = \\frac{X}{10^n} (n 为使得Ymax &lt; 1 的最小整数)$</p>\n<blockquote>\n<p>通过移动小数点的位置，进行规范化</p>\n</blockquote>\n<p><br><br><br></p>\n<h4 id=\"1-2-5-模糊量化规范化\"><a href=\"#1-2-5-模糊量化规范化\" class=\"headerlink\" title=\"1.2.5. 模糊量化规范化\"></a>1.2.5. 模糊量化规范化</h4><p>$Y = \\frac{1}{2} + \\frac{1}{2} - sin[\\frac{\\pi}{Xmax - Xmin}-(X - \\frac{Xmax - Xmin}{2})]$</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"1-数据归一化\"><a href=\"#1-数据归一化\" class=\"headerlink\" title=\"1. 数据归一化\"></a>1. 数据归一化</h1><h3 id=\"1-1-作用\"><a href=\"#1-1-作用\" class=\"headerlink\" title=\"1.1. 作用\"></a>1.1. 作用</h3><ul>\n<li>把数据映射到[0,1]的区间中</li>\n<li>把有量纲形式变成无量纲形式<br><br><br><br><h3 id=\"1-2-算法\"><a href=\"#1-2-算法\" class=\"headerlink\" title=\"1.2. 算法\"></a>1.2. 算法</h3></li>\n</ul>\n<h4 id=\"1-2-1-最小-最大归一化\"><a href=\"#1-2-1-最小-最大归一化\" class=\"headerlink\" title=\"1.2.1. 最小-最大归一化\"></a>1.2.1. 最小-最大归一化</h4><p>$Y = \\frac{X - Xmin}{Xmax - Xmin}$</p>\n<blockquote>\n<p>把X的值映射到[0, 1] 的区域中，因为必有X &lt; Xmax，所以分子(X - Xmin) &lt; 分母(Xmax - Xmin)，所以Y在[0,1]之间</p>\n</blockquote>\n<p><br><br><br></p>\n<h4 id=\"1-2-2-对数函数归一化\"><a href=\"#1-2-2-对数函数归一化\" class=\"headerlink\" title=\"1.2.2. 对数函数归一化\"></a>1.2.2. 对数函数归一化</h4><p>$Y = log10( X )$<br><br><br><br></p>\n<h4 id=\"1-2-3-反余切函数归一化\"><a href=\"#1-2-3-反余切函数归一化\" class=\"headerlink\" title=\"1.2.3. 反余切函数归一化\"></a>1.2.3. 反余切函数归一化</h4><p>$Y = arctan(x- \\frac{2}{\\pi})$</p>\n<hr>\n<p><br><br><br><br><br></p>\n<h1 id=\"2-数据规范化\"><a href=\"#2-数据规范化\" class=\"headerlink\" title=\"2. 数据规范化\"></a>2. 数据规范化</h1><h3 id=\"1-1-作用-1\"><a href=\"#1-1-作用-1\" class=\"headerlink\" title=\"1.1. 作用\"></a>1.1. 作用</h3><blockquote>\n<p>使数据按照一定的比例进行缩放，通过缩放，使得数据映射到特定的空间里面。</p>\n</blockquote>\n<h3 id=\"1-2-算法-1\"><a href=\"#1-2-算法-1\" class=\"headerlink\" title=\"1.2. 算法\"></a>1.2. 算法</h3><h4 id=\"1-2-1-最小-最大规范化\"><a href=\"#1-2-1-最小-最大规范化\" class=\"headerlink\" title=\"1.2.1. 最小-最大规范化\"></a>1.2.1. 最小-最大规范化</h4><p>$Y = \\frac{X-Xmin}{Xmax - Xmin} - (Xnewmax - Xnewmin) + Xnewmin$</p>\n<blockquote>\n<p>把X的值映射到[Xnewmin, Xnewmax] 的区域中。</p>\n</blockquote>\n<p> <br><br><br></p>\n<h4 id=\"1-2-2-Z分数-z-score-规范化（零均值规范化）\"><a href=\"#1-2-2-Z分数-z-score-规范化（零均值规范化）\" class=\"headerlink\" title=\"1.2.2. Z分数(z-score)规范化（零均值规范化）\"></a>1.2.2. Z分数(z-score)规范化（零均值规范化）</h4><p>${Y}=\\frac{X-\\mu }{\\sigma }$</p>\n<blockquote>\n<p>这种方法最大的优势在于，不需要知道数据集的最大值，最小值。离群点对结果影响较低。</p>\n</blockquote>\n<p><br><br><br></p>\n<h4 id=\"1-2-3-Sigmoid函数\"><a href=\"#1-2-3-Sigmoid函数\" class=\"headerlink\" title=\"1.2.3. Sigmoid函数\"></a>1.2.3. Sigmoid函数</h4><p>${Y}=\\frac{1}{1+{e}^{-x}}$</p>\n<blockquote>\n<p>Sigmoid函数是一个具有S形曲线的函数，当数据趋向于正无穷和负无穷的时候，映射出来的值就会无限趋向于1和0，</p>\n</blockquote>\n<p><br><br><br></p>\n<h4 id=\"1-2-4-小数定标规范化\"><a href=\"#1-2-4-小数定标规范化\" class=\"headerlink\" title=\"1.2.4. 小数定标规范化\"></a>1.2.4. 小数定标规范化</h4><p>$Y = \\frac{X}{10^n} (n 为使得Ymax &lt; 1 的最小整数)$</p>\n<blockquote>\n<p>通过移动小数点的位置，进行规范化</p>\n</blockquote>\n<p><br><br><br></p>\n<h4 id=\"1-2-5-模糊量化规范化\"><a href=\"#1-2-5-模糊量化规范化\" class=\"headerlink\" title=\"1.2.5. 模糊量化规范化\"></a>1.2.5. 模糊量化规范化</h4><p>$Y = \\frac{1}{2} + \\frac{1}{2} - sin[\\frac{\\pi}{Xmax - Xmin}-(X - \\frac{Xmax - Xmin}{2})]$</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"Matrix and Vector Transformation","date":"2017-10-28T05:23:19.000Z","mathjax":false,"copyright":true,"top":100,"_content":"\n\n\n## 1. When to Transform?\n在一些神经网络的一些优化算法中，要求传入的是向量来进行优化。但是权值却是矩阵，所以需要进行转换，在处理之后再转回来。\n所以我们需要学习如何进行Matrix 和 Vector 的转换\n\n\n------------------------------------------\n<br>\n<br>\n\n\n## 2. How to Transform?\n### 1) Octave Implement\n> 在Octave 中我们需要用的 \"[]\" 把矩阵转成向量；再通过 Reshape 函数把向量转换回数组\n\n1.分别使用 ones()函数产生2个等大的矩阵，作为输入层，隐藏层。\n```\ntheta1 = ones(3,4)\n\n> Output:\ntheta1 =\n\n  1   1   1   1\n  1   1   1   1\n  1   1   1   1\n\n\n\ntheta2 = 3*ones(3,4)\n\n> Output:\ntheta2 =\n  3   3   3   3\n  3   3   3   3\n  3   3   3   3\n```\n2. 同样使用ones()函数，产生输出层的theta\n\n```\ntheta3 = 4*ones(1,4)\n\n> Output:\ntheta3 =\n  4   4   4   4\n```\n\n3.通过 \"[]\" 来合成矩阵，注意括号里面要加上 : 符号代码把当前矩阵变成向量输出-->就可以得到我们需要的向量了\n\n```\nthetaVec = [theta1(:); theta2(:); theta3(:)]\n\n> Output:\nthetaVec =\n\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  4\n  4\n  4\n  4\n```\n\n4.再通过Reshape函数把向量转换回矩阵：要注意向量的index。\n\n```\nnewTheta1 = reshape(thetaVec(1:12),3,4)\n\n> Output:\nnewTheta1 =\n  1   1   1   1\n  1   1   1   1\n  1   1   1   1\n\n\n\n\nnewTheta2 = reshape(thetaVec(13:24),3,4)\n\n> Output:\nnewTheta2 =\n\n  3   3   3   3\n  3   3   3   3\n  3   3   3   3\n\n\n\nnewTheta3 = reshape(thetaVec(25:28),1,4)\n\n> Output:\nnewTheta3 =\n  4   4   4   4\n```\n\n\n\n### 2) Python Implement\n> @TODO\n\n<br>\n<br>\n------------------------------------------\n\n","source":"_posts/Mathematics/2017-10-28-MatrixToVector.md","raw":"---\ntitle: Matrix and Vector Transformation\ndate: 2017-10-28 13:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics]\nmathjax: false\ncopyright: true\ntop: 100\n---\n\n\n\n## 1. When to Transform?\n在一些神经网络的一些优化算法中，要求传入的是向量来进行优化。但是权值却是矩阵，所以需要进行转换，在处理之后再转回来。\n所以我们需要学习如何进行Matrix 和 Vector 的转换\n\n\n------------------------------------------\n<br>\n<br>\n\n\n## 2. How to Transform?\n### 1) Octave Implement\n> 在Octave 中我们需要用的 \"[]\" 把矩阵转成向量；再通过 Reshape 函数把向量转换回数组\n\n1.分别使用 ones()函数产生2个等大的矩阵，作为输入层，隐藏层。\n```\ntheta1 = ones(3,4)\n\n> Output:\ntheta1 =\n\n  1   1   1   1\n  1   1   1   1\n  1   1   1   1\n\n\n\ntheta2 = 3*ones(3,4)\n\n> Output:\ntheta2 =\n  3   3   3   3\n  3   3   3   3\n  3   3   3   3\n```\n2. 同样使用ones()函数，产生输出层的theta\n\n```\ntheta3 = 4*ones(1,4)\n\n> Output:\ntheta3 =\n  4   4   4   4\n```\n\n3.通过 \"[]\" 来合成矩阵，注意括号里面要加上 : 符号代码把当前矩阵变成向量输出-->就可以得到我们需要的向量了\n\n```\nthetaVec = [theta1(:); theta2(:); theta3(:)]\n\n> Output:\nthetaVec =\n\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  3\n  4\n  4\n  4\n  4\n```\n\n4.再通过Reshape函数把向量转换回矩阵：要注意向量的index。\n\n```\nnewTheta1 = reshape(thetaVec(1:12),3,4)\n\n> Output:\nnewTheta1 =\n  1   1   1   1\n  1   1   1   1\n  1   1   1   1\n\n\n\n\nnewTheta2 = reshape(thetaVec(13:24),3,4)\n\n> Output:\nnewTheta2 =\n\n  3   3   3   3\n  3   3   3   3\n  3   3   3   3\n\n\n\nnewTheta3 = reshape(thetaVec(25:28),1,4)\n\n> Output:\nnewTheta3 =\n  4   4   4   4\n```\n\n\n\n### 2) Python Implement\n> @TODO\n\n<br>\n<br>\n------------------------------------------\n\n","slug":"Mathematics/2017-10-28-MatrixToVector","published":1,"updated":"2018-04-14T19:42:06.487Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2df000frwtjblolojod","content":"<h2 id=\"1-When-to-Transform\"><a href=\"#1-When-to-Transform\" class=\"headerlink\" title=\"1. When to Transform?\"></a>1. When to Transform?</h2><p>在一些神经网络的一些优化算法中，要求传入的是向量来进行优化。但是权值却是矩阵，所以需要进行转换，在处理之后再转回来。<br>所以我们需要学习如何进行Matrix 和 Vector 的转换</p>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"2-How-to-Transform\"><a href=\"#2-How-to-Transform\" class=\"headerlink\" title=\"2. How to Transform?\"></a>2. How to Transform?</h2><h3 id=\"1-Octave-Implement\"><a href=\"#1-Octave-Implement\" class=\"headerlink\" title=\"1) Octave Implement\"></a>1) Octave Implement</h3><blockquote>\n<p>在Octave 中我们需要用的 “[]” 把矩阵转成向量；再通过 Reshape 函数把向量转换回数组</p>\n</blockquote>\n<p>1.分别使用 ones()函数产生2个等大的矩阵，作为输入层，隐藏层。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">theta1 = ones(3,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">theta1 =</div><div class=\"line\"></div><div class=\"line\">  1   1   1   1</div><div class=\"line\">  1   1   1   1</div><div class=\"line\">  1   1   1   1</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">theta2 = 3*ones(3,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">theta2 =</div><div class=\"line\">  3   3   3   3</div><div class=\"line\">  3   3   3   3</div><div class=\"line\">  3   3   3   3</div></pre></td></tr></table></figure></p>\n<ol>\n<li>同样使用ones()函数，产生输出层的theta</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">theta3 = 4*ones(1,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">theta3 =</div><div class=\"line\">  4   4   4   4</div></pre></td></tr></table></figure>\n<p>3.通过 “[]” 来合成矩阵，注意括号里面要加上 : 符号代码把当前矩阵变成向量输出—&gt;就可以得到我们需要的向量了</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">thetaVec = [theta1(:); theta2(:); theta3(:)]</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">thetaVec =</div><div class=\"line\"></div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  4</div><div class=\"line\">  4</div><div class=\"line\">  4</div><div class=\"line\">  4</div></pre></td></tr></table></figure>\n<p>4.再通过Reshape函数把向量转换回矩阵：要注意向量的index。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">newTheta1 = reshape(thetaVec(1:12),3,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">newTheta1 =</div><div class=\"line\">  1   1   1   1</div><div class=\"line\">  1   1   1   1</div><div class=\"line\">  1   1   1   1</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">newTheta2 = reshape(thetaVec(13:24),3,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">newTheta2 =</div><div class=\"line\"></div><div class=\"line\">  3   3   3   3</div><div class=\"line\">  3   3   3   3</div><div class=\"line\">  3   3   3   3</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">newTheta3 = reshape(thetaVec(25:28),1,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">newTheta3 =</div><div class=\"line\">  4   4   4   4</div></pre></td></tr></table></figure>\n<h3 id=\"2-Python-Implement\"><a href=\"#2-Python-Implement\" class=\"headerlink\" title=\"2) Python Implement\"></a>2) Python Implement</h3><blockquote>\n<p>@TODO</p>\n</blockquote>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-When-to-Transform\"><a href=\"#1-When-to-Transform\" class=\"headerlink\" title=\"1. When to Transform?\"></a>1. When to Transform?</h2><p>在一些神经网络的一些优化算法中，要求传入的是向量来进行优化。但是权值却是矩阵，所以需要进行转换，在处理之后再转回来。<br>所以我们需要学习如何进行Matrix 和 Vector 的转换</p>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"2-How-to-Transform\"><a href=\"#2-How-to-Transform\" class=\"headerlink\" title=\"2. How to Transform?\"></a>2. How to Transform?</h2><h3 id=\"1-Octave-Implement\"><a href=\"#1-Octave-Implement\" class=\"headerlink\" title=\"1) Octave Implement\"></a>1) Octave Implement</h3><blockquote>\n<p>在Octave 中我们需要用的 “[]” 把矩阵转成向量；再通过 Reshape 函数把向量转换回数组</p>\n</blockquote>\n<p>1.分别使用 ones()函数产生2个等大的矩阵，作为输入层，隐藏层。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">theta1 = ones(3,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">theta1 =</div><div class=\"line\"></div><div class=\"line\">  1   1   1   1</div><div class=\"line\">  1   1   1   1</div><div class=\"line\">  1   1   1   1</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">theta2 = 3*ones(3,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">theta2 =</div><div class=\"line\">  3   3   3   3</div><div class=\"line\">  3   3   3   3</div><div class=\"line\">  3   3   3   3</div></pre></td></tr></table></figure></p>\n<ol>\n<li>同样使用ones()函数，产生输出层的theta</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">theta3 = 4*ones(1,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">theta3 =</div><div class=\"line\">  4   4   4   4</div></pre></td></tr></table></figure>\n<p>3.通过 “[]” 来合成矩阵，注意括号里面要加上 : 符号代码把当前矩阵变成向量输出—&gt;就可以得到我们需要的向量了</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">thetaVec = [theta1(:); theta2(:); theta3(:)]</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">thetaVec =</div><div class=\"line\"></div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  1</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  3</div><div class=\"line\">  4</div><div class=\"line\">  4</div><div class=\"line\">  4</div><div class=\"line\">  4</div></pre></td></tr></table></figure>\n<p>4.再通过Reshape函数把向量转换回矩阵：要注意向量的index。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">newTheta1 = reshape(thetaVec(1:12),3,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">newTheta1 =</div><div class=\"line\">  1   1   1   1</div><div class=\"line\">  1   1   1   1</div><div class=\"line\">  1   1   1   1</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">newTheta2 = reshape(thetaVec(13:24),3,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">newTheta2 =</div><div class=\"line\"></div><div class=\"line\">  3   3   3   3</div><div class=\"line\">  3   3   3   3</div><div class=\"line\">  3   3   3   3</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">newTheta3 = reshape(thetaVec(25:28),1,4)</div><div class=\"line\"></div><div class=\"line\">&gt; Output:</div><div class=\"line\">newTheta3 =</div><div class=\"line\">  4   4   4   4</div></pre></td></tr></table></figure>\n<h3 id=\"2-Python-Implement\"><a href=\"#2-Python-Implement\" class=\"headerlink\" title=\"2) Python Implement\"></a>2) Python Implement</h3><blockquote>\n<p>@TODO</p>\n</blockquote>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"数据预处理-数据分割","date":"2017-10-31T04:23:19.000Z","mathjax":false,"copyright":true,"top":100,"_content":"\n\n# 1. 为什么是数据分割\n通过把数据集 (Dataset) 中的数据内容分割成训练集 (Train Set) 和测试集 (Test Set)，用训练集来训练模型，再通过测试集来测试模型的性能，如果测试通过，才会考虑投放在实际应用中。\n\n------------------\n\n\n\n# 2. 数据分割的注意事项\n## 2.1. 保证数据的随机性\n如果数据分割是按照一定的规律进行的话，那么训练出来的模型也会被“模式化”，一旦遇到特殊值，就会判断出错。\n\n## 2.2. 保证训练集的大小\n训练的数量越大，模型的准确率一般会更高。如果训练集太小，模型就“不稳定”，在测试中遇到特殊值，就容易得到错误的结果。\n\n------------------\n\n\n# 3. Python实现方法\n> Github代码：https://github.com/JasonDean-1/MachineLearningDemo/blob/master/DataPreprocessing/1.DataSplit.py\n## 3.1. 调用sklearn的包\n\n```\n# --- coding: utf-8 ---\n\n\"\"\"\nPackage Version(s):\n    sklearn: 0.18\n\"\"\"\n\nfrom sklearn.datasets import load_digits\n\n# load in datasets\ndatasets = load_digits()\n# Print out the datashape.\nprint \"The length of dataset is: \", datasets.data.shape\n# Ans = (1797L, 64L)\n\nfrom sklearn.cross_validation import train_test_split\n# Split the data\nx_train, x_test, y_train, y_test = train_test_split( datasets.data, datasets.target, test_size = 0.25, random_state = 42 )\n# Print out the datashape.\nprint \"The length of train set is: \", y_train.shape\n# Ans = (1347L,)\nprint \"The length of test set is: \", y_test.shape\n# Ans = (450L,)\n\n\n----------\nOutput:\n# The length of dataset is: (1797L, 64L)\n# The length of train set is: Ans = (1347L,)\n# The length of test set is: Ans = (450L,)\n```\n\n<br>\n<br>\n<br>\n\n\n## 3.2. 通过循环来分割数据\n\n```\n# --- coding: utf-8 ---\n\n\"\"\"\nPackage Version(s):\n    sklearn: 0.18\n\"\"\"\n\nfrom sklearn.datasets import load_digits\n\n# load in datasets\ndatasets = load_digits()\n# Print out the datashape.\nprint \"The length of dataset is: \", datasets.data.shape\n# Ans = (1797L, 64L)\nx_train = []\nx_test = []\ny_train = []\ny_test = []\n\n# Split the data\nx_test = [datasets.data[i] for i in range(len(datasets.data)) if i%4 == 0]\nx_train = [datasets.data[i] for i in range(len(datasets.data)) if i%4 != 0]\ny_test = [datasets.target[i] for i in range(len(datasets.data)) if i%4 == 0]\ny_train = [datasets.target[i] for i in range(len(datasets.data)) if i%4 != 0]\n\n# Print out the datashape.\nprint \"The length of train set is: \", len( y_train )\n# Ans = 1347\nprint \"The length of test set is: \", len( y_test )\n# Ans = 450\n\n\n----------\nOutput:\n# The length of dataset is: (1797L, 64L)\n# The length of train set is: Ans = 1347\n# The length of test set is: Ans = 450\n```\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/2017-10-31-数据分割.md","raw":"---\ntitle: 数据预处理-数据分割\ndate: 2017-10-31 12:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics]\nmathjax: false\ncopyright: true\ntop: 100\n---\n\n\n# 1. 为什么是数据分割\n通过把数据集 (Dataset) 中的数据内容分割成训练集 (Train Set) 和测试集 (Test Set)，用训练集来训练模型，再通过测试集来测试模型的性能，如果测试通过，才会考虑投放在实际应用中。\n\n------------------\n\n\n\n# 2. 数据分割的注意事项\n## 2.1. 保证数据的随机性\n如果数据分割是按照一定的规律进行的话，那么训练出来的模型也会被“模式化”，一旦遇到特殊值，就会判断出错。\n\n## 2.2. 保证训练集的大小\n训练的数量越大，模型的准确率一般会更高。如果训练集太小，模型就“不稳定”，在测试中遇到特殊值，就容易得到错误的结果。\n\n------------------\n\n\n# 3. Python实现方法\n> Github代码：https://github.com/JasonDean-1/MachineLearningDemo/blob/master/DataPreprocessing/1.DataSplit.py\n## 3.1. 调用sklearn的包\n\n```\n# --- coding: utf-8 ---\n\n\"\"\"\nPackage Version(s):\n    sklearn: 0.18\n\"\"\"\n\nfrom sklearn.datasets import load_digits\n\n# load in datasets\ndatasets = load_digits()\n# Print out the datashape.\nprint \"The length of dataset is: \", datasets.data.shape\n# Ans = (1797L, 64L)\n\nfrom sklearn.cross_validation import train_test_split\n# Split the data\nx_train, x_test, y_train, y_test = train_test_split( datasets.data, datasets.target, test_size = 0.25, random_state = 42 )\n# Print out the datashape.\nprint \"The length of train set is: \", y_train.shape\n# Ans = (1347L,)\nprint \"The length of test set is: \", y_test.shape\n# Ans = (450L,)\n\n\n----------\nOutput:\n# The length of dataset is: (1797L, 64L)\n# The length of train set is: Ans = (1347L,)\n# The length of test set is: Ans = (450L,)\n```\n\n<br>\n<br>\n<br>\n\n\n## 3.2. 通过循环来分割数据\n\n```\n# --- coding: utf-8 ---\n\n\"\"\"\nPackage Version(s):\n    sklearn: 0.18\n\"\"\"\n\nfrom sklearn.datasets import load_digits\n\n# load in datasets\ndatasets = load_digits()\n# Print out the datashape.\nprint \"The length of dataset is: \", datasets.data.shape\n# Ans = (1797L, 64L)\nx_train = []\nx_test = []\ny_train = []\ny_test = []\n\n# Split the data\nx_test = [datasets.data[i] for i in range(len(datasets.data)) if i%4 == 0]\nx_train = [datasets.data[i] for i in range(len(datasets.data)) if i%4 != 0]\ny_test = [datasets.target[i] for i in range(len(datasets.data)) if i%4 == 0]\ny_train = [datasets.target[i] for i in range(len(datasets.data)) if i%4 != 0]\n\n# Print out the datashape.\nprint \"The length of train set is: \", len( y_train )\n# Ans = 1347\nprint \"The length of test set is: \", len( y_test )\n# Ans = 450\n\n\n----------\nOutput:\n# The length of dataset is: (1797L, 64L)\n# The length of train set is: Ans = 1347\n# The length of test set is: Ans = 450\n```\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/2017-10-31-数据分割","published":1,"updated":"2018-04-14T19:42:06.489Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2dh000jrwtj6ajvr2if","content":"<h1 id=\"1-为什么是数据分割\"><a href=\"#1-为什么是数据分割\" class=\"headerlink\" title=\"1. 为什么是数据分割\"></a>1. 为什么是数据分割</h1><p>通过把数据集 (Dataset) 中的数据内容分割成训练集 (Train Set) 和测试集 (Test Set)，用训练集来训练模型，再通过测试集来测试模型的性能，如果测试通过，才会考虑投放在实际应用中。</p>\n<hr>\n<h1 id=\"2-数据分割的注意事项\"><a href=\"#2-数据分割的注意事项\" class=\"headerlink\" title=\"2. 数据分割的注意事项\"></a>2. 数据分割的注意事项</h1><h2 id=\"2-1-保证数据的随机性\"><a href=\"#2-1-保证数据的随机性\" class=\"headerlink\" title=\"2.1. 保证数据的随机性\"></a>2.1. 保证数据的随机性</h2><p>如果数据分割是按照一定的规律进行的话，那么训练出来的模型也会被“模式化”，一旦遇到特殊值，就会判断出错。</p>\n<h2 id=\"2-2-保证训练集的大小\"><a href=\"#2-2-保证训练集的大小\" class=\"headerlink\" title=\"2.2. 保证训练集的大小\"></a>2.2. 保证训练集的大小</h2><p>训练的数量越大，模型的准确率一般会更高。如果训练集太小，模型就“不稳定”，在测试中遇到特殊值，就容易得到错误的结果。</p>\n<hr>\n<h1 id=\"3-Python实现方法\"><a href=\"#3-Python实现方法\" class=\"headerlink\" title=\"3. Python实现方法\"></a>3. Python实现方法</h1><blockquote>\n<p>Github代码：<a href=\"https://github.com/JasonDean-1/MachineLearningDemo/blob/master/DataPreprocessing/1.DataSplit.py\" target=\"_blank\" rel=\"external\">https://github.com/JasonDean-1/MachineLearningDemo/blob/master/DataPreprocessing/1.DataSplit.py</a></p>\n<h2 id=\"3-1-调用sklearn的包\"><a href=\"#3-1-调用sklearn的包\" class=\"headerlink\" title=\"3.1. 调用sklearn的包\"></a>3.1. 调用sklearn的包</h2></blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\"># --- coding: utf-8 ---</div><div class=\"line\"></div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\">Package Version(s):</div><div class=\"line\">    sklearn: 0.18</div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\"></div><div class=\"line\">from sklearn.datasets import load_digits</div><div class=\"line\"></div><div class=\"line\"># load in datasets</div><div class=\"line\">datasets = load_digits()</div><div class=\"line\"># Print out the datashape.</div><div class=\"line\">print &quot;The length of dataset is: &quot;, datasets.data.shape</div><div class=\"line\"># Ans = (1797L, 64L)</div><div class=\"line\"></div><div class=\"line\">from sklearn.cross_validation import train_test_split</div><div class=\"line\"># Split the data</div><div class=\"line\">x_train, x_test, y_train, y_test = train_test_split( datasets.data, datasets.target, test_size = 0.25, random_state = 42 )</div><div class=\"line\"># Print out the datashape.</div><div class=\"line\">print &quot;The length of train set is: &quot;, y_train.shape</div><div class=\"line\"># Ans = (1347L,)</div><div class=\"line\">print &quot;The length of test set is: &quot;, y_test.shape</div><div class=\"line\"># Ans = (450L,)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">----------</div><div class=\"line\">Output:</div><div class=\"line\"># The length of dataset is: (1797L, 64L)</div><div class=\"line\"># The length of train set is: Ans = (1347L,)</div><div class=\"line\"># The length of test set is: Ans = (450L,)</div></pre></td></tr></table></figure>\n<p><br><br><br><br><br></p>\n<h2 id=\"3-2-通过循环来分割数据\"><a href=\"#3-2-通过循环来分割数据\" class=\"headerlink\" title=\"3.2. 通过循环来分割数据\"></a>3.2. 通过循环来分割数据</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\"># --- coding: utf-8 ---</div><div class=\"line\"></div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\">Package Version(s):</div><div class=\"line\">    sklearn: 0.18</div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\"></div><div class=\"line\">from sklearn.datasets import load_digits</div><div class=\"line\"></div><div class=\"line\"># load in datasets</div><div class=\"line\">datasets = load_digits()</div><div class=\"line\"># Print out the datashape.</div><div class=\"line\">print &quot;The length of dataset is: &quot;, datasets.data.shape</div><div class=\"line\"># Ans = (1797L, 64L)</div><div class=\"line\">x_train = []</div><div class=\"line\">x_test = []</div><div class=\"line\">y_train = []</div><div class=\"line\">y_test = []</div><div class=\"line\"></div><div class=\"line\"># Split the data</div><div class=\"line\">x_test = [datasets.data[i] for i in range(len(datasets.data)) if i%4 == 0]</div><div class=\"line\">x_train = [datasets.data[i] for i in range(len(datasets.data)) if i%4 != 0]</div><div class=\"line\">y_test = [datasets.target[i] for i in range(len(datasets.data)) if i%4 == 0]</div><div class=\"line\">y_train = [datasets.target[i] for i in range(len(datasets.data)) if i%4 != 0]</div><div class=\"line\"></div><div class=\"line\"># Print out the datashape.</div><div class=\"line\">print &quot;The length of train set is: &quot;, len( y_train )</div><div class=\"line\"># Ans = 1347</div><div class=\"line\">print &quot;The length of test set is: &quot;, len( y_test )</div><div class=\"line\"># Ans = 450</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">----------</div><div class=\"line\">Output:</div><div class=\"line\"># The length of dataset is: (1797L, 64L)</div><div class=\"line\"># The length of train set is: Ans = 1347</div><div class=\"line\"># The length of test set is: Ans = 450</div></pre></td></tr></table></figure>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"1-为什么是数据分割\"><a href=\"#1-为什么是数据分割\" class=\"headerlink\" title=\"1. 为什么是数据分割\"></a>1. 为什么是数据分割</h1><p>通过把数据集 (Dataset) 中的数据内容分割成训练集 (Train Set) 和测试集 (Test Set)，用训练集来训练模型，再通过测试集来测试模型的性能，如果测试通过，才会考虑投放在实际应用中。</p>\n<hr>\n<h1 id=\"2-数据分割的注意事项\"><a href=\"#2-数据分割的注意事项\" class=\"headerlink\" title=\"2. 数据分割的注意事项\"></a>2. 数据分割的注意事项</h1><h2 id=\"2-1-保证数据的随机性\"><a href=\"#2-1-保证数据的随机性\" class=\"headerlink\" title=\"2.1. 保证数据的随机性\"></a>2.1. 保证数据的随机性</h2><p>如果数据分割是按照一定的规律进行的话，那么训练出来的模型也会被“模式化”，一旦遇到特殊值，就会判断出错。</p>\n<h2 id=\"2-2-保证训练集的大小\"><a href=\"#2-2-保证训练集的大小\" class=\"headerlink\" title=\"2.2. 保证训练集的大小\"></a>2.2. 保证训练集的大小</h2><p>训练的数量越大，模型的准确率一般会更高。如果训练集太小，模型就“不稳定”，在测试中遇到特殊值，就容易得到错误的结果。</p>\n<hr>\n<h1 id=\"3-Python实现方法\"><a href=\"#3-Python实现方法\" class=\"headerlink\" title=\"3. Python实现方法\"></a>3. Python实现方法</h1><blockquote>\n<p>Github代码：<a href=\"https://github.com/JasonDean-1/MachineLearningDemo/blob/master/DataPreprocessing/1.DataSplit.py\" target=\"_blank\" rel=\"external\">https://github.com/JasonDean-1/MachineLearningDemo/blob/master/DataPreprocessing/1.DataSplit.py</a></p>\n<h2 id=\"3-1-调用sklearn的包\"><a href=\"#3-1-调用sklearn的包\" class=\"headerlink\" title=\"3.1. 调用sklearn的包\"></a>3.1. 调用sklearn的包</h2></blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\"># --- coding: utf-8 ---</div><div class=\"line\"></div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\">Package Version(s):</div><div class=\"line\">    sklearn: 0.18</div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\"></div><div class=\"line\">from sklearn.datasets import load_digits</div><div class=\"line\"></div><div class=\"line\"># load in datasets</div><div class=\"line\">datasets = load_digits()</div><div class=\"line\"># Print out the datashape.</div><div class=\"line\">print &quot;The length of dataset is: &quot;, datasets.data.shape</div><div class=\"line\"># Ans = (1797L, 64L)</div><div class=\"line\"></div><div class=\"line\">from sklearn.cross_validation import train_test_split</div><div class=\"line\"># Split the data</div><div class=\"line\">x_train, x_test, y_train, y_test = train_test_split( datasets.data, datasets.target, test_size = 0.25, random_state = 42 )</div><div class=\"line\"># Print out the datashape.</div><div class=\"line\">print &quot;The length of train set is: &quot;, y_train.shape</div><div class=\"line\"># Ans = (1347L,)</div><div class=\"line\">print &quot;The length of test set is: &quot;, y_test.shape</div><div class=\"line\"># Ans = (450L,)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">----------</div><div class=\"line\">Output:</div><div class=\"line\"># The length of dataset is: (1797L, 64L)</div><div class=\"line\"># The length of train set is: Ans = (1347L,)</div><div class=\"line\"># The length of test set is: Ans = (450L,)</div></pre></td></tr></table></figure>\n<p><br><br><br><br><br></p>\n<h2 id=\"3-2-通过循环来分割数据\"><a href=\"#3-2-通过循环来分割数据\" class=\"headerlink\" title=\"3.2. 通过循环来分割数据\"></a>3.2. 通过循环来分割数据</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\"># --- coding: utf-8 ---</div><div class=\"line\"></div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\">Package Version(s):</div><div class=\"line\">    sklearn: 0.18</div><div class=\"line\">&quot;&quot;&quot;</div><div class=\"line\"></div><div class=\"line\">from sklearn.datasets import load_digits</div><div class=\"line\"></div><div class=\"line\"># load in datasets</div><div class=\"line\">datasets = load_digits()</div><div class=\"line\"># Print out the datashape.</div><div class=\"line\">print &quot;The length of dataset is: &quot;, datasets.data.shape</div><div class=\"line\"># Ans = (1797L, 64L)</div><div class=\"line\">x_train = []</div><div class=\"line\">x_test = []</div><div class=\"line\">y_train = []</div><div class=\"line\">y_test = []</div><div class=\"line\"></div><div class=\"line\"># Split the data</div><div class=\"line\">x_test = [datasets.data[i] for i in range(len(datasets.data)) if i%4 == 0]</div><div class=\"line\">x_train = [datasets.data[i] for i in range(len(datasets.data)) if i%4 != 0]</div><div class=\"line\">y_test = [datasets.target[i] for i in range(len(datasets.data)) if i%4 == 0]</div><div class=\"line\">y_train = [datasets.target[i] for i in range(len(datasets.data)) if i%4 != 0]</div><div class=\"line\"></div><div class=\"line\"># Print out the datashape.</div><div class=\"line\">print &quot;The length of train set is: &quot;, len( y_train )</div><div class=\"line\"># Ans = 1347</div><div class=\"line\">print &quot;The length of test set is: &quot;, len( y_test )</div><div class=\"line\"># Ans = 450</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">----------</div><div class=\"line\">Output:</div><div class=\"line\"># The length of dataset is: (1797L, 64L)</div><div class=\"line\"># The length of train set is: Ans = 1347</div><div class=\"line\"># The length of test set is: Ans = 450</div></pre></td></tr></table></figure>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"线性模型性能分析--混淆矩阵(Confusion Matrix)","date":"2017-11-01T04:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n\n# 1.  什么是混淆矩阵\n>在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。在图像精度评价中，主要用于比较分类结果和实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面。混淆矩阵是通过将每个实测像元的位置和分类与分类图像中的相应位置和分类像比较计算的<sup>[1]</sup>。\n\n通过分析混淆矩阵，我们可以得到:\n- TPR (True Positive Rate), FPR (False Positive Rate) 并画出ROC (Receiver Operating Characteristic)曲线和求出AUC (Area Under Curve)\n- 准确率(Accuracy), 精确率(Precision), 召回率(Recall), F1值(F1 Score)\n下面我们来分析混淆矩阵。\n\n-----------------------\n<br>\n<br>\n\n# 2. 混淆矩阵分析\n![Confusion Matrix](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/92bc9a9e0201360c0245f364051a9b2fadc77b54/MachineLearning/ConfusionMatrix.png)\n<center>Confusion Matrix<sup>[2]<sup></center>\n\n分析：\nTP：模型判定为P，实际上也是P，即判断正确\nFP：模型判定为N，实际上却是P，即判断错误\nFN：模型判定为P，实际上却是N，即判断错误\nTN：模型判定为N，实际上也是N，即判断正确\n\n-----------\n存在关系：\n$TPR = \\frac{TP}{TP+FP}$\n\n$FPR = \\frac{FP}{TN+FN}$\n\n$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$\n\n$Precision = \\frac{TP}{TP+FP}$\n\n$Recall = \\frac{TP}{TP+FN}$\n\n$F1-Measure= \\cfrac{2}{\\cfrac{1}{Precision}+\\cfrac{1}{Recall}}$\n\n-----------------------\n<br>\n<br>\n\n#3. Accuracy, Precision, Recall，F1-Measure的分析\n> 举例：要对癌症患者分类：良性和恶性。现在有200个患者，刚好100个良性，100个恶性，训练之后的预测50个良性，150个恶性，即：预测50个良性正确，有50个良性被预测为恶性，100个恶性预测全部正确。\n> 此时：\n> TP: 50\n> FP: 50\n> FN: 0\n>TN: 100\n>TPR: 0.5\n>FPR: 0.5\n>Accuracy: 75%\n>Precision: 50%\n>Recall: 100%\n>F1-Measure: 66.7% 即($\\frac{2}{3}$)\n\n\n关于精确率和召回率，要根据具体情境去判断，那个高才好，参考知乎第一条回答[精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？](https://www.zhihu.com/question/30643044/answer/48955833)\n\n-----------------------\n<br>\n<br>\n\n#4. ROC，AUC的分析：\n\n##4.1. ROC分析\n关于ROC，先看下图，\n![ROC](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/ff7b13d768029291632ed1196d4729e41f30d371/MachineLearning/ROC.jpg)\n\n根据刚刚上面对TPR，FPR的分析，容易发现:\n\n- 在（0,0）点，TP和FP都为0（FN和TN都为1），也就是说，对于所有值，预测模型都预测为Negative，即判断为Positive的阈值过高。\n- 在（1,1）点，TP和FP都为1（FN和TN都为0），也就是说，对于所有值，预测模型都预测为Positive，即判断为Positive的阈值过低。\n\n\n##4.2. AUC分析\nAUC（Area Under Curve），我觉得原名应为（Area Under roc Curve）更好，其定义为ROC曲线下的面积，面积的数值不会大于1。\nROC曲线一般都处于$y=x$这条直线的上方，没有人希望模型在$y=x$的线以下，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，但对应一个数据来说，AUC往往越大越好。\n\n-----------------------\n<br>\n<br>\n\n# 参考\n[1]百度百科:https://baike.baidu.com/item/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/10087822?fr=aladdin&fromid=18082441&fromtitle=Confusion+Matrix\n[2]维基百科:https://en.wikipedia.org/wiki/Confusion_matrix\n[3]精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？https://www.zhihu.com/question/30643044/answer/48955833\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/2017-11-01-线性模型性能分析--混淆矩阵(Confusion Matrix).md","raw":"---\ntitle: 线性模型性能分析--混淆矩阵(Confusion Matrix)\ndate: 2017-11-01 12:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n\n# 1.  什么是混淆矩阵\n>在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。在图像精度评价中，主要用于比较分类结果和实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面。混淆矩阵是通过将每个实测像元的位置和分类与分类图像中的相应位置和分类像比较计算的<sup>[1]</sup>。\n\n通过分析混淆矩阵，我们可以得到:\n- TPR (True Positive Rate), FPR (False Positive Rate) 并画出ROC (Receiver Operating Characteristic)曲线和求出AUC (Area Under Curve)\n- 准确率(Accuracy), 精确率(Precision), 召回率(Recall), F1值(F1 Score)\n下面我们来分析混淆矩阵。\n\n-----------------------\n<br>\n<br>\n\n# 2. 混淆矩阵分析\n![Confusion Matrix](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/92bc9a9e0201360c0245f364051a9b2fadc77b54/MachineLearning/ConfusionMatrix.png)\n<center>Confusion Matrix<sup>[2]<sup></center>\n\n分析：\nTP：模型判定为P，实际上也是P，即判断正确\nFP：模型判定为N，实际上却是P，即判断错误\nFN：模型判定为P，实际上却是N，即判断错误\nTN：模型判定为N，实际上也是N，即判断正确\n\n-----------\n存在关系：\n$TPR = \\frac{TP}{TP+FP}$\n\n$FPR = \\frac{FP}{TN+FN}$\n\n$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$\n\n$Precision = \\frac{TP}{TP+FP}$\n\n$Recall = \\frac{TP}{TP+FN}$\n\n$F1-Measure= \\cfrac{2}{\\cfrac{1}{Precision}+\\cfrac{1}{Recall}}$\n\n-----------------------\n<br>\n<br>\n\n#3. Accuracy, Precision, Recall，F1-Measure的分析\n> 举例：要对癌症患者分类：良性和恶性。现在有200个患者，刚好100个良性，100个恶性，训练之后的预测50个良性，150个恶性，即：预测50个良性正确，有50个良性被预测为恶性，100个恶性预测全部正确。\n> 此时：\n> TP: 50\n> FP: 50\n> FN: 0\n>TN: 100\n>TPR: 0.5\n>FPR: 0.5\n>Accuracy: 75%\n>Precision: 50%\n>Recall: 100%\n>F1-Measure: 66.7% 即($\\frac{2}{3}$)\n\n\n关于精确率和召回率，要根据具体情境去判断，那个高才好，参考知乎第一条回答[精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？](https://www.zhihu.com/question/30643044/answer/48955833)\n\n-----------------------\n<br>\n<br>\n\n#4. ROC，AUC的分析：\n\n##4.1. ROC分析\n关于ROC，先看下图，\n![ROC](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/ff7b13d768029291632ed1196d4729e41f30d371/MachineLearning/ROC.jpg)\n\n根据刚刚上面对TPR，FPR的分析，容易发现:\n\n- 在（0,0）点，TP和FP都为0（FN和TN都为1），也就是说，对于所有值，预测模型都预测为Negative，即判断为Positive的阈值过高。\n- 在（1,1）点，TP和FP都为1（FN和TN都为0），也就是说，对于所有值，预测模型都预测为Positive，即判断为Positive的阈值过低。\n\n\n##4.2. AUC分析\nAUC（Area Under Curve），我觉得原名应为（Area Under roc Curve）更好，其定义为ROC曲线下的面积，面积的数值不会大于1。\nROC曲线一般都处于$y=x$这条直线的上方，没有人希望模型在$y=x$的线以下，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，但对应一个数据来说，AUC往往越大越好。\n\n-----------------------\n<br>\n<br>\n\n# 参考\n[1]百度百科:https://baike.baidu.com/item/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/10087822?fr=aladdin&fromid=18082441&fromtitle=Confusion+Matrix\n[2]维基百科:https://en.wikipedia.org/wiki/Confusion_matrix\n[3]精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？https://www.zhihu.com/question/30643044/answer/48955833\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/2017-11-01-线性模型性能分析--混淆矩阵(Confusion Matrix)","published":1,"updated":"2018-04-14T19:42:06.489Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2di000krwtjgc8uf215","content":"<h1 id=\"1-什么是混淆矩阵\"><a href=\"#1-什么是混淆矩阵\" class=\"headerlink\" title=\"1.  什么是混淆矩阵\"></a>1.  什么是混淆矩阵</h1><blockquote>\n<p>在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。在图像精度评价中，主要用于比较分类结果和实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面。混淆矩阵是通过将每个实测像元的位置和分类与分类图像中的相应位置和分类像比较计算的<sup>[1]</sup>。</p>\n</blockquote>\n<p>通过分析混淆矩阵，我们可以得到:</p>\n<ul>\n<li>TPR (True Positive Rate), FPR (False Positive Rate) 并画出ROC (Receiver Operating Characteristic)曲线和求出AUC (Area Under Curve)</li>\n<li>准确率(Accuracy), 精确率(Precision), 召回率(Recall), F1值(F1 Score)<br>下面我们来分析混淆矩阵。</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"2-混淆矩阵分析\"><a href=\"#2-混淆矩阵分析\" class=\"headerlink\" title=\"2. 混淆矩阵分析\"></a>2. 混淆矩阵分析</h1><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/92bc9a9e0201360c0245f364051a9b2fadc77b54/MachineLearning/ConfusionMatrix.png\" alt=\"Confusion Matrix\"></p>\n<center>Confusion Matrix<sup>[2]<sup></sup></sup></center>\n\n<p>分析：<br>TP：模型判定为P，实际上也是P，即判断正确<br>FP：模型判定为N，实际上却是P，即判断错误<br>FN：模型判定为P，实际上却是N，即判断错误<br>TN：模型判定为N，实际上也是N，即判断正确</p>\n<hr>\n<p>存在关系：<br>$TPR = \\frac{TP}{TP+FP}$</p>\n<p>$FPR = \\frac{FP}{TN+FN}$</p>\n<p>$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$</p>\n<p>$Precision = \\frac{TP}{TP+FP}$</p>\n<p>$Recall = \\frac{TP}{TP+FN}$</p>\n<p>$F1-Measure= \\cfrac{2}{\\cfrac{1}{Precision}+\\cfrac{1}{Recall}}$</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"3-Accuracy-Precision-Recall，F1-Measure的分析\"><a href=\"#3-Accuracy-Precision-Recall，F1-Measure的分析\" class=\"headerlink\" title=\"3. Accuracy, Precision, Recall，F1-Measure的分析\"></a>3. Accuracy, Precision, Recall，F1-Measure的分析</h1><blockquote>\n<p>举例：要对癌症患者分类：良性和恶性。现在有200个患者，刚好100个良性，100个恶性，训练之后的预测50个良性，150个恶性，即：预测50个良性正确，有50个良性被预测为恶性，100个恶性预测全部正确。<br>此时：<br>TP: 50<br>FP: 50<br>FN: 0<br>TN: 100<br>TPR: 0.5<br>FPR: 0.5<br>Accuracy: 75%<br>Precision: 50%<br>Recall: 100%<br>F1-Measure: 66.7% 即($\\frac{2}{3}$)</p>\n</blockquote>\n<p>关于精确率和召回率，要根据具体情境去判断，那个高才好，参考知乎第一条回答<a href=\"https://www.zhihu.com/question/30643044/answer/48955833\" target=\"_blank\" rel=\"external\">精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？</a></p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"4-ROC，AUC的分析：\"><a href=\"#4-ROC，AUC的分析：\" class=\"headerlink\" title=\"4. ROC，AUC的分析：\"></a>4. ROC，AUC的分析：</h1><h2 id=\"4-1-ROC分析\"><a href=\"#4-1-ROC分析\" class=\"headerlink\" title=\"4.1. ROC分析\"></a>4.1. ROC分析</h2><p>关于ROC，先看下图，<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/ff7b13d768029291632ed1196d4729e41f30d371/MachineLearning/ROC.jpg\" alt=\"ROC\"></p>\n<p>根据刚刚上面对TPR，FPR的分析，容易发现:</p>\n<ul>\n<li>在（0,0）点，TP和FP都为0（FN和TN都为1），也就是说，对于所有值，预测模型都预测为Negative，即判断为Positive的阈值过高。</li>\n<li>在（1,1）点，TP和FP都为1（FN和TN都为0），也就是说，对于所有值，预测模型都预测为Positive，即判断为Positive的阈值过低。</li>\n</ul>\n<h2 id=\"4-2-AUC分析\"><a href=\"#4-2-AUC分析\" class=\"headerlink\" title=\"4.2. AUC分析\"></a>4.2. AUC分析</h2><p>AUC（Area Under Curve），我觉得原名应为（Area Under roc Curve）更好，其定义为ROC曲线下的面积，面积的数值不会大于1。<br>ROC曲线一般都处于$y=x$这条直线的上方，没有人希望模型在$y=x$的线以下，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，但对应一个数据来说，AUC往往越大越好。</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>[1]百度百科:<a href=\"https://baike.baidu.com/item/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/10087822?fr=aladdin&amp;fromid=18082441&amp;fromtitle=Confusion+Matrix\" target=\"_blank\" rel=\"external\">https://baike.baidu.com/item/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/10087822?fr=aladdin&amp;fromid=18082441&amp;fromtitle=Confusion+Matrix</a><br>[2]维基百科:<a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Confusion_matrix</a><br>[3]精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？<a href=\"https://www.zhihu.com/question/30643044/answer/48955833\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/30643044/answer/48955833</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"1-什么是混淆矩阵\"><a href=\"#1-什么是混淆矩阵\" class=\"headerlink\" title=\"1.  什么是混淆矩阵\"></a>1.  什么是混淆矩阵</h1><blockquote>\n<p>在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。在图像精度评价中，主要用于比较分类结果和实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面。混淆矩阵是通过将每个实测像元的位置和分类与分类图像中的相应位置和分类像比较计算的<sup>[1]</sup>。</p>\n</blockquote>\n<p>通过分析混淆矩阵，我们可以得到:</p>\n<ul>\n<li>TPR (True Positive Rate), FPR (False Positive Rate) 并画出ROC (Receiver Operating Characteristic)曲线和求出AUC (Area Under Curve)</li>\n<li>准确率(Accuracy), 精确率(Precision), 召回率(Recall), F1值(F1 Score)<br>下面我们来分析混淆矩阵。</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"2-混淆矩阵分析\"><a href=\"#2-混淆矩阵分析\" class=\"headerlink\" title=\"2. 混淆矩阵分析\"></a>2. 混淆矩阵分析</h1><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/92bc9a9e0201360c0245f364051a9b2fadc77b54/MachineLearning/ConfusionMatrix.png\" alt=\"Confusion Matrix\"></p>\n<center>Confusion Matrix<sup>[2]<sup></sup></sup></center>\n\n<p>分析：<br>TP：模型判定为P，实际上也是P，即判断正确<br>FP：模型判定为N，实际上却是P，即判断错误<br>FN：模型判定为P，实际上却是N，即判断错误<br>TN：模型判定为N，实际上也是N，即判断正确</p>\n<hr>\n<p>存在关系：<br>$TPR = \\frac{TP}{TP+FP}$</p>\n<p>$FPR = \\frac{FP}{TN+FN}$</p>\n<p>$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$</p>\n<p>$Precision = \\frac{TP}{TP+FP}$</p>\n<p>$Recall = \\frac{TP}{TP+FN}$</p>\n<p>$F1-Measure= \\cfrac{2}{\\cfrac{1}{Precision}+\\cfrac{1}{Recall}}$</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"3-Accuracy-Precision-Recall，F1-Measure的分析\"><a href=\"#3-Accuracy-Precision-Recall，F1-Measure的分析\" class=\"headerlink\" title=\"3. Accuracy, Precision, Recall，F1-Measure的分析\"></a>3. Accuracy, Precision, Recall，F1-Measure的分析</h1><blockquote>\n<p>举例：要对癌症患者分类：良性和恶性。现在有200个患者，刚好100个良性，100个恶性，训练之后的预测50个良性，150个恶性，即：预测50个良性正确，有50个良性被预测为恶性，100个恶性预测全部正确。<br>此时：<br>TP: 50<br>FP: 50<br>FN: 0<br>TN: 100<br>TPR: 0.5<br>FPR: 0.5<br>Accuracy: 75%<br>Precision: 50%<br>Recall: 100%<br>F1-Measure: 66.7% 即($\\frac{2}{3}$)</p>\n</blockquote>\n<p>关于精确率和召回率，要根据具体情境去判断，那个高才好，参考知乎第一条回答<a href=\"https://www.zhihu.com/question/30643044/answer/48955833\" target=\"_blank\" rel=\"external\">精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？</a></p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"4-ROC，AUC的分析：\"><a href=\"#4-ROC，AUC的分析：\" class=\"headerlink\" title=\"4. ROC，AUC的分析：\"></a>4. ROC，AUC的分析：</h1><h2 id=\"4-1-ROC分析\"><a href=\"#4-1-ROC分析\" class=\"headerlink\" title=\"4.1. ROC分析\"></a>4.1. ROC分析</h2><p>关于ROC，先看下图，<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/ff7b13d768029291632ed1196d4729e41f30d371/MachineLearning/ROC.jpg\" alt=\"ROC\"></p>\n<p>根据刚刚上面对TPR，FPR的分析，容易发现:</p>\n<ul>\n<li>在（0,0）点，TP和FP都为0（FN和TN都为1），也就是说，对于所有值，预测模型都预测为Negative，即判断为Positive的阈值过高。</li>\n<li>在（1,1）点，TP和FP都为1（FN和TN都为0），也就是说，对于所有值，预测模型都预测为Positive，即判断为Positive的阈值过低。</li>\n</ul>\n<h2 id=\"4-2-AUC分析\"><a href=\"#4-2-AUC分析\" class=\"headerlink\" title=\"4.2. AUC分析\"></a>4.2. AUC分析</h2><p>AUC（Area Under Curve），我觉得原名应为（Area Under roc Curve）更好，其定义为ROC曲线下的面积，面积的数值不会大于1。<br>ROC曲线一般都处于$y=x$这条直线的上方，没有人希望模型在$y=x$的线以下，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，但对应一个数据来说，AUC往往越大越好。</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>[1]百度百科:<a href=\"https://baike.baidu.com/item/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/10087822?fr=aladdin&amp;fromid=18082441&amp;fromtitle=Confusion+Matrix\" target=\"_blank\" rel=\"external\">https://baike.baidu.com/item/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/10087822?fr=aladdin&amp;fromid=18082441&amp;fromtitle=Confusion+Matrix</a><br>[2]维基百科:<a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Confusion_matrix</a><br>[3]精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？<a href=\"https://www.zhihu.com/question/30643044/answer/48955833\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/30643044/answer/48955833</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"Concave and Convex Function","date":"2017-11-30T13:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Concave and Convex Function\n\n## What is Concave Function?\nConcave function is a function where the line segement between any two points of the function lies below or on the graph.[1]\n\nMathematically, as for concave function, the derivative of the left side of the max point is > 0, whereas the right side of the max point is < 0. There is one and only one value to make the derivative of the function equal to 0.\n\nSee the example below.\n\n![Concave function](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Concave%20Function.jpg)\nConcave Function\n\n## What is Convex Function?\n\nOppose to concave function, convex function is a function where the line segement between any two points of the function lies above or on the graph.[2]\n\nMathematically, as for concave function, the derivative of the left side of the max point is < 0, whereas the right side of the max point is > 0. There is one and only one value to make the derivative of the function equal to 0.\n\nSee the example below.\n\n![Convex function](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Convex%20Function.jpg)\nConvex Function\n\n## Neither Concave or Convex Function\n\nAs for other function which is neither concave or convex function, the number of the value to make the derivative of the function equal to 0 is multiple or none.\n\nSee the example below.\n\n![Neither](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Neither.jpg)\nNeither\n\n\n## How to Find the Max/Min of Concave/Convex Function?\n\nTo find the max/min of concave/convex function, we need to make the value converge to the max/min.\n\nSo to converge the value, as for concave function, we should plus the derivative of the function, whereas we should substract the derivative of the function.\n\n\n\nLet summary the attribute concave and convex function\n\nConcave| Convex |\n:-------:|:------:|\nConcave function is a function where the line segement between any two points of the function lies below or on the graph.|Oppose to concave function, convex function is a function where the line segement between any two points of the function lies above or on the graph.|\nMathematically, as for concave function, the derivative of the left side of the max point is > 0, whereas the right side of the max point is < 0. There is one and only one value to make the derivative of the function equal to 0.|Mathematically, as for concave function, the derivative of the left side of the max point is < 0, whereas the right side of the max point is > 0. There is one and only one value to make the derivative of the function equal to 0.\nPlus the derivative of the function | Substract the derivative of the function|\n\n\nSee more detail below.\n\n![Converge](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Optimize.jpg)\nConverge\n\n\n# Reference\n[1] [Concave function - Wikipedia](https://en.wikipedia.org/wiki/Concave_function)\n\n[2] [Convex function - Wikipedia](https://en.wikipedia.org/wiki/Convex_function)\n","source":"_posts/Mathematics/2017-11-30-Concave and Convex Function.md","raw":"---\ntitle: Concave and Convex Function\ndate: 2017-11-30 21:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning-Mathematics]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Concave and Convex Function\n\n## What is Concave Function?\nConcave function is a function where the line segement between any two points of the function lies below or on the graph.[1]\n\nMathematically, as for concave function, the derivative of the left side of the max point is > 0, whereas the right side of the max point is < 0. There is one and only one value to make the derivative of the function equal to 0.\n\nSee the example below.\n\n![Concave function](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Concave%20Function.jpg)\nConcave Function\n\n## What is Convex Function?\n\nOppose to concave function, convex function is a function where the line segement between any two points of the function lies above or on the graph.[2]\n\nMathematically, as for concave function, the derivative of the left side of the max point is < 0, whereas the right side of the max point is > 0. There is one and only one value to make the derivative of the function equal to 0.\n\nSee the example below.\n\n![Convex function](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Convex%20Function.jpg)\nConvex Function\n\n## Neither Concave or Convex Function\n\nAs for other function which is neither concave or convex function, the number of the value to make the derivative of the function equal to 0 is multiple or none.\n\nSee the example below.\n\n![Neither](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Neither.jpg)\nNeither\n\n\n## How to Find the Max/Min of Concave/Convex Function?\n\nTo find the max/min of concave/convex function, we need to make the value converge to the max/min.\n\nSo to converge the value, as for concave function, we should plus the derivative of the function, whereas we should substract the derivative of the function.\n\n\n\nLet summary the attribute concave and convex function\n\nConcave| Convex |\n:-------:|:------:|\nConcave function is a function where the line segement between any two points of the function lies below or on the graph.|Oppose to concave function, convex function is a function where the line segement between any two points of the function lies above or on the graph.|\nMathematically, as for concave function, the derivative of the left side of the max point is > 0, whereas the right side of the max point is < 0. There is one and only one value to make the derivative of the function equal to 0.|Mathematically, as for concave function, the derivative of the left side of the max point is < 0, whereas the right side of the max point is > 0. There is one and only one value to make the derivative of the function equal to 0.\nPlus the derivative of the function | Substract the derivative of the function|\n\n\nSee more detail below.\n\n![Converge](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Optimize.jpg)\nConverge\n\n\n# Reference\n[1] [Concave function - Wikipedia](https://en.wikipedia.org/wiki/Concave_function)\n\n[2] [Convex function - Wikipedia](https://en.wikipedia.org/wiki/Convex_function)\n","slug":"Mathematics/2017-11-30-Concave and Convex Function","published":1,"updated":"2018-04-14T19:42:06.490Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2dk000orwtjp508shor","content":"<h1 id=\"Concave-and-Convex-Function\"><a href=\"#Concave-and-Convex-Function\" class=\"headerlink\" title=\"Concave and Convex Function\"></a>Concave and Convex Function</h1><h2 id=\"What-is-Concave-Function\"><a href=\"#What-is-Concave-Function\" class=\"headerlink\" title=\"What is Concave Function?\"></a>What is Concave Function?</h2><p>Concave function is a function where the line segement between any two points of the function lies below or on the graph.[1]</p>\n<p>Mathematically, as for concave function, the derivative of the left side of the max point is &gt; 0, whereas the right side of the max point is &lt; 0. There is one and only one value to make the derivative of the function equal to 0.</p>\n<p>See the example below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Concave%20Function.jpg\" alt=\"Concave function\"><br>Concave Function</p>\n<h2 id=\"What-is-Convex-Function\"><a href=\"#What-is-Convex-Function\" class=\"headerlink\" title=\"What is Convex Function?\"></a>What is Convex Function?</h2><p>Oppose to concave function, convex function is a function where the line segement between any two points of the function lies above or on the graph.[2]</p>\n<p>Mathematically, as for concave function, the derivative of the left side of the max point is &lt; 0, whereas the right side of the max point is &gt; 0. There is one and only one value to make the derivative of the function equal to 0.</p>\n<p>See the example below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Convex%20Function.jpg\" alt=\"Convex function\"><br>Convex Function</p>\n<h2 id=\"Neither-Concave-or-Convex-Function\"><a href=\"#Neither-Concave-or-Convex-Function\" class=\"headerlink\" title=\"Neither Concave or Convex Function\"></a>Neither Concave or Convex Function</h2><p>As for other function which is neither concave or convex function, the number of the value to make the derivative of the function equal to 0 is multiple or none.</p>\n<p>See the example below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Neither.jpg\" alt=\"Neither\"><br>Neither</p>\n<h2 id=\"How-to-Find-the-Max-Min-of-Concave-Convex-Function\"><a href=\"#How-to-Find-the-Max-Min-of-Concave-Convex-Function\" class=\"headerlink\" title=\"How to Find the Max/Min of Concave/Convex Function?\"></a>How to Find the Max/Min of Concave/Convex Function?</h2><p>To find the max/min of concave/convex function, we need to make the value converge to the max/min.</p>\n<p>So to converge the value, as for concave function, we should plus the derivative of the function, whereas we should substract the derivative of the function.</p>\n<p>Let summary the attribute concave and convex function</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Concave</th>\n<th style=\"text-align:center\">Convex</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Concave function is a function where the line segement between any two points of the function lies below or on the graph.</td>\n<td style=\"text-align:center\">Oppose to concave function, convex function is a function where the line segement between any two points of the function lies above or on the graph.</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Mathematically, as for concave function, the derivative of the left side of the max point is &gt; 0, whereas the right side of the max point is &lt; 0. There is one and only one value to make the derivative of the function equal to 0.</td>\n<td style=\"text-align:center\">Mathematically, as for concave function, the derivative of the left side of the max point is &lt; 0, whereas the right side of the max point is &gt; 0. There is one and only one value to make the derivative of the function equal to 0.</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Plus the derivative of the function</td>\n<td style=\"text-align:center\">Substract the derivative of the function</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>See more detail below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Optimize.jpg\" alt=\"Converge\"><br>Converge</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] <a href=\"https://en.wikipedia.org/wiki/Concave_function\" target=\"_blank\" rel=\"external\">Concave function - Wikipedia</a></p>\n<p>[2] <a href=\"https://en.wikipedia.org/wiki/Convex_function\" target=\"_blank\" rel=\"external\">Convex function - Wikipedia</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Concave-and-Convex-Function\"><a href=\"#Concave-and-Convex-Function\" class=\"headerlink\" title=\"Concave and Convex Function\"></a>Concave and Convex Function</h1><h2 id=\"What-is-Concave-Function\"><a href=\"#What-is-Concave-Function\" class=\"headerlink\" title=\"What is Concave Function?\"></a>What is Concave Function?</h2><p>Concave function is a function where the line segement between any two points of the function lies below or on the graph.[1]</p>\n<p>Mathematically, as for concave function, the derivative of the left side of the max point is &gt; 0, whereas the right side of the max point is &lt; 0. There is one and only one value to make the derivative of the function equal to 0.</p>\n<p>See the example below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Concave%20Function.jpg\" alt=\"Concave function\"><br>Concave Function</p>\n<h2 id=\"What-is-Convex-Function\"><a href=\"#What-is-Convex-Function\" class=\"headerlink\" title=\"What is Convex Function?\"></a>What is Convex Function?</h2><p>Oppose to concave function, convex function is a function where the line segement between any two points of the function lies above or on the graph.[2]</p>\n<p>Mathematically, as for concave function, the derivative of the left side of the max point is &lt; 0, whereas the right side of the max point is &gt; 0. There is one and only one value to make the derivative of the function equal to 0.</p>\n<p>See the example below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Convex%20Function.jpg\" alt=\"Convex function\"><br>Convex Function</p>\n<h2 id=\"Neither-Concave-or-Convex-Function\"><a href=\"#Neither-Concave-or-Convex-Function\" class=\"headerlink\" title=\"Neither Concave or Convex Function\"></a>Neither Concave or Convex Function</h2><p>As for other function which is neither concave or convex function, the number of the value to make the derivative of the function equal to 0 is multiple or none.</p>\n<p>See the example below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Neither.jpg\" alt=\"Neither\"><br>Neither</p>\n<h2 id=\"How-to-Find-the-Max-Min-of-Concave-Convex-Function\"><a href=\"#How-to-Find-the-Max-Min-of-Concave-Convex-Function\" class=\"headerlink\" title=\"How to Find the Max/Min of Concave/Convex Function?\"></a>How to Find the Max/Min of Concave/Convex Function?</h2><p>To find the max/min of concave/convex function, we need to make the value converge to the max/min.</p>\n<p>So to converge the value, as for concave function, we should plus the derivative of the function, whereas we should substract the derivative of the function.</p>\n<p>Let summary the attribute concave and convex function</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Concave</th>\n<th style=\"text-align:center\">Convex</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">Concave function is a function where the line segement between any two points of the function lies below or on the graph.</td>\n<td style=\"text-align:center\">Oppose to concave function, convex function is a function where the line segement between any two points of the function lies above or on the graph.</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Mathematically, as for concave function, the derivative of the left side of the max point is &gt; 0, whereas the right side of the max point is &lt; 0. There is one and only one value to make the derivative of the function equal to 0.</td>\n<td style=\"text-align:center\">Mathematically, as for concave function, the derivative of the left side of the max point is &lt; 0, whereas the right side of the max point is &gt; 0. There is one and only one value to make the derivative of the function equal to 0.</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">Plus the derivative of the function</td>\n<td style=\"text-align:center\">Substract the derivative of the function</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>See more detail below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/c4798307c21222a06dec48ee4e47e31f990da5e2/__Blog/__Personal%20Understanding/_archive/_images/Concave%20and%20Convex%20Function-Optimize.jpg\" alt=\"Converge\"><br>Converge</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] <a href=\"https://en.wikipedia.org/wiki/Concave_function\" target=\"_blank\" rel=\"external\">Concave function - Wikipedia</a></p>\n<p>[2] <a href=\"https://en.wikipedia.org/wiki/Convex_function\" target=\"_blank\" rel=\"external\">Convex function - Wikipedia</a></p>\n"},{"title":"Sigmoid Function","date":"2017-10-30T07:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Sigmoid Function\n\n\n## 1. Introduction\n> To limit all data within the range of 0 to 1.\n\n## 2. Formula\n$$\ny = \\frac{1}{1+e^{-x}}\n$$\n\n## 3. Implementation\n### 3.1 Octave\n```\n x = linspace(-10, 10 ,10000);\n y = zeros( size(x, 1), size(x, 2));\n\n for i = 1:length(x)\n   y(i) = 1/(1+e^(-x(i)));\n endfor\n\n figure();\n plot( x,y);\n grid on;\n xlabel(\"x\");\n ylabel(\"y=1/(1+e^-x)\");\n title(\"Sigmoid Function\");\n```\n\nOutput figure\n![Sigmoid Function](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/fd188539ca35c6e4d8859d07bbde8f5439760bae/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/2.Logistic%20Regression%20Hypothesis.png)\n<center>Sigmoid Function</center>\n\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/2017-10-30-Sigmoid Function.md","raw":"---\ntitle: Sigmoid Function\ndate: 2017-10-30 15:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Sigmoid Function\n\n\n## 1. Introduction\n> To limit all data within the range of 0 to 1.\n\n## 2. Formula\n$$\ny = \\frac{1}{1+e^{-x}}\n$$\n\n## 3. Implementation\n### 3.1 Octave\n```\n x = linspace(-10, 10 ,10000);\n y = zeros( size(x, 1), size(x, 2));\n\n for i = 1:length(x)\n   y(i) = 1/(1+e^(-x(i)));\n endfor\n\n figure();\n plot( x,y);\n grid on;\n xlabel(\"x\");\n ylabel(\"y=1/(1+e^-x)\");\n title(\"Sigmoid Function\");\n```\n\nOutput figure\n![Sigmoid Function](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/fd188539ca35c6e4d8859d07bbde8f5439760bae/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/2.Logistic%20Regression%20Hypothesis.png)\n<center>Sigmoid Function</center>\n\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/2017-10-30-Sigmoid Function","published":1,"updated":"2018-04-14T19:42:06.488Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2dl000prwtj7qzom3mz","content":"<h1 id=\"Sigmoid-Function\"><a href=\"#Sigmoid-Function\" class=\"headerlink\" title=\"Sigmoid Function\"></a>Sigmoid Function</h1><h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><blockquote>\n<p>To limit all data within the range of 0 to 1.</p>\n</blockquote>\n<h2 id=\"2-Formula\"><a href=\"#2-Formula\" class=\"headerlink\" title=\"2. Formula\"></a>2. Formula</h2><script type=\"math/tex; mode=display\">\ny = \\frac{1}{1+e^{-x}}</script><h2 id=\"3-Implementation\"><a href=\"#3-Implementation\" class=\"headerlink\" title=\"3. Implementation\"></a>3. Implementation</h2><h3 id=\"3-1-Octave\"><a href=\"#3-1-Octave\" class=\"headerlink\" title=\"3.1 Octave\"></a>3.1 Octave</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">x = linspace(-10, 10 ,10000);</div><div class=\"line\">y = zeros( size(x, 1), size(x, 2));</div><div class=\"line\"></div><div class=\"line\">for i = 1:length(x)</div><div class=\"line\">  y(i) = 1/(1+e^(-x(i)));</div><div class=\"line\">endfor</div><div class=\"line\"></div><div class=\"line\">figure();</div><div class=\"line\">plot( x,y);</div><div class=\"line\">grid on;</div><div class=\"line\">xlabel(&quot;x&quot;);</div><div class=\"line\">ylabel(&quot;y=1/(1+e^-x)&quot;);</div><div class=\"line\">title(&quot;Sigmoid Function&quot;);</div></pre></td></tr></table></figure>\n<p>Output figure<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/fd188539ca35c6e4d8859d07bbde8f5439760bae/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/2.Logistic%20Regression%20Hypothesis.png\" alt=\"Sigmoid Function\"></p>\n<center>Sigmoid Function</center>\n\n\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Sigmoid-Function\"><a href=\"#Sigmoid-Function\" class=\"headerlink\" title=\"Sigmoid Function\"></a>Sigmoid Function</h1><h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><blockquote>\n<p>To limit all data within the range of 0 to 1.</p>\n</blockquote>\n<h2 id=\"2-Formula\"><a href=\"#2-Formula\" class=\"headerlink\" title=\"2. Formula\"></a>2. Formula</h2><script type=\"math/tex; mode=display\">\ny = \\frac{1}{1+e^{-x}}</script><h2 id=\"3-Implementation\"><a href=\"#3-Implementation\" class=\"headerlink\" title=\"3. Implementation\"></a>3. Implementation</h2><h3 id=\"3-1-Octave\"><a href=\"#3-1-Octave\" class=\"headerlink\" title=\"3.1 Octave\"></a>3.1 Octave</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">x = linspace(-10, 10 ,10000);</div><div class=\"line\">y = zeros( size(x, 1), size(x, 2));</div><div class=\"line\"></div><div class=\"line\">for i = 1:length(x)</div><div class=\"line\">  y(i) = 1/(1+e^(-x(i)));</div><div class=\"line\">endfor</div><div class=\"line\"></div><div class=\"line\">figure();</div><div class=\"line\">plot( x,y);</div><div class=\"line\">grid on;</div><div class=\"line\">xlabel(&quot;x&quot;);</div><div class=\"line\">ylabel(&quot;y=1/(1+e^-x)&quot;);</div><div class=\"line\">title(&quot;Sigmoid Function&quot;);</div></pre></td></tr></table></figure>\n<p>Output figure<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/fd188539ca35c6e4d8859d07bbde8f5439760bae/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/2.Logistic%20Regression%20Hypothesis.png\" alt=\"Sigmoid Function\"></p>\n<center>Sigmoid Function</center>\n\n\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"1.When can Machine Learn? - The Learning Problem","date":"2017-10-02T08:03:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# When can Machine Learn? - The Learning Problem\n\n## 1. The Learning Problem\nTo figure this out, we need to compare Human Learning and Machine Learning.\n\n### 1) Human Learning and Machine Learning\n\n#### ① Human Learning\nHuman learning means people learn from perception (E.g., observation, touching, hearing).\n\n#### ② Machine Learning\nLike human learning, machine learning means that machine learn things by collecting data, then computing the data to get skills.\n\n#### ③ Summary\n![Human Learning and Machine Learning](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a8ff8cec23221b4da00516a4de437336adfb1653/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-1%20When%20can%20machine%20learn-machine%20learning%20vs%20human%20learning-cropped.jpg)\n\n### 2) Human Learning V.S. Machine Learning\n既然人类和机器学习的过程一样，为什么我们还要耗费精力去让机器可以学习呢？\n- 一些数据或者信息，人类难以识别；\n- 学习的数据量特别大，人脑难以处理\n- 人脑处理问题的速度很慢，但是很多情况下要求系统能快速的给出答案\n\n<br>\n\n总结如下表：\n\n|      | Human Learning                                | Machine Learning                               |\n| ---- | --------------------------------------------- | ---------------------------------------------- |\n| Pros | Learn emotionally and skillfully              | Processing big data                            |\n| Cons | Cannot dealing with big data, cannot act fast | Cannot work with human programming, no emotion |\n\n### 3) Key to Machine Learning\n不是所以情况都可以使用机器学习，必须满足一下3个关键条件：\n- 存在一个模型，能让我们对它进行改进。（不需要改进，就不需要进行ML了）\n- 规则不容易找出。（如果太简单的话，用ML反而使得其反，耗费了人力物力）\n- 需要有数据的支持，且数据量理论上越大越好。（这给机器学习提供了保证，后面会介绍）\n\n------------------------------------------\n<br>\n<br>\n\n## 2. Application of Machine Learning\nMachine Learning actually can apply to everything.\n\nE.g.,\n\n- Daily need\n   - Food\n      - How does the food taste?\n      - How many chances that some specific people will like the food?\n      - ...\n   - Clothing\n      - The information of the clothing.\n      - Fashion recommendation\n      - ...\n   - Housing\n      - Energy load\n      - Sell price\n      - ...\n   - Transportation\n      - Driving automation\n      - Transportation times\n      - Traffic jam possibilities\n      - ...\n- Education\n   - Math tutoring system.\n   - Quiz generator\n   - ...\n- Entertaining\n   - Recommendation system\n   - Real view experiencing of traveling\n\n\n------------------------------------------\n<br>\n<br>\n\n## 3. Components of Machine Learning\n\n> 以银行是否应该对客户发放信用卡作为例子\n\n### 1) Basic Notation\n\n![Basic Notation](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b871ce6fb178e7433d1565e2b0a1791c11d8d39a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-2%20Basic%20Notation.png)\n<center>Basic Notation<sup>[1]</sup></center>\n\n\n\n\n1.输入(input)：$x∈X$（代表银行所掌握的用户信息）\n\n2.输出(output)：$y∈Y$ （是否会发信用卡给用户）\n\n3.未知的函数，即目标函数（target function）：$f：X→Y$（理想的信用卡发放公式）\n\n4.数据或者叫做资料（ data），即训练样本（ training examples）：$D = {（x_1, y_1）, (x_2, y_2), …, (x_n, y_n)}$（银行的历史记录）\n\n5.假设（hypothesis），根据训练样本得到的实际的函数：$g：X→Y$\n\n\n### 2) Practical Definition\n![Practical Definition](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c5e0095cd06f69646a45c981a77c9bcb8033535a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-3%20Practical%20Definition%20of%20Machine%20Learning.png)\n<center>Practical Definition<sup>[1]</sup></center>\n\n机器学习算法（learning algorithm）一般用$A$表示。还多出来一个新的项目，就是假设空间或者叫做假设集合（hypothesis set）一般用$H$表示，而这时$A$的作用就是从$H$集合中挑选出它认为最好的假设从而得到函数$g$。\n\n\n\n------------------------------------------\n<br>\n<br>\n\n\n## 4. Machine Learning and Other Fields\n> Machine Learning VS Data Mining, Artificial Intelligence, Statistic\n\n### 1) Machine Learning V.S. Data Mining\n机器学习与数据挖掘都叫知识发现（KDD Knowledge Discovery in Dataset）。\n- 两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。\n- 两者是互助的：ML需要大数据的支持才能保持能“学到东西”。\n- 数据挖掘更关注于从大量的数据中的计算问题。\n总的来时，两者密不可分。\n\n### 2) Machine Learning V.S. Artificial Intelligence\nAI是通过特定的方法让机器能做出Intelligent的行为，ML属于AI的一个分支，是AI实现的一种方式\n\n### 3) Machine Learning V.S. Statistic\n统计是通过对已知数据的处理，从而推断出未知的事件的属性\n所以统计学是实现ML的一种方法，统计学里面有许多实用的工具可以用于证明ML。\n\n\n------------------------------------------\n<br>\n<br>\n\n# Summary\n1. 机器学习类似于人类的学习\n2. 机器学习的应用很广，可以说应用领域是各行各业\n3. 机器学习包含：输入数据，输出结果，目标函数，假设函数 ，数据集\n4. 机器学习ML与AI，DM， Statistics有关系， ML∈AI, ML≈DM, ML使用Statistics\n\n总的来说，机器学习的任务是找出一个假设函数 $g(x)$ ，使得假设 $g(x)$ 和目标函数 $f(x)$ 很接近，即 $g(x) \\approx f(x)$, 用第四章的概念可以解释为在测试时的错误率接近零 $E_{out} \\approx 0$\n\n------------------------------------------\n<br>\n<br>\n\n# Reference\n[1]机器学习基石(台湾大学-林轩田)\\1\\1 - 4 - Components of Machine Learning (11-45)\n\n<br><br>\n--------------------------------","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-02-1.When can Machine Learn - The Learning Problem.md","raw":"---\ntitle: 1.When can Machine Learn? - The Learning Problem\ndate: 2017-10-02 16:03:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# When can Machine Learn? - The Learning Problem\n\n## 1. The Learning Problem\nTo figure this out, we need to compare Human Learning and Machine Learning.\n\n### 1) Human Learning and Machine Learning\n\n#### ① Human Learning\nHuman learning means people learn from perception (E.g., observation, touching, hearing).\n\n#### ② Machine Learning\nLike human learning, machine learning means that machine learn things by collecting data, then computing the data to get skills.\n\n#### ③ Summary\n![Human Learning and Machine Learning](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a8ff8cec23221b4da00516a4de437336adfb1653/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-1%20When%20can%20machine%20learn-machine%20learning%20vs%20human%20learning-cropped.jpg)\n\n### 2) Human Learning V.S. Machine Learning\n既然人类和机器学习的过程一样，为什么我们还要耗费精力去让机器可以学习呢？\n- 一些数据或者信息，人类难以识别；\n- 学习的数据量特别大，人脑难以处理\n- 人脑处理问题的速度很慢，但是很多情况下要求系统能快速的给出答案\n\n<br>\n\n总结如下表：\n\n|      | Human Learning                                | Machine Learning                               |\n| ---- | --------------------------------------------- | ---------------------------------------------- |\n| Pros | Learn emotionally and skillfully              | Processing big data                            |\n| Cons | Cannot dealing with big data, cannot act fast | Cannot work with human programming, no emotion |\n\n### 3) Key to Machine Learning\n不是所以情况都可以使用机器学习，必须满足一下3个关键条件：\n- 存在一个模型，能让我们对它进行改进。（不需要改进，就不需要进行ML了）\n- 规则不容易找出。（如果太简单的话，用ML反而使得其反，耗费了人力物力）\n- 需要有数据的支持，且数据量理论上越大越好。（这给机器学习提供了保证，后面会介绍）\n\n------------------------------------------\n<br>\n<br>\n\n## 2. Application of Machine Learning\nMachine Learning actually can apply to everything.\n\nE.g.,\n\n- Daily need\n   - Food\n      - How does the food taste?\n      - How many chances that some specific people will like the food?\n      - ...\n   - Clothing\n      - The information of the clothing.\n      - Fashion recommendation\n      - ...\n   - Housing\n      - Energy load\n      - Sell price\n      - ...\n   - Transportation\n      - Driving automation\n      - Transportation times\n      - Traffic jam possibilities\n      - ...\n- Education\n   - Math tutoring system.\n   - Quiz generator\n   - ...\n- Entertaining\n   - Recommendation system\n   - Real view experiencing of traveling\n\n\n------------------------------------------\n<br>\n<br>\n\n## 3. Components of Machine Learning\n\n> 以银行是否应该对客户发放信用卡作为例子\n\n### 1) Basic Notation\n\n![Basic Notation](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b871ce6fb178e7433d1565e2b0a1791c11d8d39a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-2%20Basic%20Notation.png)\n<center>Basic Notation<sup>[1]</sup></center>\n\n\n\n\n1.输入(input)：$x∈X$（代表银行所掌握的用户信息）\n\n2.输出(output)：$y∈Y$ （是否会发信用卡给用户）\n\n3.未知的函数，即目标函数（target function）：$f：X→Y$（理想的信用卡发放公式）\n\n4.数据或者叫做资料（ data），即训练样本（ training examples）：$D = {（x_1, y_1）, (x_2, y_2), …, (x_n, y_n)}$（银行的历史记录）\n\n5.假设（hypothesis），根据训练样本得到的实际的函数：$g：X→Y$\n\n\n### 2) Practical Definition\n![Practical Definition](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c5e0095cd06f69646a45c981a77c9bcb8033535a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-3%20Practical%20Definition%20of%20Machine%20Learning.png)\n<center>Practical Definition<sup>[1]</sup></center>\n\n机器学习算法（learning algorithm）一般用$A$表示。还多出来一个新的项目，就是假设空间或者叫做假设集合（hypothesis set）一般用$H$表示，而这时$A$的作用就是从$H$集合中挑选出它认为最好的假设从而得到函数$g$。\n\n\n\n------------------------------------------\n<br>\n<br>\n\n\n## 4. Machine Learning and Other Fields\n> Machine Learning VS Data Mining, Artificial Intelligence, Statistic\n\n### 1) Machine Learning V.S. Data Mining\n机器学习与数据挖掘都叫知识发现（KDD Knowledge Discovery in Dataset）。\n- 两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。\n- 两者是互助的：ML需要大数据的支持才能保持能“学到东西”。\n- 数据挖掘更关注于从大量的数据中的计算问题。\n总的来时，两者密不可分。\n\n### 2) Machine Learning V.S. Artificial Intelligence\nAI是通过特定的方法让机器能做出Intelligent的行为，ML属于AI的一个分支，是AI实现的一种方式\n\n### 3) Machine Learning V.S. Statistic\n统计是通过对已知数据的处理，从而推断出未知的事件的属性\n所以统计学是实现ML的一种方法，统计学里面有许多实用的工具可以用于证明ML。\n\n\n------------------------------------------\n<br>\n<br>\n\n# Summary\n1. 机器学习类似于人类的学习\n2. 机器学习的应用很广，可以说应用领域是各行各业\n3. 机器学习包含：输入数据，输出结果，目标函数，假设函数 ，数据集\n4. 机器学习ML与AI，DM， Statistics有关系， ML∈AI, ML≈DM, ML使用Statistics\n\n总的来说，机器学习的任务是找出一个假设函数 $g(x)$ ，使得假设 $g(x)$ 和目标函数 $f(x)$ 很接近，即 $g(x) \\approx f(x)$, 用第四章的概念可以解释为在测试时的错误率接近零 $E_{out} \\approx 0$\n\n------------------------------------------\n<br>\n<br>\n\n# Reference\n[1]机器学习基石(台湾大学-林轩田)\\1\\1 - 4 - Components of Machine Learning (11-45)\n\n<br><br>\n--------------------------------","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-02-1.When can Machine Learn - The Learning Problem","published":1,"updated":"2018-04-14T19:42:06.501Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2do000trwtjqnz3ah9z","content":"<h1 id=\"When-can-Machine-Learn-The-Learning-Problem\"><a href=\"#When-can-Machine-Learn-The-Learning-Problem\" class=\"headerlink\" title=\"When can Machine Learn? - The Learning Problem\"></a>When can Machine Learn? - The Learning Problem</h1><h2 id=\"1-The-Learning-Problem\"><a href=\"#1-The-Learning-Problem\" class=\"headerlink\" title=\"1. The Learning Problem\"></a>1. The Learning Problem</h2><p>To figure this out, we need to compare Human Learning and Machine Learning.</p>\n<h3 id=\"1-Human-Learning-and-Machine-Learning\"><a href=\"#1-Human-Learning-and-Machine-Learning\" class=\"headerlink\" title=\"1) Human Learning and Machine Learning\"></a>1) Human Learning and Machine Learning</h3><h4 id=\"①-Human-Learning\"><a href=\"#①-Human-Learning\" class=\"headerlink\" title=\"① Human Learning\"></a>① Human Learning</h4><p>Human learning means people learn from perception (E.g., observation, touching, hearing).</p>\n<h4 id=\"②-Machine-Learning\"><a href=\"#②-Machine-Learning\" class=\"headerlink\" title=\"② Machine Learning\"></a>② Machine Learning</h4><p>Like human learning, machine learning means that machine learn things by collecting data, then computing the data to get skills.</p>\n<h4 id=\"③-Summary\"><a href=\"#③-Summary\" class=\"headerlink\" title=\"③ Summary\"></a>③ Summary</h4><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a8ff8cec23221b4da00516a4de437336adfb1653/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-1%20When%20can%20machine%20learn-machine%20learning%20vs%20human%20learning-cropped.jpg\" alt=\"Human Learning and Machine Learning\"></p>\n<h3 id=\"2-Human-Learning-V-S-Machine-Learning\"><a href=\"#2-Human-Learning-V-S-Machine-Learning\" class=\"headerlink\" title=\"2) Human Learning V.S. Machine Learning\"></a>2) Human Learning V.S. Machine Learning</h3><p>既然人类和机器学习的过程一样，为什么我们还要耗费精力去让机器可以学习呢？</p>\n<ul>\n<li>一些数据或者信息，人类难以识别；</li>\n<li>学习的数据量特别大，人脑难以处理</li>\n<li>人脑处理问题的速度很慢，但是很多情况下要求系统能快速的给出答案</li>\n</ul>\n<p><br></p>\n<p>总结如下表：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Human Learning</th>\n<th>Machine Learning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Pros</td>\n<td>Learn emotionally and skillfully</td>\n<td>Processing big data</td>\n</tr>\n<tr>\n<td>Cons</td>\n<td>Cannot dealing with big data, cannot act fast</td>\n<td>Cannot work with human programming, no emotion</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"3-Key-to-Machine-Learning\"><a href=\"#3-Key-to-Machine-Learning\" class=\"headerlink\" title=\"3) Key to Machine Learning\"></a>3) Key to Machine Learning</h3><p>不是所以情况都可以使用机器学习，必须满足一下3个关键条件：</p>\n<ul>\n<li>存在一个模型，能让我们对它进行改进。（不需要改进，就不需要进行ML了）</li>\n<li>规则不容易找出。（如果太简单的话，用ML反而使得其反，耗费了人力物力）</li>\n<li>需要有数据的支持，且数据量理论上越大越好。（这给机器学习提供了保证，后面会介绍）</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"2-Application-of-Machine-Learning\"><a href=\"#2-Application-of-Machine-Learning\" class=\"headerlink\" title=\"2. Application of Machine Learning\"></a>2. Application of Machine Learning</h2><p>Machine Learning actually can apply to everything.</p>\n<p>E.g.,</p>\n<ul>\n<li>Daily need<ul>\n<li>Food<ul>\n<li>How does the food taste?</li>\n<li>How many chances that some specific people will like the food?</li>\n<li>…</li>\n</ul>\n</li>\n<li>Clothing<ul>\n<li>The information of the clothing.</li>\n<li>Fashion recommendation</li>\n<li>…</li>\n</ul>\n</li>\n<li>Housing<ul>\n<li>Energy load</li>\n<li>Sell price</li>\n<li>…</li>\n</ul>\n</li>\n<li>Transportation<ul>\n<li>Driving automation</li>\n<li>Transportation times</li>\n<li>Traffic jam possibilities</li>\n<li>…</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Education<ul>\n<li>Math tutoring system.</li>\n<li>Quiz generator</li>\n<li>…</li>\n</ul>\n</li>\n<li>Entertaining<ul>\n<li>Recommendation system</li>\n<li>Real view experiencing of traveling</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"3-Components-of-Machine-Learning\"><a href=\"#3-Components-of-Machine-Learning\" class=\"headerlink\" title=\"3. Components of Machine Learning\"></a>3. Components of Machine Learning</h2><blockquote>\n<p>以银行是否应该对客户发放信用卡作为例子</p>\n</blockquote>\n<h3 id=\"1-Basic-Notation\"><a href=\"#1-Basic-Notation\" class=\"headerlink\" title=\"1) Basic Notation\"></a>1) Basic Notation</h3><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b871ce6fb178e7433d1565e2b0a1791c11d8d39a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-2%20Basic%20Notation.png\" alt=\"Basic Notation\"></p>\n<center>Basic Notation<sup>[1]</sup></center>\n\n\n\n\n<p>1.输入(input)：$x∈X$（代表银行所掌握的用户信息）</p>\n<p>2.输出(output)：$y∈Y$ （是否会发信用卡给用户）</p>\n<p>3.未知的函数，即目标函数（target function）：$f：X→Y$（理想的信用卡发放公式）</p>\n<p>4.数据或者叫做资料（ data），即训练样本（ training examples）：$D = {（x_1, y_1）, (x_2, y_2), …, (x_n, y_n)}$（银行的历史记录）</p>\n<p>5.假设（hypothesis），根据训练样本得到的实际的函数：$g：X→Y$</p>\n<h3 id=\"2-Practical-Definition\"><a href=\"#2-Practical-Definition\" class=\"headerlink\" title=\"2) Practical Definition\"></a>2) Practical Definition</h3><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c5e0095cd06f69646a45c981a77c9bcb8033535a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-3%20Practical%20Definition%20of%20Machine%20Learning.png\" alt=\"Practical Definition\"></p>\n<center>Practical Definition<sup>[1]</sup></center>\n\n<p>机器学习算法（learning algorithm）一般用$A$表示。还多出来一个新的项目，就是假设空间或者叫做假设集合（hypothesis set）一般用$H$表示，而这时$A$的作用就是从$H$集合中挑选出它认为最好的假设从而得到函数$g$。</p>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"4-Machine-Learning-and-Other-Fields\"><a href=\"#4-Machine-Learning-and-Other-Fields\" class=\"headerlink\" title=\"4. Machine Learning and Other Fields\"></a>4. Machine Learning and Other Fields</h2><blockquote>\n<p>Machine Learning VS Data Mining, Artificial Intelligence, Statistic</p>\n</blockquote>\n<h3 id=\"1-Machine-Learning-V-S-Data-Mining\"><a href=\"#1-Machine-Learning-V-S-Data-Mining\" class=\"headerlink\" title=\"1) Machine Learning V.S. Data Mining\"></a>1) Machine Learning V.S. Data Mining</h3><p>机器学习与数据挖掘都叫知识发现（KDD Knowledge Discovery in Dataset）。</p>\n<ul>\n<li>两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。</li>\n<li>两者是互助的：ML需要大数据的支持才能保持能“学到东西”。</li>\n<li>数据挖掘更关注于从大量的数据中的计算问题。<br>总的来时，两者密不可分。</li>\n</ul>\n<h3 id=\"2-Machine-Learning-V-S-Artificial-Intelligence\"><a href=\"#2-Machine-Learning-V-S-Artificial-Intelligence\" class=\"headerlink\" title=\"2) Machine Learning V.S. Artificial Intelligence\"></a>2) Machine Learning V.S. Artificial Intelligence</h3><p>AI是通过特定的方法让机器能做出Intelligent的行为，ML属于AI的一个分支，是AI实现的一种方式</p>\n<h3 id=\"3-Machine-Learning-V-S-Statistic\"><a href=\"#3-Machine-Learning-V-S-Statistic\" class=\"headerlink\" title=\"3) Machine Learning V.S. Statistic\"></a>3) Machine Learning V.S. Statistic</h3><p>统计是通过对已知数据的处理，从而推断出未知的事件的属性<br>所以统计学是实现ML的一种方法，统计学里面有许多实用的工具可以用于证明ML。</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>机器学习类似于人类的学习</li>\n<li>机器学习的应用很广，可以说应用领域是各行各业</li>\n<li>机器学习包含：输入数据，输出结果，目标函数，假设函数 ，数据集</li>\n<li>机器学习ML与AI，DM， Statistics有关系， ML∈AI, ML≈DM, ML使用Statistics</li>\n</ol>\n<p>总的来说，机器学习的任务是找出一个假设函数 $g(x)$ ，使得假设 $g(x)$ 和目标函数 $f(x)$ 很接近，即 $g(x) \\approx f(x)$, 用第四章的概念可以解释为在测试时的错误率接近零 $E_{out} \\approx 0$</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1]机器学习基石(台湾大学-林轩田)\\1\\1 - 4 - Components of Machine Learning (11-45)</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"When-can-Machine-Learn-The-Learning-Problem\"><a href=\"#When-can-Machine-Learn-The-Learning-Problem\" class=\"headerlink\" title=\"When can Machine Learn? - The Learning Problem\"></a>When can Machine Learn? - The Learning Problem</h1><h2 id=\"1-The-Learning-Problem\"><a href=\"#1-The-Learning-Problem\" class=\"headerlink\" title=\"1. The Learning Problem\"></a>1. The Learning Problem</h2><p>To figure this out, we need to compare Human Learning and Machine Learning.</p>\n<h3 id=\"1-Human-Learning-and-Machine-Learning\"><a href=\"#1-Human-Learning-and-Machine-Learning\" class=\"headerlink\" title=\"1) Human Learning and Machine Learning\"></a>1) Human Learning and Machine Learning</h3><h4 id=\"①-Human-Learning\"><a href=\"#①-Human-Learning\" class=\"headerlink\" title=\"① Human Learning\"></a>① Human Learning</h4><p>Human learning means people learn from perception (E.g., observation, touching, hearing).</p>\n<h4 id=\"②-Machine-Learning\"><a href=\"#②-Machine-Learning\" class=\"headerlink\" title=\"② Machine Learning\"></a>② Machine Learning</h4><p>Like human learning, machine learning means that machine learn things by collecting data, then computing the data to get skills.</p>\n<h4 id=\"③-Summary\"><a href=\"#③-Summary\" class=\"headerlink\" title=\"③ Summary\"></a>③ Summary</h4><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a8ff8cec23221b4da00516a4de437336adfb1653/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-1%20When%20can%20machine%20learn-machine%20learning%20vs%20human%20learning-cropped.jpg\" alt=\"Human Learning and Machine Learning\"></p>\n<h3 id=\"2-Human-Learning-V-S-Machine-Learning\"><a href=\"#2-Human-Learning-V-S-Machine-Learning\" class=\"headerlink\" title=\"2) Human Learning V.S. Machine Learning\"></a>2) Human Learning V.S. Machine Learning</h3><p>既然人类和机器学习的过程一样，为什么我们还要耗费精力去让机器可以学习呢？</p>\n<ul>\n<li>一些数据或者信息，人类难以识别；</li>\n<li>学习的数据量特别大，人脑难以处理</li>\n<li>人脑处理问题的速度很慢，但是很多情况下要求系统能快速的给出答案</li>\n</ul>\n<p><br></p>\n<p>总结如下表：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Human Learning</th>\n<th>Machine Learning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Pros</td>\n<td>Learn emotionally and skillfully</td>\n<td>Processing big data</td>\n</tr>\n<tr>\n<td>Cons</td>\n<td>Cannot dealing with big data, cannot act fast</td>\n<td>Cannot work with human programming, no emotion</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"3-Key-to-Machine-Learning\"><a href=\"#3-Key-to-Machine-Learning\" class=\"headerlink\" title=\"3) Key to Machine Learning\"></a>3) Key to Machine Learning</h3><p>不是所以情况都可以使用机器学习，必须满足一下3个关键条件：</p>\n<ul>\n<li>存在一个模型，能让我们对它进行改进。（不需要改进，就不需要进行ML了）</li>\n<li>规则不容易找出。（如果太简单的话，用ML反而使得其反，耗费了人力物力）</li>\n<li>需要有数据的支持，且数据量理论上越大越好。（这给机器学习提供了保证，后面会介绍）</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"2-Application-of-Machine-Learning\"><a href=\"#2-Application-of-Machine-Learning\" class=\"headerlink\" title=\"2. Application of Machine Learning\"></a>2. Application of Machine Learning</h2><p>Machine Learning actually can apply to everything.</p>\n<p>E.g.,</p>\n<ul>\n<li>Daily need<ul>\n<li>Food<ul>\n<li>How does the food taste?</li>\n<li>How many chances that some specific people will like the food?</li>\n<li>…</li>\n</ul>\n</li>\n<li>Clothing<ul>\n<li>The information of the clothing.</li>\n<li>Fashion recommendation</li>\n<li>…</li>\n</ul>\n</li>\n<li>Housing<ul>\n<li>Energy load</li>\n<li>Sell price</li>\n<li>…</li>\n</ul>\n</li>\n<li>Transportation<ul>\n<li>Driving automation</li>\n<li>Transportation times</li>\n<li>Traffic jam possibilities</li>\n<li>…</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Education<ul>\n<li>Math tutoring system.</li>\n<li>Quiz generator</li>\n<li>…</li>\n</ul>\n</li>\n<li>Entertaining<ul>\n<li>Recommendation system</li>\n<li>Real view experiencing of traveling</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"3-Components-of-Machine-Learning\"><a href=\"#3-Components-of-Machine-Learning\" class=\"headerlink\" title=\"3. Components of Machine Learning\"></a>3. Components of Machine Learning</h2><blockquote>\n<p>以银行是否应该对客户发放信用卡作为例子</p>\n</blockquote>\n<h3 id=\"1-Basic-Notation\"><a href=\"#1-Basic-Notation\" class=\"headerlink\" title=\"1) Basic Notation\"></a>1) Basic Notation</h3><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b871ce6fb178e7433d1565e2b0a1791c11d8d39a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-2%20Basic%20Notation.png\" alt=\"Basic Notation\"></p>\n<center>Basic Notation<sup>[1]</sup></center>\n\n\n\n\n<p>1.输入(input)：$x∈X$（代表银行所掌握的用户信息）</p>\n<p>2.输出(output)：$y∈Y$ （是否会发信用卡给用户）</p>\n<p>3.未知的函数，即目标函数（target function）：$f：X→Y$（理想的信用卡发放公式）</p>\n<p>4.数据或者叫做资料（ data），即训练样本（ training examples）：$D = {（x_1, y_1）, (x_2, y_2), …, (x_n, y_n)}$（银行的历史记录）</p>\n<p>5.假设（hypothesis），根据训练样本得到的实际的函数：$g：X→Y$</p>\n<h3 id=\"2-Practical-Definition\"><a href=\"#2-Practical-Definition\" class=\"headerlink\" title=\"2) Practical Definition\"></a>2) Practical Definition</h3><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c5e0095cd06f69646a45c981a77c9bcb8033535a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter1-3%20Practical%20Definition%20of%20Machine%20Learning.png\" alt=\"Practical Definition\"></p>\n<center>Practical Definition<sup>[1]</sup></center>\n\n<p>机器学习算法（learning algorithm）一般用$A$表示。还多出来一个新的项目，就是假设空间或者叫做假设集合（hypothesis set）一般用$H$表示，而这时$A$的作用就是从$H$集合中挑选出它认为最好的假设从而得到函数$g$。</p>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"4-Machine-Learning-and-Other-Fields\"><a href=\"#4-Machine-Learning-and-Other-Fields\" class=\"headerlink\" title=\"4. Machine Learning and Other Fields\"></a>4. Machine Learning and Other Fields</h2><blockquote>\n<p>Machine Learning VS Data Mining, Artificial Intelligence, Statistic</p>\n</blockquote>\n<h3 id=\"1-Machine-Learning-V-S-Data-Mining\"><a href=\"#1-Machine-Learning-V-S-Data-Mining\" class=\"headerlink\" title=\"1) Machine Learning V.S. Data Mining\"></a>1) Machine Learning V.S. Data Mining</h3><p>机器学习与数据挖掘都叫知识发现（KDD Knowledge Discovery in Dataset）。</p>\n<ul>\n<li>两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。</li>\n<li>两者是互助的：ML需要大数据的支持才能保持能“学到东西”。</li>\n<li>数据挖掘更关注于从大量的数据中的计算问题。<br>总的来时，两者密不可分。</li>\n</ul>\n<h3 id=\"2-Machine-Learning-V-S-Artificial-Intelligence\"><a href=\"#2-Machine-Learning-V-S-Artificial-Intelligence\" class=\"headerlink\" title=\"2) Machine Learning V.S. Artificial Intelligence\"></a>2) Machine Learning V.S. Artificial Intelligence</h3><p>AI是通过特定的方法让机器能做出Intelligent的行为，ML属于AI的一个分支，是AI实现的一种方式</p>\n<h3 id=\"3-Machine-Learning-V-S-Statistic\"><a href=\"#3-Machine-Learning-V-S-Statistic\" class=\"headerlink\" title=\"3) Machine Learning V.S. Statistic\"></a>3) Machine Learning V.S. Statistic</h3><p>统计是通过对已知数据的处理，从而推断出未知的事件的属性<br>所以统计学是实现ML的一种方法，统计学里面有许多实用的工具可以用于证明ML。</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>机器学习类似于人类的学习</li>\n<li>机器学习的应用很广，可以说应用领域是各行各业</li>\n<li>机器学习包含：输入数据，输出结果，目标函数，假设函数 ，数据集</li>\n<li>机器学习ML与AI，DM， Statistics有关系， ML∈AI, ML≈DM, ML使用Statistics</li>\n</ol>\n<p>总的来说，机器学习的任务是找出一个假设函数 $g(x)$ ，使得假设 $g(x)$ 和目标函数 $f(x)$ 很接近，即 $g(x) \\approx f(x)$, 用第四章的概念可以解释为在测试时的错误率接近零 $E_{out} \\approx 0$</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1]机器学习基石(台湾大学-林轩田)\\1\\1 - 4 - Components of Machine Learning (11-45)</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"2.When can Machine Learn? - Learning to Answer Yes or No","date":"2017-10-02T23:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# When can Machine Learn? - Learning to Answer Yes or No\n\n## 1. Perceptron Hypothesis Set\n\n> 举例银行发信用卡进行解释\n\n### 1) Perceptron Hypothesis Set\n假设空间Hypothesis Set可以用感知器（Perceptron）表示。这种假设空间的思想就类似考试给的成绩，对每一题给一个特定的分数，即加权值(权重);\n然后再设计一个及格线，即所谓的阈值或门槛值（threshold），如果加权求和的值大于这个阈值就叫符合条件了，即输出1，小于对应的输出-1。\n表示为公式（1）\n\n$$\n\\begin{align}\n&h(x)∈H\n\\\\\n&h(x) = sign(\\sum_{i=1}^d w_ix_i)\n\\end{align}\n\\tag{$1$}\n$$\n\n其中 sign函数表示为取符号，即 sign(正数) = 1， sign（负数） = -1，如公式（2）所示。\n\n$$ sign(x)=\\left\\{\n\\begin{align}\n& +1 \\quad x>0 \\\\\n&-1 \\quad x<0\n\\end{align}\n\\right.\n\\tag{$2$}\n$$\n\n最后将$h(x)$ 与阈值作比较，得到公式（3）。\n$$\nh(x) = sign(\\sum_{i=1}^d w_ix_i - threshold)\n\\tag{$3$}\n$$\n\n### 2) Perceptron Hypothesis Set公式化简\n为了表达方便，可以对$h(x)$做数学上的讲话，如公式（4）所示。\n$$\n\\begin{align}\nh(x) &= sign(\\sum_{i=1}^d w_ix_i - threshold) \\\\\n&=sign(\\sum_{i=1}^d w_ix_i - threshold - 1) \\\\\n&=sign(\\sum_{i=1}^d w_ix_i - w_0-x_0) \\\\\n&=sign(\\sum_{i=0}^d w_ix_i) \\\\\n&=sign(w^T\\cdot  x)\n\\end{align}\n\\tag{$4$}\n$$\n> 如上所示，将负阈值表示为权值向量中的一项($w_0$)，而对应输入分量则被默认为1，用$x_0$ 最终将公式简化为两个向量内积的形式，其中T表示转置。\n\n\n------------------------------------------\n<br>\n<br>\n\n## 2. Peceptron Learning Algorithm(PLA)\n### 1) Introduction\n#### ① Question\nHypothesis Set $H$包含所有可能的Perceptron，那么具体选用哪一组Perceptron来组合成$g(x)$ 呢?\n\n#### ② Analyze\n- $g(x)$ 和目标函数f越接近越好，但问题是我们不知道f（如果知道了就不需要学习了）\n- 但是我们知道的是样本输入$x$在$f(x)$ 作用下得到的标记$y$。\n\n所以如果我们能使得$g(x)$ 在所有的样本输入中都能够得到跟函数$f(x)$ 作用过输入得到的输出一样的话，我们认为这时的$g(x)$ 是不错的。（在后面的章节还会在这种思想的基础上更深入的讨论这一问题）\n\n#### ③ Solution\n我们想到一个简单的方式，就是一步一步的修正错误的分类，在二维平面中可以想象成一条初始的直线，在经过不断的纠正它的错误（就是旋转平移之类的）使得最终的结果可以达到希望的效果。\n\n还要在重复上一节中已经得到的一个结论，在感知器模型中，每一个假设函数$g(x)$ 都对应一个权值向量。因此我们要做的就是不断修正这个权值向量使得最接近目标函数$f(x)$ 。即PLA算法\n\n------------------------------------------\n<br>\n<br>\n\n### 2) PLA\n#### ① PLA步骤\n首先我们在设置初始$w_0$（注意此处是向量不是向量的分量！），比如设置为0向量，然后使用训练样本来将权值向量修正的更接近目标函数$f(x)$。\n\n通过公式（5）来判断什么时候需要进行修正:\n$$\nsign( w_t^T \\cdot x_{n(t)} \\neq y_{(n)})\n\\tag{$5$}\n$$\n通过公式（6）来进行修正：\n$$\nw_{t+1} = w_t + y_{n(t)} \\cdot x_{n(t)}\n\\tag{$6$}\n$$\n\n#### ② 图示说明\n\n![PLA](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a335a9177d37b3ced5d4763a53f61f3f5b01e0e8/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-1%20PLA.png)\n\n<center>图一 PLA<sup>[1]</sup></center>\n\n>在本身标记为+1时，权值向量和输入向量的内积为负数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。\n\n>在本身标记为-1时，权值向量和输入向量的内积为正数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。\n\n重复查找错误样本和修改加权向量，直到再也找不到可以使公式(5)成立的样本为止，此时得到的加权向量，即为我们想要的最终$g(x)$\n\n具体流程如下图：\n\n![PLA](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/48df8c85a8eb4fd2af3331147cd25401dc2f728a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-2%20PLA%20procedure.jpg)\n\n<center>图二 PLA Procedure<sup>[1]</sup></center>\n\n\n\n> 疑问:如何查找错误样本点，如何确定没有错误的点了?\n\n一个简单的方式就是将训练样本编号，从1到n，以按从1到n的顺序不断查找错误点，如果没有错就自动的用下一个样本点继续查找，当从1到n这n个样本点都没有产生错误时，算法即结束得到$g(x)$ 。将这种方式的算法叫做Cyclic PLA。\n\n>新的疑问:\n>\n> 1.这个算法一定会找到一个能使所有的样本都不符合的情况吗(即是否会停下来)？\n>\n> 2.算法找到的真的是最好的g(x)吗？\n>\n> 3.应用到测试集中，结果还会一样吗？\n>\n> 详见 ③ PLA的理论保证\n\n#### ③ PLA的理论保证\n##### i) 前提条件\n首先很明显，要用一条直线做Binary Classification必须要求数据线性可分，否则不可以分开（想象一刀切）\n如图三：\n\n![Linear Separable](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/45e061db302ad3b6622daa8ecc48f802b861874c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-3%20linear%20seperable.png)\n\n<center>图三 Linear Seperable <sup>[1]</sup></center>\n\n##### ii) 理论推导\n首先PLA的核心是每次修正都让权值的向量$W^T$（T代表在第T次停下）更接近于理想的权值向量$W_f$,如何衡量两个向量的相似程度呢？内积，即对向量$W^T$和$W_f$做内积。\n> 内积的大小与2个向量的方向与大小都有关，所以如果我们直接做内积的话，会受到向量长度的影响，所以要做归一化处理，如公式7\n\n$$\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}}\n\\tag{$7$}\n$$\n\n公式用于衡量两个向量的方向的差别程度，但是怎么处理这个公式呢？\n\n先看分子,有之前的公式（6）我们可以得到公式（8）:\n$$\n{w_f^T \\cdot w_T} \\geq {w_f^T} \\cdot (w_{T-1} + y_{n(T-1)} \\cdot x_{n(T-1)})\n\\tag{$8$}\n$$\n\n如果PLA都会让权值越来越接近理想值，那么由之前的公式（5），我们其实可以推出公式（9）<--(因为$w_f$是最理想的值，那时候应该所有的预测值都和实际相等）和公式（10）<--(因为只有再犯错误的时候才改变，所以$W_t^T$与实际值会不一样):\n$$\ny_{n(t)} w_f^T x_{n(t)} \\geq \\min \\limits_n\\ y_n w_f^T x_n > 0\n\\tag{$9$}\n$$\n$$\ny_{n(t)} w_f^T x_{n(t)} \\leq 0\n\\tag{$10$}\n$$\n\n所以由公式（9），我们可以对公式（8）进一步限制他的范围：\n\n$$\n\\begin{align}\n{w_f^T \\cdot w_T} &\\geq {w_f^T} \\cdot (w_{T-1} + y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  &\\geq {w_f^T} w_{T-1} + \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  &\\geq {w_f^T} w_{T-2} + 2 - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  & ... \\\\\n                  &\\geq {w_f^T} w_{0} + T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n(w_0 = 0) \\quad   &\\geq 0 + T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot  x_{n(T-1)}) \\\\\n                  &\\geq T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\\\\n(\\min \\limits_n\\ y_n w_f^T x_n > 0) \\quad  & > 0\n\\end{align}\n\\tag{$11$}\n$$\n\n公式（10）中，我们关键是要得到\n$$\n{w_f^T \\cdot w_T} > T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)}\n$$\n\n接着，我们来研究分母:除了$w_T$不容易确定之外，$w_T$的L1范式也不容易得出,但是我们可以转换一下，结合公式（8）(10)来进行处理，即公式（12）。\n$$\n\\begin{align}\n{\\lvert \\lvert w_T\\rvert \\rvert} ^ 2 &= {\\lvert \\lvert w_{T-1} + y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2   \\\\\n                                     &= {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + {\\lvert \\lvert y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2 + 2y_{n(T-1)} w_{T-1} x_{n(T-1)}  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + {\\lvert \\lvert y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2 + 0  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-2} \\rvert \\rvert} ^ 2 + 2 - \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & ... \\\\\n                                     & \\leq {\\lvert \\lvert w_{0} \\rvert \\rvert} ^ 2 + T - \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & = T \\cdot \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2\n\\end{align}\n\\tag{$12$}\n$$\n\n到此为止，我们已经把分子的下限和分母的上限都找到了，这时候总体的值就应该大于等于某个值。有公式（11）（12），我们可以推论到公式（13）。\n$$\n\\begin{align}\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}} & \\geq \\frac{T \\cdot \\min\\limits_n y_n w_f^T x_n}{w_f^T \\sqrt{T \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert}}}  \\\\\n         & = \\sqrt{T} \\frac{\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n} {\\sqrt{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 }} \\\\\n         & = \\sqrt(T) \\cdot C\n         \\quad \\quad \\quad(其中 C = \\frac{\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n} {\\sqrt{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 }})\n\\end{align}\n\\tag{$13$}\n$$\n\n再有归一化之后的内积不可以大于1，所以公式（13）可以加以限制为公式（14)\n\n$$\n\\begin{align}\n1 \\geq\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}} & \\geq \\frac{T \\cdot \\min\\limits_n y_n w_f^T x_n}{w_f^T \\sqrt{T \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert}}}\n\\end{align}\n\\tag{$14$}\n$$\n\n求解公式（14）可以得到公式（15）\n\n$$\n\\begin{align}\nT &\\leq \\frac{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 } {\\left( {\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n}\\right)^2}  \\\\\n  & = \\frac{R^2}{ρ^2}\\\\\n &（其中：R^2 = \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2, \\quad\n                         ρ^2 = {\\left( {\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n}\\right)^2}\n\\end{align}\n\\tag{$15$}\n$$\n\n>从公式（15）可以看出，T是有上限的，所以可以说明在线性可分的情况下PLA算法最终会停止于T点，找到一个最接近目标函数的target function $g(x)$\n\n\n\n------------------------------------------\n<br>\n<br>\n\n## 3. Non-Separable Data\n### 1) PLA的缺陷\n上面提到的PLA算法是基于线性可分的数据集的，这种算法不实用，有2个原因。\n1. 在机器学习的时候，我们是不知道数据是否可分的，如果不可分，那样PLA不停止怎么办？\n2. 还是与PLA不停止有关，这个时候如果数据集真的线性可分，但是因为数据量太大，跑了很久都没有跑完，怎么办？我们不知道究竟是数据问题，还是数据量太大还没跑完的问题。\n\n所以这时候要求我们改进PLA算法。\n\n\n### 2）实际应用中线性可分多还是线性不可分的数据多？\n线性不可分的数据多：因为实际应用中存在很多噪音（Noise）。而出现噪音的原因有很多种，如：\n- 录入数据的时候出错\n- 采集数据的设备存在误差\n- ...\n\n所以存在噪音的时候，数据集会变成图四那样。\n\n![Noise](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d29c4fbd29627be6829f26874e8d23c2a246be6b/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-4%20Noise.png)\n\n图四 Noise\n\n\n因为噪音占的比例不会很大，所以最终我们还是可以找到犯错率最少的权值向量$w_g$，如公式（16）所示。\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{equation}\n  w_g = \\mathop{\\argmin}_{w}{\\sum_{n=1}^N sign(w^T \\cdot x_n) \\neq y_n}\n\\end{equation}\n\\tag{$16$}\n$$\n\n> 但是这个公式在数学上是NP难问题，我们无法直接求解，于是我们需要找出一种近似的算法来求解这个问题：pocket 算法。\n\n### 3) Pocket 算法\n#### ① Pocket算法原理\n> Pocket 算法属于贪心算法，即如果找到原来的权值更好的权值，才去做修改，否则停止。\n\n流程如下：\n1. 也是随机的初始化一个权值向量\n2. 随机的使用n个点中的一个点去发现是否有错误（此处与cyclic PLA使用的循环方式有所不同，不是按顺序一个一个的查看是否符合条件，而是在n个点中随机的抽取，这种方式可以增加其寻找最优解的速度）\n3. 和PLA一样使用公式2-5进行修正.\n4. 如果有了修正，则计算出刚刚修正过的权值向量和上一个权值向量到底谁犯的错误比较少，将少的保留重复第2步到第4步的动作。\n5. 假如很长时间都没有新的权值向量比当前的权值向量犯错更少，则返回该向量作为函数g。\n\n流程图如下：\n\n![Pocket Algorithm Procedure](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/73b88a89d3618c606f70bb3a6c915acb8686d351/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-5%20Pocket%20Algorithm%20procedure.jpg)\n\n<center> 图五 Pocket Algorithm Procedure </center>\n\n\n#### ②优缺点分析\n优点：Pocket算法不需要考虑数据集是否线性可分，都可以进行\n\n缺点：每有一个错误点，都要遍历整个数据集来获得该店的错误率$e$，所以计算量会很大\n\n\n------------------------------------------\n<br>\n<br>\n\n# Summary\n1. 首先介绍了Binary Classification中最简单的PLA算法\n2. 然后讨论了PLA算法能在某一步之后停下来。\n3. 最后讨论了PLA算法有缺陷：数据必须线性可分。所以根据这个缺点，引用了Pocket算法\n\n------------------------------------------\n<br>\n<br>\n\n# Reference\n[1]机器学习基石(台湾大学-林轩田)\\2\\2 - 2 - Perceptron Learning Algorithm (PLA) (19-46)\n\n\n<br>\n<br>\n------------------------------------------\n\n","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-03-2.When can Machine Learn - Learning to Answer Yes or No.md","raw":"---\ntitle:  2.When can Machine Learn? - Learning to Answer Yes or No\ndate: 2017-10-03 07:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# When can Machine Learn? - Learning to Answer Yes or No\n\n## 1. Perceptron Hypothesis Set\n\n> 举例银行发信用卡进行解释\n\n### 1) Perceptron Hypothesis Set\n假设空间Hypothesis Set可以用感知器（Perceptron）表示。这种假设空间的思想就类似考试给的成绩，对每一题给一个特定的分数，即加权值(权重);\n然后再设计一个及格线，即所谓的阈值或门槛值（threshold），如果加权求和的值大于这个阈值就叫符合条件了，即输出1，小于对应的输出-1。\n表示为公式（1）\n\n$$\n\\begin{align}\n&h(x)∈H\n\\\\\n&h(x) = sign(\\sum_{i=1}^d w_ix_i)\n\\end{align}\n\\tag{$1$}\n$$\n\n其中 sign函数表示为取符号，即 sign(正数) = 1， sign（负数） = -1，如公式（2）所示。\n\n$$ sign(x)=\\left\\{\n\\begin{align}\n& +1 \\quad x>0 \\\\\n&-1 \\quad x<0\n\\end{align}\n\\right.\n\\tag{$2$}\n$$\n\n最后将$h(x)$ 与阈值作比较，得到公式（3）。\n$$\nh(x) = sign(\\sum_{i=1}^d w_ix_i - threshold)\n\\tag{$3$}\n$$\n\n### 2) Perceptron Hypothesis Set公式化简\n为了表达方便，可以对$h(x)$做数学上的讲话，如公式（4）所示。\n$$\n\\begin{align}\nh(x) &= sign(\\sum_{i=1}^d w_ix_i - threshold) \\\\\n&=sign(\\sum_{i=1}^d w_ix_i - threshold - 1) \\\\\n&=sign(\\sum_{i=1}^d w_ix_i - w_0-x_0) \\\\\n&=sign(\\sum_{i=0}^d w_ix_i) \\\\\n&=sign(w^T\\cdot  x)\n\\end{align}\n\\tag{$4$}\n$$\n> 如上所示，将负阈值表示为权值向量中的一项($w_0$)，而对应输入分量则被默认为1，用$x_0$ 最终将公式简化为两个向量内积的形式，其中T表示转置。\n\n\n------------------------------------------\n<br>\n<br>\n\n## 2. Peceptron Learning Algorithm(PLA)\n### 1) Introduction\n#### ① Question\nHypothesis Set $H$包含所有可能的Perceptron，那么具体选用哪一组Perceptron来组合成$g(x)$ 呢?\n\n#### ② Analyze\n- $g(x)$ 和目标函数f越接近越好，但问题是我们不知道f（如果知道了就不需要学习了）\n- 但是我们知道的是样本输入$x$在$f(x)$ 作用下得到的标记$y$。\n\n所以如果我们能使得$g(x)$ 在所有的样本输入中都能够得到跟函数$f(x)$ 作用过输入得到的输出一样的话，我们认为这时的$g(x)$ 是不错的。（在后面的章节还会在这种思想的基础上更深入的讨论这一问题）\n\n#### ③ Solution\n我们想到一个简单的方式，就是一步一步的修正错误的分类，在二维平面中可以想象成一条初始的直线，在经过不断的纠正它的错误（就是旋转平移之类的）使得最终的结果可以达到希望的效果。\n\n还要在重复上一节中已经得到的一个结论，在感知器模型中，每一个假设函数$g(x)$ 都对应一个权值向量。因此我们要做的就是不断修正这个权值向量使得最接近目标函数$f(x)$ 。即PLA算法\n\n------------------------------------------\n<br>\n<br>\n\n### 2) PLA\n#### ① PLA步骤\n首先我们在设置初始$w_0$（注意此处是向量不是向量的分量！），比如设置为0向量，然后使用训练样本来将权值向量修正的更接近目标函数$f(x)$。\n\n通过公式（5）来判断什么时候需要进行修正:\n$$\nsign( w_t^T \\cdot x_{n(t)} \\neq y_{(n)})\n\\tag{$5$}\n$$\n通过公式（6）来进行修正：\n$$\nw_{t+1} = w_t + y_{n(t)} \\cdot x_{n(t)}\n\\tag{$6$}\n$$\n\n#### ② 图示说明\n\n![PLA](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a335a9177d37b3ced5d4763a53f61f3f5b01e0e8/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-1%20PLA.png)\n\n<center>图一 PLA<sup>[1]</sup></center>\n\n>在本身标记为+1时，权值向量和输入向量的内积为负数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。\n\n>在本身标记为-1时，权值向量和输入向量的内积为正数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。\n\n重复查找错误样本和修改加权向量，直到再也找不到可以使公式(5)成立的样本为止，此时得到的加权向量，即为我们想要的最终$g(x)$\n\n具体流程如下图：\n\n![PLA](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/48df8c85a8eb4fd2af3331147cd25401dc2f728a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-2%20PLA%20procedure.jpg)\n\n<center>图二 PLA Procedure<sup>[1]</sup></center>\n\n\n\n> 疑问:如何查找错误样本点，如何确定没有错误的点了?\n\n一个简单的方式就是将训练样本编号，从1到n，以按从1到n的顺序不断查找错误点，如果没有错就自动的用下一个样本点继续查找，当从1到n这n个样本点都没有产生错误时，算法即结束得到$g(x)$ 。将这种方式的算法叫做Cyclic PLA。\n\n>新的疑问:\n>\n> 1.这个算法一定会找到一个能使所有的样本都不符合的情况吗(即是否会停下来)？\n>\n> 2.算法找到的真的是最好的g(x)吗？\n>\n> 3.应用到测试集中，结果还会一样吗？\n>\n> 详见 ③ PLA的理论保证\n\n#### ③ PLA的理论保证\n##### i) 前提条件\n首先很明显，要用一条直线做Binary Classification必须要求数据线性可分，否则不可以分开（想象一刀切）\n如图三：\n\n![Linear Separable](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/45e061db302ad3b6622daa8ecc48f802b861874c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-3%20linear%20seperable.png)\n\n<center>图三 Linear Seperable <sup>[1]</sup></center>\n\n##### ii) 理论推导\n首先PLA的核心是每次修正都让权值的向量$W^T$（T代表在第T次停下）更接近于理想的权值向量$W_f$,如何衡量两个向量的相似程度呢？内积，即对向量$W^T$和$W_f$做内积。\n> 内积的大小与2个向量的方向与大小都有关，所以如果我们直接做内积的话，会受到向量长度的影响，所以要做归一化处理，如公式7\n\n$$\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}}\n\\tag{$7$}\n$$\n\n公式用于衡量两个向量的方向的差别程度，但是怎么处理这个公式呢？\n\n先看分子,有之前的公式（6）我们可以得到公式（8）:\n$$\n{w_f^T \\cdot w_T} \\geq {w_f^T} \\cdot (w_{T-1} + y_{n(T-1)} \\cdot x_{n(T-1)})\n\\tag{$8$}\n$$\n\n如果PLA都会让权值越来越接近理想值，那么由之前的公式（5），我们其实可以推出公式（9）<--(因为$w_f$是最理想的值，那时候应该所有的预测值都和实际相等）和公式（10）<--(因为只有再犯错误的时候才改变，所以$W_t^T$与实际值会不一样):\n$$\ny_{n(t)} w_f^T x_{n(t)} \\geq \\min \\limits_n\\ y_n w_f^T x_n > 0\n\\tag{$9$}\n$$\n$$\ny_{n(t)} w_f^T x_{n(t)} \\leq 0\n\\tag{$10$}\n$$\n\n所以由公式（9），我们可以对公式（8）进一步限制他的范围：\n\n$$\n\\begin{align}\n{w_f^T \\cdot w_T} &\\geq {w_f^T} \\cdot (w_{T-1} + y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  &\\geq {w_f^T} w_{T-1} + \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  &\\geq {w_f^T} w_{T-2} + 2 - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  & ... \\\\\n                  &\\geq {w_f^T} w_{0} + T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n(w_0 = 0) \\quad   &\\geq 0 + T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot  x_{n(T-1)}) \\\\\n                  &\\geq T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\\\\n(\\min \\limits_n\\ y_n w_f^T x_n > 0) \\quad  & > 0\n\\end{align}\n\\tag{$11$}\n$$\n\n公式（10）中，我们关键是要得到\n$$\n{w_f^T \\cdot w_T} > T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)}\n$$\n\n接着，我们来研究分母:除了$w_T$不容易确定之外，$w_T$的L1范式也不容易得出,但是我们可以转换一下，结合公式（8）(10)来进行处理，即公式（12）。\n$$\n\\begin{align}\n{\\lvert \\lvert w_T\\rvert \\rvert} ^ 2 &= {\\lvert \\lvert w_{T-1} + y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2   \\\\\n                                     &= {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + {\\lvert \\lvert y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2 + 2y_{n(T-1)} w_{T-1} x_{n(T-1)}  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + {\\lvert \\lvert y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2 + 0  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-2} \\rvert \\rvert} ^ 2 + 2 - \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & ... \\\\\n                                     & \\leq {\\lvert \\lvert w_{0} \\rvert \\rvert} ^ 2 + T - \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & = T \\cdot \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2\n\\end{align}\n\\tag{$12$}\n$$\n\n到此为止，我们已经把分子的下限和分母的上限都找到了，这时候总体的值就应该大于等于某个值。有公式（11）（12），我们可以推论到公式（13）。\n$$\n\\begin{align}\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}} & \\geq \\frac{T \\cdot \\min\\limits_n y_n w_f^T x_n}{w_f^T \\sqrt{T \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert}}}  \\\\\n         & = \\sqrt{T} \\frac{\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n} {\\sqrt{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 }} \\\\\n         & = \\sqrt(T) \\cdot C\n         \\quad \\quad \\quad(其中 C = \\frac{\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n} {\\sqrt{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 }})\n\\end{align}\n\\tag{$13$}\n$$\n\n再有归一化之后的内积不可以大于1，所以公式（13）可以加以限制为公式（14)\n\n$$\n\\begin{align}\n1 \\geq\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}} & \\geq \\frac{T \\cdot \\min\\limits_n y_n w_f^T x_n}{w_f^T \\sqrt{T \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert}}}\n\\end{align}\n\\tag{$14$}\n$$\n\n求解公式（14）可以得到公式（15）\n\n$$\n\\begin{align}\nT &\\leq \\frac{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 } {\\left( {\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n}\\right)^2}  \\\\\n  & = \\frac{R^2}{ρ^2}\\\\\n &（其中：R^2 = \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2, \\quad\n                         ρ^2 = {\\left( {\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n}\\right)^2}\n\\end{align}\n\\tag{$15$}\n$$\n\n>从公式（15）可以看出，T是有上限的，所以可以说明在线性可分的情况下PLA算法最终会停止于T点，找到一个最接近目标函数的target function $g(x)$\n\n\n\n------------------------------------------\n<br>\n<br>\n\n## 3. Non-Separable Data\n### 1) PLA的缺陷\n上面提到的PLA算法是基于线性可分的数据集的，这种算法不实用，有2个原因。\n1. 在机器学习的时候，我们是不知道数据是否可分的，如果不可分，那样PLA不停止怎么办？\n2. 还是与PLA不停止有关，这个时候如果数据集真的线性可分，但是因为数据量太大，跑了很久都没有跑完，怎么办？我们不知道究竟是数据问题，还是数据量太大还没跑完的问题。\n\n所以这时候要求我们改进PLA算法。\n\n\n### 2）实际应用中线性可分多还是线性不可分的数据多？\n线性不可分的数据多：因为实际应用中存在很多噪音（Noise）。而出现噪音的原因有很多种，如：\n- 录入数据的时候出错\n- 采集数据的设备存在误差\n- ...\n\n所以存在噪音的时候，数据集会变成图四那样。\n\n![Noise](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d29c4fbd29627be6829f26874e8d23c2a246be6b/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-4%20Noise.png)\n\n图四 Noise\n\n\n因为噪音占的比例不会很大，所以最终我们还是可以找到犯错率最少的权值向量$w_g$，如公式（16）所示。\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{equation}\n  w_g = \\mathop{\\argmin}_{w}{\\sum_{n=1}^N sign(w^T \\cdot x_n) \\neq y_n}\n\\end{equation}\n\\tag{$16$}\n$$\n\n> 但是这个公式在数学上是NP难问题，我们无法直接求解，于是我们需要找出一种近似的算法来求解这个问题：pocket 算法。\n\n### 3) Pocket 算法\n#### ① Pocket算法原理\n> Pocket 算法属于贪心算法，即如果找到原来的权值更好的权值，才去做修改，否则停止。\n\n流程如下：\n1. 也是随机的初始化一个权值向量\n2. 随机的使用n个点中的一个点去发现是否有错误（此处与cyclic PLA使用的循环方式有所不同，不是按顺序一个一个的查看是否符合条件，而是在n个点中随机的抽取，这种方式可以增加其寻找最优解的速度）\n3. 和PLA一样使用公式2-5进行修正.\n4. 如果有了修正，则计算出刚刚修正过的权值向量和上一个权值向量到底谁犯的错误比较少，将少的保留重复第2步到第4步的动作。\n5. 假如很长时间都没有新的权值向量比当前的权值向量犯错更少，则返回该向量作为函数g。\n\n流程图如下：\n\n![Pocket Algorithm Procedure](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/73b88a89d3618c606f70bb3a6c915acb8686d351/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-5%20Pocket%20Algorithm%20procedure.jpg)\n\n<center> 图五 Pocket Algorithm Procedure </center>\n\n\n#### ②优缺点分析\n优点：Pocket算法不需要考虑数据集是否线性可分，都可以进行\n\n缺点：每有一个错误点，都要遍历整个数据集来获得该店的错误率$e$，所以计算量会很大\n\n\n------------------------------------------\n<br>\n<br>\n\n# Summary\n1. 首先介绍了Binary Classification中最简单的PLA算法\n2. 然后讨论了PLA算法能在某一步之后停下来。\n3. 最后讨论了PLA算法有缺陷：数据必须线性可分。所以根据这个缺点，引用了Pocket算法\n\n------------------------------------------\n<br>\n<br>\n\n# Reference\n[1]机器学习基石(台湾大学-林轩田)\\2\\2 - 2 - Perceptron Learning Algorithm (PLA) (19-46)\n\n\n<br>\n<br>\n------------------------------------------\n\n","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-03-2.When can Machine Learn - Learning to Answer Yes or No","published":1,"updated":"2018-04-14T19:42:06.502Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2dr000wrwtjvhch2302","content":"<h1 id=\"When-can-Machine-Learn-Learning-to-Answer-Yes-or-No\"><a href=\"#When-can-Machine-Learn-Learning-to-Answer-Yes-or-No\" class=\"headerlink\" title=\"When can Machine Learn? - Learning to Answer Yes or No\"></a>When can Machine Learn? - Learning to Answer Yes or No</h1><h2 id=\"1-Perceptron-Hypothesis-Set\"><a href=\"#1-Perceptron-Hypothesis-Set\" class=\"headerlink\" title=\"1. Perceptron Hypothesis Set\"></a>1. Perceptron Hypothesis Set</h2><blockquote>\n<p>举例银行发信用卡进行解释</p>\n</blockquote>\n<h3 id=\"1-Perceptron-Hypothesis-Set-1\"><a href=\"#1-Perceptron-Hypothesis-Set-1\" class=\"headerlink\" title=\"1) Perceptron Hypothesis Set\"></a>1) Perceptron Hypothesis Set</h3><p>假设空间Hypothesis Set可以用感知器（Perceptron）表示。这种假设空间的思想就类似考试给的成绩，对每一题给一个特定的分数，即加权值(权重);<br>然后再设计一个及格线，即所谓的阈值或门槛值（threshold），如果加权求和的值大于这个阈值就叫符合条件了，即输出1，小于对应的输出-1。<br>表示为公式（1）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&h(x)∈H\n\\\\\n&h(x) = sign(\\sum_{i=1}^d w_ix_i)\n\\end{align}\n\\tag{$1$}</script><p>其中 sign函数表示为取符号，即 sign(正数) = 1， sign（负数） = -1，如公式（2）所示。</p>\n<script type=\"math/tex; mode=display\">sign(x)=\\left\\{\n\\begin{align}\n& +1 \\quad x>0 \\\\\n&-1 \\quad x<0\n\\end{align}\n\\right.\n\\tag{$2$}</script><p>最后将$h(x)$ 与阈值作比较，得到公式（3）。</p>\n<script type=\"math/tex; mode=display\">\nh(x) = sign(\\sum_{i=1}^d w_ix_i - threshold)\n\\tag{$3$}</script><h3 id=\"2-Perceptron-Hypothesis-Set公式化简\"><a href=\"#2-Perceptron-Hypothesis-Set公式化简\" class=\"headerlink\" title=\"2) Perceptron Hypothesis Set公式化简\"></a>2) Perceptron Hypothesis Set公式化简</h3><p>为了表达方便，可以对$h(x)$做数学上的讲话，如公式（4）所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh(x) &= sign(\\sum_{i=1}^d w_ix_i - threshold) \\\\\n&=sign(\\sum_{i=1}^d w_ix_i - threshold - 1) \\\\\n&=sign(\\sum_{i=1}^d w_ix_i - w_0-x_0) \\\\\n&=sign(\\sum_{i=0}^d w_ix_i) \\\\\n&=sign(w^T\\cdot  x)\n\\end{align}\n\\tag{$4$}</script><blockquote>\n<p>如上所示，将负阈值表示为权值向量中的一项($w_0$)，而对应输入分量则被默认为1，用$x_0$ 最终将公式简化为两个向量内积的形式，其中T表示转置。</p>\n</blockquote>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"2-Peceptron-Learning-Algorithm-PLA\"><a href=\"#2-Peceptron-Learning-Algorithm-PLA\" class=\"headerlink\" title=\"2. Peceptron Learning Algorithm(PLA)\"></a>2. Peceptron Learning Algorithm(PLA)</h2><h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1) Introduction\"></a>1) Introduction</h3><h4 id=\"①-Question\"><a href=\"#①-Question\" class=\"headerlink\" title=\"① Question\"></a>① Question</h4><p>Hypothesis Set $H$包含所有可能的Perceptron，那么具体选用哪一组Perceptron来组合成$g(x)$ 呢?</p>\n<h4 id=\"②-Analyze\"><a href=\"#②-Analyze\" class=\"headerlink\" title=\"② Analyze\"></a>② Analyze</h4><ul>\n<li>$g(x)$ 和目标函数f越接近越好，但问题是我们不知道f（如果知道了就不需要学习了）</li>\n<li>但是我们知道的是样本输入$x$在$f(x)$ 作用下得到的标记$y$。</li>\n</ul>\n<p>所以如果我们能使得$g(x)$ 在所有的样本输入中都能够得到跟函数$f(x)$ 作用过输入得到的输出一样的话，我们认为这时的$g(x)$ 是不错的。（在后面的章节还会在这种思想的基础上更深入的讨论这一问题）</p>\n<h4 id=\"③-Solution\"><a href=\"#③-Solution\" class=\"headerlink\" title=\"③ Solution\"></a>③ Solution</h4><p>我们想到一个简单的方式，就是一步一步的修正错误的分类，在二维平面中可以想象成一条初始的直线，在经过不断的纠正它的错误（就是旋转平移之类的）使得最终的结果可以达到希望的效果。</p>\n<p>还要在重复上一节中已经得到的一个结论，在感知器模型中，每一个假设函数$g(x)$ 都对应一个权值向量。因此我们要做的就是不断修正这个权值向量使得最接近目标函数$f(x)$ 。即PLA算法</p>\n<hr>\n<p><br><br><br></p>\n<h3 id=\"2-PLA\"><a href=\"#2-PLA\" class=\"headerlink\" title=\"2) PLA\"></a>2) PLA</h3><h4 id=\"①-PLA步骤\"><a href=\"#①-PLA步骤\" class=\"headerlink\" title=\"① PLA步骤\"></a>① PLA步骤</h4><p>首先我们在设置初始$w_0$（注意此处是向量不是向量的分量！），比如设置为0向量，然后使用训练样本来将权值向量修正的更接近目标函数$f(x)$。</p>\n<p>通过公式（5）来判断什么时候需要进行修正:</p>\n<script type=\"math/tex; mode=display\">\nsign( w_t^T \\cdot x_{n(t)} \\neq y_{(n)})\n\\tag{$5$}</script><p>通过公式（6）来进行修正：</p>\n<script type=\"math/tex; mode=display\">\nw_{t+1} = w_t + y_{n(t)} \\cdot x_{n(t)}\n\\tag{$6$}</script><h4 id=\"②-图示说明\"><a href=\"#②-图示说明\" class=\"headerlink\" title=\"② 图示说明\"></a>② 图示说明</h4><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a335a9177d37b3ced5d4763a53f61f3f5b01e0e8/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-1%20PLA.png\" alt=\"PLA\"></p>\n<center>图一 PLA<sup>[1]</sup></center>\n\n<blockquote>\n<p>在本身标记为+1时，权值向量和输入向量的内积为负数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。</p>\n<p>在本身标记为-1时，权值向量和输入向量的内积为正数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。</p>\n</blockquote>\n<p>重复查找错误样本和修改加权向量，直到再也找不到可以使公式(5)成立的样本为止，此时得到的加权向量，即为我们想要的最终$g(x)$</p>\n<p>具体流程如下图：</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/48df8c85a8eb4fd2af3331147cd25401dc2f728a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-2%20PLA%20procedure.jpg\" alt=\"PLA\"></p>\n<center>图二 PLA Procedure<sup>[1]</sup></center>\n\n\n\n<blockquote>\n<p>疑问:如何查找错误样本点，如何确定没有错误的点了?</p>\n</blockquote>\n<p>一个简单的方式就是将训练样本编号，从1到n，以按从1到n的顺序不断查找错误点，如果没有错就自动的用下一个样本点继续查找，当从1到n这n个样本点都没有产生错误时，算法即结束得到$g(x)$ 。将这种方式的算法叫做Cyclic PLA。</p>\n<blockquote>\n<p>新的疑问:</p>\n<p>1.这个算法一定会找到一个能使所有的样本都不符合的情况吗(即是否会停下来)？</p>\n<p>2.算法找到的真的是最好的g(x)吗？</p>\n<p>3.应用到测试集中，结果还会一样吗？</p>\n<p>详见 ③ PLA的理论保证</p>\n</blockquote>\n<h4 id=\"③-PLA的理论保证\"><a href=\"#③-PLA的理论保证\" class=\"headerlink\" title=\"③ PLA的理论保证\"></a>③ PLA的理论保证</h4><h5 id=\"i-前提条件\"><a href=\"#i-前提条件\" class=\"headerlink\" title=\"i) 前提条件\"></a>i) 前提条件</h5><p>首先很明显，要用一条直线做Binary Classification必须要求数据线性可分，否则不可以分开（想象一刀切）<br>如图三：</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/45e061db302ad3b6622daa8ecc48f802b861874c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-3%20linear%20seperable.png\" alt=\"Linear Separable\"></p>\n<center>图三 Linear Seperable <sup>[1]</sup></center>\n\n<h5 id=\"ii-理论推导\"><a href=\"#ii-理论推导\" class=\"headerlink\" title=\"ii) 理论推导\"></a>ii) 理论推导</h5><p>首先PLA的核心是每次修正都让权值的向量$W^T$（T代表在第T次停下）更接近于理想的权值向量$W_f$,如何衡量两个向量的相似程度呢？内积，即对向量$W^T$和$W_f$做内积。</p>\n<blockquote>\n<p>内积的大小与2个向量的方向与大小都有关，所以如果我们直接做内积的话，会受到向量长度的影响，所以要做归一化处理，如公式7</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}}\n\\tag{$7$}</script><p>公式用于衡量两个向量的方向的差别程度，但是怎么处理这个公式呢？</p>\n<p>先看分子,有之前的公式（6）我们可以得到公式（8）:</p>\n<script type=\"math/tex; mode=display\">\n{w_f^T \\cdot w_T} \\geq {w_f^T} \\cdot (w_{T-1} + y_{n(T-1)} \\cdot x_{n(T-1)})\n\\tag{$8$}</script><p>如果PLA都会让权值越来越接近理想值，那么由之前的公式（5），我们其实可以推出公式（9）&lt;—(因为$w_f$是最理想的值，那时候应该所有的预测值都和实际相等）和公式（10）&lt;—(因为只有再犯错误的时候才改变，所以$W_t^T$与实际值会不一样):</p>\n<script type=\"math/tex; mode=display\">\ny_{n(t)} w_f^T x_{n(t)} \\geq \\min \\limits_n\\ y_n w_f^T x_n > 0\n\\tag{$9$}</script><script type=\"math/tex; mode=display\">\ny_{n(t)} w_f^T x_{n(t)} \\leq 0\n\\tag{$10$}</script><p>所以由公式（9），我们可以对公式（8）进一步限制他的范围：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n{w_f^T \\cdot w_T} &\\geq {w_f^T} \\cdot (w_{T-1} + y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  &\\geq {w_f^T} w_{T-1} + \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  &\\geq {w_f^T} w_{T-2} + 2 - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  & ... \\\\\n                  &\\geq {w_f^T} w_{0} + T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n(w_0 = 0) \\quad   &\\geq 0 + T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot  x_{n(T-1)}) \\\\\n                  &\\geq T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\\\\n(\\min \\limits_n\\ y_n w_f^T x_n > 0) \\quad  & > 0\n\\end{align}\n\\tag{$11$}</script><p>公式（10）中，我们关键是要得到</p>\n<script type=\"math/tex; mode=display\">\n{w_f^T \\cdot w_T} > T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)}</script><p>接着，我们来研究分母:除了$w_T$不容易确定之外，$w_T$的L1范式也不容易得出,但是我们可以转换一下，结合公式（8）(10)来进行处理，即公式（12）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n{\\lvert \\lvert w_T\\rvert \\rvert} ^ 2 &= {\\lvert \\lvert w_{T-1} + y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2   \\\\\n                                     &= {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + {\\lvert \\lvert y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2 + 2y_{n(T-1)} w_{T-1} x_{n(T-1)}  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + {\\lvert \\lvert y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2 + 0  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-2} \\rvert \\rvert} ^ 2 + 2 - \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & ... \\\\\n                                     & \\leq {\\lvert \\lvert w_{0} \\rvert \\rvert} ^ 2 + T - \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & = T \\cdot \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2\n\\end{align}\n\\tag{$12$}</script><p>到此为止，我们已经把分子的下限和分母的上限都找到了，这时候总体的值就应该大于等于某个值。有公式（11）（12），我们可以推论到公式（13）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}} & \\geq \\frac{T \\cdot \\min\\limits_n y_n w_f^T x_n}{w_f^T \\sqrt{T \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert}}}  \\\\\n         & = \\sqrt{T} \\frac{\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n} {\\sqrt{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 }} \\\\\n         & = \\sqrt(T) \\cdot C\n         \\quad \\quad \\quad(其中 C = \\frac{\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n} {\\sqrt{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 }})\n\\end{align}\n\\tag{$13$}</script><p>再有归一化之后的内积不可以大于1，所以公式（13）可以加以限制为公式（14)</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n1 \\geq\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}} & \\geq \\frac{T \\cdot \\min\\limits_n y_n w_f^T x_n}{w_f^T \\sqrt{T \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert}}}\n\\end{align}\n\\tag{$14$}</script><p>求解公式（14）可以得到公式（15）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nT &\\leq \\frac{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 } {\\left( {\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n}\\right)^2}  \\\\\n  & = \\frac{R^2}{ρ^2}\\\\\n &（其中：R^2 = \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2, \\quad\n                         ρ^2 = {\\left( {\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n}\\right)^2}\n\\end{align}\n\\tag{$15$}</script><blockquote>\n<p>从公式（15）可以看出，T是有上限的，所以可以说明在线性可分的情况下PLA算法最终会停止于T点，找到一个最接近目标函数的target function $g(x)$</p>\n</blockquote>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"3-Non-Separable-Data\"><a href=\"#3-Non-Separable-Data\" class=\"headerlink\" title=\"3. Non-Separable Data\"></a>3. Non-Separable Data</h2><h3 id=\"1-PLA的缺陷\"><a href=\"#1-PLA的缺陷\" class=\"headerlink\" title=\"1) PLA的缺陷\"></a>1) PLA的缺陷</h3><p>上面提到的PLA算法是基于线性可分的数据集的，这种算法不实用，有2个原因。</p>\n<ol>\n<li>在机器学习的时候，我们是不知道数据是否可分的，如果不可分，那样PLA不停止怎么办？</li>\n<li>还是与PLA不停止有关，这个时候如果数据集真的线性可分，但是因为数据量太大，跑了很久都没有跑完，怎么办？我们不知道究竟是数据问题，还是数据量太大还没跑完的问题。</li>\n</ol>\n<p>所以这时候要求我们改进PLA算法。</p>\n<h3 id=\"2）实际应用中线性可分多还是线性不可分的数据多？\"><a href=\"#2）实际应用中线性可分多还是线性不可分的数据多？\" class=\"headerlink\" title=\"2）实际应用中线性可分多还是线性不可分的数据多？\"></a>2）实际应用中线性可分多还是线性不可分的数据多？</h3><p>线性不可分的数据多：因为实际应用中存在很多噪音（Noise）。而出现噪音的原因有很多种，如：</p>\n<ul>\n<li>录入数据的时候出错</li>\n<li>采集数据的设备存在误差</li>\n<li>…</li>\n</ul>\n<p>所以存在噪音的时候，数据集会变成图四那样。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d29c4fbd29627be6829f26874e8d23c2a246be6b/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-4%20Noise.png\" alt=\"Noise\"></p>\n<p>图四 Noise</p>\n<p>因为噪音占的比例不会很大，所以最终我们还是可以找到犯错率最少的权值向量$w_g$，如公式（16）所示。</p>\n<script type=\"math/tex; mode=display\">\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{equation}\n  w_g = \\mathop{\\argmin}_{w}{\\sum_{n=1}^N sign(w^T \\cdot x_n) \\neq y_n}\n\\end{equation}\n\\tag{$16$}</script><blockquote>\n<p>但是这个公式在数学上是NP难问题，我们无法直接求解，于是我们需要找出一种近似的算法来求解这个问题：pocket 算法。</p>\n</blockquote>\n<h3 id=\"3-Pocket-算法\"><a href=\"#3-Pocket-算法\" class=\"headerlink\" title=\"3) Pocket 算法\"></a>3) Pocket 算法</h3><h4 id=\"①-Pocket算法原理\"><a href=\"#①-Pocket算法原理\" class=\"headerlink\" title=\"① Pocket算法原理\"></a>① Pocket算法原理</h4><blockquote>\n<p>Pocket 算法属于贪心算法，即如果找到原来的权值更好的权值，才去做修改，否则停止。</p>\n</blockquote>\n<p>流程如下：</p>\n<ol>\n<li>也是随机的初始化一个权值向量</li>\n<li>随机的使用n个点中的一个点去发现是否有错误（此处与cyclic PLA使用的循环方式有所不同，不是按顺序一个一个的查看是否符合条件，而是在n个点中随机的抽取，这种方式可以增加其寻找最优解的速度）</li>\n<li>和PLA一样使用公式2-5进行修正.</li>\n<li>如果有了修正，则计算出刚刚修正过的权值向量和上一个权值向量到底谁犯的错误比较少，将少的保留重复第2步到第4步的动作。</li>\n<li>假如很长时间都没有新的权值向量比当前的权值向量犯错更少，则返回该向量作为函数g。</li>\n</ol>\n<p>流程图如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/73b88a89d3618c606f70bb3a6c915acb8686d351/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-5%20Pocket%20Algorithm%20procedure.jpg\" alt=\"Pocket Algorithm Procedure\"></p>\n<center> 图五 Pocket Algorithm Procedure </center>\n\n\n<h4 id=\"②优缺点分析\"><a href=\"#②优缺点分析\" class=\"headerlink\" title=\"②优缺点分析\"></a>②优缺点分析</h4><p>优点：Pocket算法不需要考虑数据集是否线性可分，都可以进行</p>\n<p>缺点：每有一个错误点，都要遍历整个数据集来获得该店的错误率$e$，所以计算量会很大</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了Binary Classification中最简单的PLA算法</li>\n<li>然后讨论了PLA算法能在某一步之后停下来。</li>\n<li>最后讨论了PLA算法有缺陷：数据必须线性可分。所以根据这个缺点，引用了Pocket算法</li>\n</ol>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1]机器学习基石(台湾大学-林轩田)\\2\\2 - 2 - Perceptron Learning Algorithm (PLA) (19-46)</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"When-can-Machine-Learn-Learning-to-Answer-Yes-or-No\"><a href=\"#When-can-Machine-Learn-Learning-to-Answer-Yes-or-No\" class=\"headerlink\" title=\"When can Machine Learn? - Learning to Answer Yes or No\"></a>When can Machine Learn? - Learning to Answer Yes or No</h1><h2 id=\"1-Perceptron-Hypothesis-Set\"><a href=\"#1-Perceptron-Hypothesis-Set\" class=\"headerlink\" title=\"1. Perceptron Hypothesis Set\"></a>1. Perceptron Hypothesis Set</h2><blockquote>\n<p>举例银行发信用卡进行解释</p>\n</blockquote>\n<h3 id=\"1-Perceptron-Hypothesis-Set-1\"><a href=\"#1-Perceptron-Hypothesis-Set-1\" class=\"headerlink\" title=\"1) Perceptron Hypothesis Set\"></a>1) Perceptron Hypothesis Set</h3><p>假设空间Hypothesis Set可以用感知器（Perceptron）表示。这种假设空间的思想就类似考试给的成绩，对每一题给一个特定的分数，即加权值(权重);<br>然后再设计一个及格线，即所谓的阈值或门槛值（threshold），如果加权求和的值大于这个阈值就叫符合条件了，即输出1，小于对应的输出-1。<br>表示为公式（1）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&h(x)∈H\n\\\\\n&h(x) = sign(\\sum_{i=1}^d w_ix_i)\n\\end{align}\n\\tag{$1$}</script><p>其中 sign函数表示为取符号，即 sign(正数) = 1， sign（负数） = -1，如公式（2）所示。</p>\n<script type=\"math/tex; mode=display\">sign(x)=\\left\\{\n\\begin{align}\n& +1 \\quad x>0 \\\\\n&-1 \\quad x<0\n\\end{align}\n\\right.\n\\tag{$2$}</script><p>最后将$h(x)$ 与阈值作比较，得到公式（3）。</p>\n<script type=\"math/tex; mode=display\">\nh(x) = sign(\\sum_{i=1}^d w_ix_i - threshold)\n\\tag{$3$}</script><h3 id=\"2-Perceptron-Hypothesis-Set公式化简\"><a href=\"#2-Perceptron-Hypothesis-Set公式化简\" class=\"headerlink\" title=\"2) Perceptron Hypothesis Set公式化简\"></a>2) Perceptron Hypothesis Set公式化简</h3><p>为了表达方便，可以对$h(x)$做数学上的讲话，如公式（4）所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh(x) &= sign(\\sum_{i=1}^d w_ix_i - threshold) \\\\\n&=sign(\\sum_{i=1}^d w_ix_i - threshold - 1) \\\\\n&=sign(\\sum_{i=1}^d w_ix_i - w_0-x_0) \\\\\n&=sign(\\sum_{i=0}^d w_ix_i) \\\\\n&=sign(w^T\\cdot  x)\n\\end{align}\n\\tag{$4$}</script><blockquote>\n<p>如上所示，将负阈值表示为权值向量中的一项($w_0$)，而对应输入分量则被默认为1，用$x_0$ 最终将公式简化为两个向量内积的形式，其中T表示转置。</p>\n</blockquote>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"2-Peceptron-Learning-Algorithm-PLA\"><a href=\"#2-Peceptron-Learning-Algorithm-PLA\" class=\"headerlink\" title=\"2. Peceptron Learning Algorithm(PLA)\"></a>2. Peceptron Learning Algorithm(PLA)</h2><h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1) Introduction\"></a>1) Introduction</h3><h4 id=\"①-Question\"><a href=\"#①-Question\" class=\"headerlink\" title=\"① Question\"></a>① Question</h4><p>Hypothesis Set $H$包含所有可能的Perceptron，那么具体选用哪一组Perceptron来组合成$g(x)$ 呢?</p>\n<h4 id=\"②-Analyze\"><a href=\"#②-Analyze\" class=\"headerlink\" title=\"② Analyze\"></a>② Analyze</h4><ul>\n<li>$g(x)$ 和目标函数f越接近越好，但问题是我们不知道f（如果知道了就不需要学习了）</li>\n<li>但是我们知道的是样本输入$x$在$f(x)$ 作用下得到的标记$y$。</li>\n</ul>\n<p>所以如果我们能使得$g(x)$ 在所有的样本输入中都能够得到跟函数$f(x)$ 作用过输入得到的输出一样的话，我们认为这时的$g(x)$ 是不错的。（在后面的章节还会在这种思想的基础上更深入的讨论这一问题）</p>\n<h4 id=\"③-Solution\"><a href=\"#③-Solution\" class=\"headerlink\" title=\"③ Solution\"></a>③ Solution</h4><p>我们想到一个简单的方式，就是一步一步的修正错误的分类，在二维平面中可以想象成一条初始的直线，在经过不断的纠正它的错误（就是旋转平移之类的）使得最终的结果可以达到希望的效果。</p>\n<p>还要在重复上一节中已经得到的一个结论，在感知器模型中，每一个假设函数$g(x)$ 都对应一个权值向量。因此我们要做的就是不断修正这个权值向量使得最接近目标函数$f(x)$ 。即PLA算法</p>\n<hr>\n<p><br><br><br></p>\n<h3 id=\"2-PLA\"><a href=\"#2-PLA\" class=\"headerlink\" title=\"2) PLA\"></a>2) PLA</h3><h4 id=\"①-PLA步骤\"><a href=\"#①-PLA步骤\" class=\"headerlink\" title=\"① PLA步骤\"></a>① PLA步骤</h4><p>首先我们在设置初始$w_0$（注意此处是向量不是向量的分量！），比如设置为0向量，然后使用训练样本来将权值向量修正的更接近目标函数$f(x)$。</p>\n<p>通过公式（5）来判断什么时候需要进行修正:</p>\n<script type=\"math/tex; mode=display\">\nsign( w_t^T \\cdot x_{n(t)} \\neq y_{(n)})\n\\tag{$5$}</script><p>通过公式（6）来进行修正：</p>\n<script type=\"math/tex; mode=display\">\nw_{t+1} = w_t + y_{n(t)} \\cdot x_{n(t)}\n\\tag{$6$}</script><h4 id=\"②-图示说明\"><a href=\"#②-图示说明\" class=\"headerlink\" title=\"② 图示说明\"></a>② 图示说明</h4><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a335a9177d37b3ced5d4763a53f61f3f5b01e0e8/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-1%20PLA.png\" alt=\"PLA\"></p>\n<center>图一 PLA<sup>[1]</sup></center>\n\n<blockquote>\n<p>在本身标记为+1时，权值向量和输入向量的内积为负数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。</p>\n<p>在本身标记为-1时，权值向量和输入向量的内积为正数，对权值向量略作修改，加上一个标记y和输入向量的乘积，得到一个新的权值向量，可以看出新的权值向量和输入向量的相乘之后符合了标记的要求。</p>\n</blockquote>\n<p>重复查找错误样本和修改加权向量，直到再也找不到可以使公式(5)成立的样本为止，此时得到的加权向量，即为我们想要的最终$g(x)$</p>\n<p>具体流程如下图：</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/48df8c85a8eb4fd2af3331147cd25401dc2f728a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-2%20PLA%20procedure.jpg\" alt=\"PLA\"></p>\n<center>图二 PLA Procedure<sup>[1]</sup></center>\n\n\n\n<blockquote>\n<p>疑问:如何查找错误样本点，如何确定没有错误的点了?</p>\n</blockquote>\n<p>一个简单的方式就是将训练样本编号，从1到n，以按从1到n的顺序不断查找错误点，如果没有错就自动的用下一个样本点继续查找，当从1到n这n个样本点都没有产生错误时，算法即结束得到$g(x)$ 。将这种方式的算法叫做Cyclic PLA。</p>\n<blockquote>\n<p>新的疑问:</p>\n<p>1.这个算法一定会找到一个能使所有的样本都不符合的情况吗(即是否会停下来)？</p>\n<p>2.算法找到的真的是最好的g(x)吗？</p>\n<p>3.应用到测试集中，结果还会一样吗？</p>\n<p>详见 ③ PLA的理论保证</p>\n</blockquote>\n<h4 id=\"③-PLA的理论保证\"><a href=\"#③-PLA的理论保证\" class=\"headerlink\" title=\"③ PLA的理论保证\"></a>③ PLA的理论保证</h4><h5 id=\"i-前提条件\"><a href=\"#i-前提条件\" class=\"headerlink\" title=\"i) 前提条件\"></a>i) 前提条件</h5><p>首先很明显，要用一条直线做Binary Classification必须要求数据线性可分，否则不可以分开（想象一刀切）<br>如图三：</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/45e061db302ad3b6622daa8ecc48f802b861874c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-3%20linear%20seperable.png\" alt=\"Linear Separable\"></p>\n<center>图三 Linear Seperable <sup>[1]</sup></center>\n\n<h5 id=\"ii-理论推导\"><a href=\"#ii-理论推导\" class=\"headerlink\" title=\"ii) 理论推导\"></a>ii) 理论推导</h5><p>首先PLA的核心是每次修正都让权值的向量$W^T$（T代表在第T次停下）更接近于理想的权值向量$W_f$,如何衡量两个向量的相似程度呢？内积，即对向量$W^T$和$W_f$做内积。</p>\n<blockquote>\n<p>内积的大小与2个向量的方向与大小都有关，所以如果我们直接做内积的话，会受到向量长度的影响，所以要做归一化处理，如公式7</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}}\n\\tag{$7$}</script><p>公式用于衡量两个向量的方向的差别程度，但是怎么处理这个公式呢？</p>\n<p>先看分子,有之前的公式（6）我们可以得到公式（8）:</p>\n<script type=\"math/tex; mode=display\">\n{w_f^T \\cdot w_T} \\geq {w_f^T} \\cdot (w_{T-1} + y_{n(T-1)} \\cdot x_{n(T-1)})\n\\tag{$8$}</script><p>如果PLA都会让权值越来越接近理想值，那么由之前的公式（5），我们其实可以推出公式（9）&lt;—(因为$w_f$是最理想的值，那时候应该所有的预测值都和实际相等）和公式（10）&lt;—(因为只有再犯错误的时候才改变，所以$W_t^T$与实际值会不一样):</p>\n<script type=\"math/tex; mode=display\">\ny_{n(t)} w_f^T x_{n(t)} \\geq \\min \\limits_n\\ y_n w_f^T x_n > 0\n\\tag{$9$}</script><script type=\"math/tex; mode=display\">\ny_{n(t)} w_f^T x_{n(t)} \\leq 0\n\\tag{$10$}</script><p>所以由公式（9），我们可以对公式（8）进一步限制他的范围：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n{w_f^T \\cdot w_T} &\\geq {w_f^T} \\cdot (w_{T-1} + y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  &\\geq {w_f^T} w_{T-1} + \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  &\\geq {w_f^T} w_{T-2} + 2 - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n                  & ... \\\\\n                  &\\geq {w_f^T} w_{0} + T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot x_{n(T-1)}) \\\\\n(w_0 = 0) \\quad   &\\geq 0 + T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\cdot  x_{n(T-1)}) \\\\\n                  &\\geq T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)} \\\\\n(\\min \\limits_n\\ y_n w_f^T x_n > 0) \\quad  & > 0\n\\end{align}\n\\tag{$11$}</script><p>公式（10）中，我们关键是要得到</p>\n<script type=\"math/tex; mode=display\">\n{w_f^T \\cdot w_T} > T - \\min \\limits_n w_f^T \\cdot y_{n(T-1)}</script><p>接着，我们来研究分母:除了$w_T$不容易确定之外，$w_T$的L1范式也不容易得出,但是我们可以转换一下，结合公式（8）(10)来进行处理，即公式（12）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n{\\lvert \\lvert w_T\\rvert \\rvert} ^ 2 &= {\\lvert \\lvert w_{T-1} + y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2   \\\\\n                                     &= {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + {\\lvert \\lvert y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2 + 2y_{n(T-1)} w_{T-1} x_{n(T-1)}  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + {\\lvert \\lvert y_{n(T-1)} x_{n(T-1)} \\rvert \\rvert} ^ 2 + 0  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-1} \\rvert \\rvert} ^ 2 + \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & \\leq {\\lvert \\lvert w_{T-2} \\rvert \\rvert} ^ 2 + 2 - \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & ... \\\\\n                                     & \\leq {\\lvert \\lvert w_{0} \\rvert \\rvert} ^ 2 + T - \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2  \\\\\n                                     & = T \\cdot \\max\\limits_n {\\lvert \\lvert x_{n} \\rvert \\rvert} ^ 2\n\\end{align}\n\\tag{$12$}</script><p>到此为止，我们已经把分子的下限和分母的上限都找到了，这时候总体的值就应该大于等于某个值。有公式（11）（12），我们可以推论到公式（13）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}} & \\geq \\frac{T \\cdot \\min\\limits_n y_n w_f^T x_n}{w_f^T \\sqrt{T \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert}}}  \\\\\n         & = \\sqrt{T} \\frac{\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n} {\\sqrt{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 }} \\\\\n         & = \\sqrt(T) \\cdot C\n         \\quad \\quad \\quad(其中 C = \\frac{\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n} {\\sqrt{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 }})\n\\end{align}\n\\tag{$13$}</script><p>再有归一化之后的内积不可以大于1，所以公式（13）可以加以限制为公式（14)</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n1 \\geq\n\\frac{w_f^T \\cdot w_T}{ {\\lvert \\lvert w_f^T \\rvert \\rvert} \\cdot {\\lvert \\lvert w_T\\rvert \\rvert}} & \\geq \\frac{T \\cdot \\min\\limits_n y_n w_f^T x_n}{w_f^T \\sqrt{T \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert}}}\n\\end{align}\n\\tag{$14$}</script><p>求解公式（14）可以得到公式（15）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nT &\\leq \\frac{\\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2 } {\\left( {\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n}\\right)^2}  \\\\\n  & = \\frac{R^2}{ρ^2}\\\\\n &（其中：R^2 = \\max\\limits_n {\\lvert \\lvert w_n \\rvert \\rvert} ^ 2, \\quad\n                         ρ^2 = {\\left( {\\min\\limits_n y_n \\frac{w_f^T}{\\lvert \\lvert w_f^T \\rvert \\rvert} x_n}\\right)^2}\n\\end{align}\n\\tag{$15$}</script><blockquote>\n<p>从公式（15）可以看出，T是有上限的，所以可以说明在线性可分的情况下PLA算法最终会停止于T点，找到一个最接近目标函数的target function $g(x)$</p>\n</blockquote>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"3-Non-Separable-Data\"><a href=\"#3-Non-Separable-Data\" class=\"headerlink\" title=\"3. Non-Separable Data\"></a>3. Non-Separable Data</h2><h3 id=\"1-PLA的缺陷\"><a href=\"#1-PLA的缺陷\" class=\"headerlink\" title=\"1) PLA的缺陷\"></a>1) PLA的缺陷</h3><p>上面提到的PLA算法是基于线性可分的数据集的，这种算法不实用，有2个原因。</p>\n<ol>\n<li>在机器学习的时候，我们是不知道数据是否可分的，如果不可分，那样PLA不停止怎么办？</li>\n<li>还是与PLA不停止有关，这个时候如果数据集真的线性可分，但是因为数据量太大，跑了很久都没有跑完，怎么办？我们不知道究竟是数据问题，还是数据量太大还没跑完的问题。</li>\n</ol>\n<p>所以这时候要求我们改进PLA算法。</p>\n<h3 id=\"2）实际应用中线性可分多还是线性不可分的数据多？\"><a href=\"#2）实际应用中线性可分多还是线性不可分的数据多？\" class=\"headerlink\" title=\"2）实际应用中线性可分多还是线性不可分的数据多？\"></a>2）实际应用中线性可分多还是线性不可分的数据多？</h3><p>线性不可分的数据多：因为实际应用中存在很多噪音（Noise）。而出现噪音的原因有很多种，如：</p>\n<ul>\n<li>录入数据的时候出错</li>\n<li>采集数据的设备存在误差</li>\n<li>…</li>\n</ul>\n<p>所以存在噪音的时候，数据集会变成图四那样。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d29c4fbd29627be6829f26874e8d23c2a246be6b/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-4%20Noise.png\" alt=\"Noise\"></p>\n<p>图四 Noise</p>\n<p>因为噪音占的比例不会很大，所以最终我们还是可以找到犯错率最少的权值向量$w_g$，如公式（16）所示。</p>\n<script type=\"math/tex; mode=display\">\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{equation}\n  w_g = \\mathop{\\argmin}_{w}{\\sum_{n=1}^N sign(w^T \\cdot x_n) \\neq y_n}\n\\end{equation}\n\\tag{$16$}</script><blockquote>\n<p>但是这个公式在数学上是NP难问题，我们无法直接求解，于是我们需要找出一种近似的算法来求解这个问题：pocket 算法。</p>\n</blockquote>\n<h3 id=\"3-Pocket-算法\"><a href=\"#3-Pocket-算法\" class=\"headerlink\" title=\"3) Pocket 算法\"></a>3) Pocket 算法</h3><h4 id=\"①-Pocket算法原理\"><a href=\"#①-Pocket算法原理\" class=\"headerlink\" title=\"① Pocket算法原理\"></a>① Pocket算法原理</h4><blockquote>\n<p>Pocket 算法属于贪心算法，即如果找到原来的权值更好的权值，才去做修改，否则停止。</p>\n</blockquote>\n<p>流程如下：</p>\n<ol>\n<li>也是随机的初始化一个权值向量</li>\n<li>随机的使用n个点中的一个点去发现是否有错误（此处与cyclic PLA使用的循环方式有所不同，不是按顺序一个一个的查看是否符合条件，而是在n个点中随机的抽取，这种方式可以增加其寻找最优解的速度）</li>\n<li>和PLA一样使用公式2-5进行修正.</li>\n<li>如果有了修正，则计算出刚刚修正过的权值向量和上一个权值向量到底谁犯的错误比较少，将少的保留重复第2步到第4步的动作。</li>\n<li>假如很长时间都没有新的权值向量比当前的权值向量犯错更少，则返回该向量作为函数g。</li>\n</ol>\n<p>流程图如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/73b88a89d3618c606f70bb3a6c915acb8686d351/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter2-5%20Pocket%20Algorithm%20procedure.jpg\" alt=\"Pocket Algorithm Procedure\"></p>\n<center> 图五 Pocket Algorithm Procedure </center>\n\n\n<h4 id=\"②优缺点分析\"><a href=\"#②优缺点分析\" class=\"headerlink\" title=\"②优缺点分析\"></a>②优缺点分析</h4><p>优点：Pocket算法不需要考虑数据集是否线性可分，都可以进行</p>\n<p>缺点：每有一个错误点，都要遍历整个数据集来获得该店的错误率$e$，所以计算量会很大</p>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了Binary Classification中最简单的PLA算法</li>\n<li>然后讨论了PLA算法能在某一步之后停下来。</li>\n<li>最后讨论了PLA算法有缺陷：数据必须线性可分。所以根据这个缺点，引用了Pocket算法</li>\n</ol>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1]机器学习基石(台湾大学-林轩田)\\2\\2 - 2 - Perceptron Learning Algorithm (PLA) (19-46)</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"0.Machine learning Foundation - Table of Contents","date":"2017-10-01T03:23:19.000Z","mathjax":false,"copyright":true,"top":100,"_content":"\n# 机器学习基石(Machine learning Foundation) - Table of Contents\n\n> 这系列博客是基于台湾大学的林轩田教授的机器学习基石，自己加以整理的学习笔记。\n\n## 为什么选这个教程?\n\n> 网上大部分人都是推荐Andrew Ng教授的机器学习，但是对比之后我觉得各有优劣，Ng的课程更适合没有任何基础的学生，算法没有讲的很深，相比之下，Hsuan-Tien Lin的教程遵循着：例子引入知识点，知识点基础简介，算法深入，讲解实例的方式。这种方法更适合我。\n\n来自林轩田教授的教学PPT的文件会在图片后加以说明\n\n# Roadmap of this course\n- When can machine learning? (illustrative + technical)\n- Why can machine learn? (theoretical + illustrative)\n- How can machine learn? (technical + practical)\n- How can machine learn better (practical + theoretical)\n\n\n# 各章节目录\n[1. 机器学习基石-When can Machine Learn? - The Learning Problem](https://zhichengmle.github.io/2017/10/02/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-02-1.When%20can%20Machine%20Learn%20-%20The%20Learning%20Problem/)\n\n[2. 机器学习基石-When can Machine Learn? - Learning to Answer Yes or No](https://zhichengmle.github.io/2017/10/02/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-02-1.When%20can%20Machine%20Learn%20-%20The%20Learning%20Problem/)\n\n[3. 机器学习基石-When can Machine Learn? - Types of Learning](https://zhichengmle.github.io/2017/10/04/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-04-3.When%20can%20Machine%20Learn%20-%20Types%20of%20Learning/)\n\n[4. 机器学习基石-When can Machine Learn? - Feasible of Learning](https://zhichengmle.github.io/2017/10/06/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-06-4.When%20can%20Machine%20Learn%20-%20Feasible%20of%20Learning/)\n\n[5. 机器学习基石-Why can Machine Learn?](https://zhichengmle.github.io/2017/10/08/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-08-5.Why%20Can%20Machine%20Learn/)\n\n[6. 机器学习基石-Why can Machine Learn? - Noice and Error](https://zhichengmle.github.io/2017/10/09/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-09-6.How%20can%20Machine%20Learn%20-%20Noice%20and%20Error/)\n\n[7. 机器学习基石-How can Machine Learn? - Linear Regression](https://zhichengmle.github.io/2017/10/11/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-11-7.How%20can%20Machine%20Learn%20-%20Linear%20Regression/)\n\n[8. 机器学习基石-How can Machine Learn? - Logistic Regression](https://zhichengmle.github.io/2017/10/13/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-13-8.How%20can%20Machine%20Learn%20-%20Logistic%20Regression/)\n\n[9. 机器学习基石-How can Machine Learn? - Linear Model for Classification](https://zhichengmle.github.io/2017/10/14/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-14-9.How%20can%20Machine%20Learn%20-%20Linear%20Model%20for%20Classification/)\n\n[10. 机器学习基石-How can Machine Learn? - Nonlinear Transformation](https://zhichengmle.github.io/2017/10/15/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-15-10.How%20can%20Machine%20Learn%20-%20Nonlinear%20Transformation/)\n\n[11. 机器学习基石-How can Machine Learn Better? - Overfitting and Solution](https://zhichengmle.github.io/2017/10/16/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-16-11.How%20can%20Machine%20Learn%20Better%20-%20Overfitting%20and%20Solution/)\n\n[12. 机器学习基石-How can Machine Learn Better? - Regularization](https://zhichengmle.github.io/2017/10/17/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-17-12.How%20can%20Machine%20Learn%20Better%20-%20Regularization/)\n\n[13. 机器学习基石-How can Machine Learn Better? - Validation](https://zhichengmle.github.io/2017/10/18/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-18-13.How%20can%20Machine%20Learn%20Better%20-%20Validation/)\n\n[14. 机器学习基石-How can Machine Learn Better? - Three Learning Principles](https://zhichengmle.github.io/2017/10/21/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-21-14.How%20can%20Machine%20Learn%20Better%20-%20Three%20Learning%20Principles/)\n\n[15. 机器学习基石 - Summary - Power of Three](https://zhichengmle.github.io/2017/10/22/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-22-15.Summary%20-%20Power%20of%20Three/)\n","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-01-0.Machine Learning Foundation - Table of Contents.md","raw":"---\ntitle: 0.Machine learning Foundation - Table of Contents\ndate: 2017-10-01 11:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: false\ncopyright: true\ntop: 100\n---\n\n# 机器学习基石(Machine learning Foundation) - Table of Contents\n\n> 这系列博客是基于台湾大学的林轩田教授的机器学习基石，自己加以整理的学习笔记。\n\n## 为什么选这个教程?\n\n> 网上大部分人都是推荐Andrew Ng教授的机器学习，但是对比之后我觉得各有优劣，Ng的课程更适合没有任何基础的学生，算法没有讲的很深，相比之下，Hsuan-Tien Lin的教程遵循着：例子引入知识点，知识点基础简介，算法深入，讲解实例的方式。这种方法更适合我。\n\n来自林轩田教授的教学PPT的文件会在图片后加以说明\n\n# Roadmap of this course\n- When can machine learning? (illustrative + technical)\n- Why can machine learn? (theoretical + illustrative)\n- How can machine learn? (technical + practical)\n- How can machine learn better (practical + theoretical)\n\n\n# 各章节目录\n[1. 机器学习基石-When can Machine Learn? - The Learning Problem](https://zhichengmle.github.io/2017/10/02/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-02-1.When%20can%20Machine%20Learn%20-%20The%20Learning%20Problem/)\n\n[2. 机器学习基石-When can Machine Learn? - Learning to Answer Yes or No](https://zhichengmle.github.io/2017/10/02/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-02-1.When%20can%20Machine%20Learn%20-%20The%20Learning%20Problem/)\n\n[3. 机器学习基石-When can Machine Learn? - Types of Learning](https://zhichengmle.github.io/2017/10/04/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-04-3.When%20can%20Machine%20Learn%20-%20Types%20of%20Learning/)\n\n[4. 机器学习基石-When can Machine Learn? - Feasible of Learning](https://zhichengmle.github.io/2017/10/06/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-06-4.When%20can%20Machine%20Learn%20-%20Feasible%20of%20Learning/)\n\n[5. 机器学习基石-Why can Machine Learn?](https://zhichengmle.github.io/2017/10/08/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-08-5.Why%20Can%20Machine%20Learn/)\n\n[6. 机器学习基石-Why can Machine Learn? - Noice and Error](https://zhichengmle.github.io/2017/10/09/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-09-6.How%20can%20Machine%20Learn%20-%20Noice%20and%20Error/)\n\n[7. 机器学习基石-How can Machine Learn? - Linear Regression](https://zhichengmle.github.io/2017/10/11/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-11-7.How%20can%20Machine%20Learn%20-%20Linear%20Regression/)\n\n[8. 机器学习基石-How can Machine Learn? - Logistic Regression](https://zhichengmle.github.io/2017/10/13/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-13-8.How%20can%20Machine%20Learn%20-%20Logistic%20Regression/)\n\n[9. 机器学习基石-How can Machine Learn? - Linear Model for Classification](https://zhichengmle.github.io/2017/10/14/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-14-9.How%20can%20Machine%20Learn%20-%20Linear%20Model%20for%20Classification/)\n\n[10. 机器学习基石-How can Machine Learn? - Nonlinear Transformation](https://zhichengmle.github.io/2017/10/15/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-15-10.How%20can%20Machine%20Learn%20-%20Nonlinear%20Transformation/)\n\n[11. 机器学习基石-How can Machine Learn Better? - Overfitting and Solution](https://zhichengmle.github.io/2017/10/16/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-16-11.How%20can%20Machine%20Learn%20Better%20-%20Overfitting%20and%20Solution/)\n\n[12. 机器学习基石-How can Machine Learn Better? - Regularization](https://zhichengmle.github.io/2017/10/17/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-17-12.How%20can%20Machine%20Learn%20Better%20-%20Regularization/)\n\n[13. 机器学习基石-How can Machine Learn Better? - Validation](https://zhichengmle.github.io/2017/10/18/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-18-13.How%20can%20Machine%20Learn%20Better%20-%20Validation/)\n\n[14. 机器学习基石-How can Machine Learn Better? - Three Learning Principles](https://zhichengmle.github.io/2017/10/21/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-21-14.How%20can%20Machine%20Learn%20Better%20-%20Three%20Learning%20Principles/)\n\n[15. 机器学习基石 - Summary - Power of Three](https://zhichengmle.github.io/2017/10/22/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-22-15.Summary%20-%20Power%20of%20Three/)\n","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-01-0.Machine Learning Foundation - Table of Contents","published":1,"updated":"2018-04-14T19:42:06.501Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2du0011rwtjdk5k7l5i","content":"<h1 id=\"机器学习基石-Machine-learning-Foundation-Table-of-Contents\"><a href=\"#机器学习基石-Machine-learning-Foundation-Table-of-Contents\" class=\"headerlink\" title=\"机器学习基石(Machine learning Foundation) - Table of Contents\"></a>机器学习基石(Machine learning Foundation) - Table of Contents</h1><blockquote>\n<p>这系列博客是基于台湾大学的林轩田教授的机器学习基石，自己加以整理的学习笔记。</p>\n</blockquote>\n<h2 id=\"为什么选这个教程\"><a href=\"#为什么选这个教程\" class=\"headerlink\" title=\"为什么选这个教程?\"></a>为什么选这个教程?</h2><blockquote>\n<p>网上大部分人都是推荐Andrew Ng教授的机器学习，但是对比之后我觉得各有优劣，Ng的课程更适合没有任何基础的学生，算法没有讲的很深，相比之下，Hsuan-Tien Lin的教程遵循着：例子引入知识点，知识点基础简介，算法深入，讲解实例的方式。这种方法更适合我。</p>\n</blockquote>\n<p>来自林轩田教授的教学PPT的文件会在图片后加以说明</p>\n<h1 id=\"Roadmap-of-this-course\"><a href=\"#Roadmap-of-this-course\" class=\"headerlink\" title=\"Roadmap of this course\"></a>Roadmap of this course</h1><ul>\n<li>When can machine learning? (illustrative + technical)</li>\n<li>Why can machine learn? (theoretical + illustrative)</li>\n<li>How can machine learn? (technical + practical)</li>\n<li>How can machine learn better (practical + theoretical)</li>\n</ul>\n<h1 id=\"各章节目录\"><a href=\"#各章节目录\" class=\"headerlink\" title=\"各章节目录\"></a>各章节目录</h1><p><a href=\"https://zhichengmle.github.io/2017/10/02/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-02-1.When%20can%20Machine%20Learn%20-%20The%20Learning%20Problem/\">1. 机器学习基石-When can Machine Learn? - The Learning Problem</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/02/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-02-1.When%20can%20Machine%20Learn%20-%20The%20Learning%20Problem/\">2. 机器学习基石-When can Machine Learn? - Learning to Answer Yes or No</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/04/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-04-3.When%20can%20Machine%20Learn%20-%20Types%20of%20Learning/\">3. 机器学习基石-When can Machine Learn? - Types of Learning</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/06/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-06-4.When%20can%20Machine%20Learn%20-%20Feasible%20of%20Learning/\">4. 机器学习基石-When can Machine Learn? - Feasible of Learning</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/08/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-08-5.Why%20Can%20Machine%20Learn/\">5. 机器学习基石-Why can Machine Learn?</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/09/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-09-6.How%20can%20Machine%20Learn%20-%20Noice%20and%20Error/\">6. 机器学习基石-Why can Machine Learn? - Noice and Error</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/11/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-11-7.How%20can%20Machine%20Learn%20-%20Linear%20Regression/\">7. 机器学习基石-How can Machine Learn? - Linear Regression</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/13/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-13-8.How%20can%20Machine%20Learn%20-%20Logistic%20Regression/\">8. 机器学习基石-How can Machine Learn? - Logistic Regression</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/14/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-14-9.How%20can%20Machine%20Learn%20-%20Linear%20Model%20for%20Classification/\">9. 机器学习基石-How can Machine Learn? - Linear Model for Classification</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/15/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-15-10.How%20can%20Machine%20Learn%20-%20Nonlinear%20Transformation/\">10. 机器学习基石-How can Machine Learn? - Nonlinear Transformation</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/16/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-16-11.How%20can%20Machine%20Learn%20Better%20-%20Overfitting%20and%20Solution/\">11. 机器学习基石-How can Machine Learn Better? - Overfitting and Solution</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/17/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-17-12.How%20can%20Machine%20Learn%20Better%20-%20Regularization/\">12. 机器学习基石-How can Machine Learn Better? - Regularization</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/18/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-18-13.How%20can%20Machine%20Learn%20Better%20-%20Validation/\">13. 机器学习基石-How can Machine Learn Better? - Validation</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/21/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-21-14.How%20can%20Machine%20Learn%20Better%20-%20Three%20Learning%20Principles/\">14. 机器学习基石-How can Machine Learn Better? - Three Learning Principles</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/22/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-22-15.Summary%20-%20Power%20of%20Three/\">15. 机器学习基石 - Summary - Power of Three</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"机器学习基石-Machine-learning-Foundation-Table-of-Contents\"><a href=\"#机器学习基石-Machine-learning-Foundation-Table-of-Contents\" class=\"headerlink\" title=\"机器学习基石(Machine learning Foundation) - Table of Contents\"></a>机器学习基石(Machine learning Foundation) - Table of Contents</h1><blockquote>\n<p>这系列博客是基于台湾大学的林轩田教授的机器学习基石，自己加以整理的学习笔记。</p>\n</blockquote>\n<h2 id=\"为什么选这个教程\"><a href=\"#为什么选这个教程\" class=\"headerlink\" title=\"为什么选这个教程?\"></a>为什么选这个教程?</h2><blockquote>\n<p>网上大部分人都是推荐Andrew Ng教授的机器学习，但是对比之后我觉得各有优劣，Ng的课程更适合没有任何基础的学生，算法没有讲的很深，相比之下，Hsuan-Tien Lin的教程遵循着：例子引入知识点，知识点基础简介，算法深入，讲解实例的方式。这种方法更适合我。</p>\n</blockquote>\n<p>来自林轩田教授的教学PPT的文件会在图片后加以说明</p>\n<h1 id=\"Roadmap-of-this-course\"><a href=\"#Roadmap-of-this-course\" class=\"headerlink\" title=\"Roadmap of this course\"></a>Roadmap of this course</h1><ul>\n<li>When can machine learning? (illustrative + technical)</li>\n<li>Why can machine learn? (theoretical + illustrative)</li>\n<li>How can machine learn? (technical + practical)</li>\n<li>How can machine learn better (practical + theoretical)</li>\n</ul>\n<h1 id=\"各章节目录\"><a href=\"#各章节目录\" class=\"headerlink\" title=\"各章节目录\"></a>各章节目录</h1><p><a href=\"https://zhichengmle.github.io/2017/10/02/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-02-1.When%20can%20Machine%20Learn%20-%20The%20Learning%20Problem/\">1. 机器学习基石-When can Machine Learn? - The Learning Problem</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/02/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-02-1.When%20can%20Machine%20Learn%20-%20The%20Learning%20Problem/\">2. 机器学习基石-When can Machine Learn? - Learning to Answer Yes or No</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/04/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-04-3.When%20can%20Machine%20Learn%20-%20Types%20of%20Learning/\">3. 机器学习基石-When can Machine Learn? - Types of Learning</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/06/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-06-4.When%20can%20Machine%20Learn%20-%20Feasible%20of%20Learning/\">4. 机器学习基石-When can Machine Learn? - Feasible of Learning</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/08/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-08-5.Why%20Can%20Machine%20Learn/\">5. 机器学习基石-Why can Machine Learn?</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/09/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-09-6.How%20can%20Machine%20Learn%20-%20Noice%20and%20Error/\">6. 机器学习基石-Why can Machine Learn? - Noice and Error</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/11/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-11-7.How%20can%20Machine%20Learn%20-%20Linear%20Regression/\">7. 机器学习基石-How can Machine Learn? - Linear Regression</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/13/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-13-8.How%20can%20Machine%20Learn%20-%20Logistic%20Regression/\">8. 机器学习基石-How can Machine Learn? - Logistic Regression</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/14/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-14-9.How%20can%20Machine%20Learn%20-%20Linear%20Model%20for%20Classification/\">9. 机器学习基石-How can Machine Learn? - Linear Model for Classification</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/15/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-15-10.How%20can%20Machine%20Learn%20-%20Nonlinear%20Transformation/\">10. 机器学习基石-How can Machine Learn? - Nonlinear Transformation</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/16/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-16-11.How%20can%20Machine%20Learn%20Better%20-%20Overfitting%20and%20Solution/\">11. 机器学习基石-How can Machine Learn Better? - Overfitting and Solution</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/17/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-17-12.How%20can%20Machine%20Learn%20Better%20-%20Regularization/\">12. 机器学习基石-How can Machine Learn Better? - Regularization</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/18/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-18-13.How%20can%20Machine%20Learn%20Better%20-%20Validation/\">13. 机器学习基石-How can Machine Learn Better? - Validation</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/21/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-21-14.How%20can%20Machine%20Learn%20Better%20-%20Three%20Learning%20Principles/\">14. 机器学习基石-How can Machine Learn Better? - Three Learning Principles</a></p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/22/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-22-15.Summary%20-%20Power%20of%20Three/\">15. 机器学习基石 - Summary - Power of Three</a></p>\n"},{"title":"3.When can Machine Learn? - Types of Learning","date":"2017-10-04T04:03:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# When can Machine Learn? - Types of Learning\n\n## 1. Learning with Different Output Space\n介绍类型的输出空间：二值输出（二元分类），多值输出（多元分类），实数输出（回归），结构输出\n\n### 1) Binary Classification\n前两章中提到的银行发信用卡问题就是一个典型的二元分类问题，其输出空间只包含两个标记+1和-1，分别对应着发卡与不发卡。\n用符号可以表示为：\n$$\ng(x) ∈ \\{value_1, value_2\\}\n\\tag{$1$}\n$$\n\n常用的算法有：\n> 以后补充\n\n### 2) Multiclass Classification\n有二元分类，就不难想到多元分类的问题，该类问题输出标签不止两种，而是{1,2,…,K}。这在人们的生活中非常常见，比如给病人症状的分类，购买物品的种类等等，其主要的应用场景就是模式识别。\n用符号可以表示为：\n$$\ng(x) ∈ \\{value_1, value_2, ..., value_n\\}\n\\tag{$2$}\n$$\n\n常用的算法有：\n> 以后补充\n\n### 3) Regression\n当输出的空间为实数的时候，就属于回归问题，这种输出与二元，多元分类的区别在于，我们无法提前打好标签到输出结果中。应用场景为：病人患病几率，给客户发信用卡的几率等。统计学中对回归问题有很多处理方法，以及评估的方法。\n用符号可以表示为：\n$$\ng(x) ∈ [ a, b ]\n\\tag{$$}\n$$\n\n常用的算法有：\n> 以后补充\n\n### 4) Structured Learning （不熟悉）\n结构化的学习，就是说输出的结果可能是一串特定的结构的数据，比如说语义识别中的语意结构。\n\n常用的算法有：\n> 以后补充\n\n\n\n--------------------------------\n\n<br><br>\n\n\n## 2. Learning with Different Data Label\n不同的数据标记: 标记了输入和输出（监督学习），标记部分数据的输入和输出（半监督学习），什么都不标记（无监督学习），训练模型根据后天的反馈进行调整（增强学习）\n\n常用的算法有：\n> 以后补充\n\n### 1) Supervised Learning\n知道数据输入的同时还知道数据的标记。就相当于告诉你题目的同时还告诉你答案，让你在这种环境下学习，称之为监督学习（supervised learning）或者叫有师学习（learning with a teacher），之前讨论的一些算法都是这类问题。\n\n常用的算法有：\n> 以后补充\n\n### 2) Semi-supervised Learning\n半监督学习，它通过少量有标记的训练点和大量无标记的训练点达到学习的目的。这种类型的例子也有很多，比如图像的识别，很多情况下我们不可能把每张图片都做上标记（因为做这种标记需要耗费大量的人力物力，是一种昂贵的行为），此时，使用半监督学习是一种不错的选择。\n\n常用的算法有：\n> 以后补充\n\n### 3) Unsupervised Learning\n这是一种没有标示（就是没有输出y）的问题，就是不告诉你题目的正确答案让你自己去做题。\n\n常用的算法有：\n> 以后补充\n\n### 4) Reinforcement Learning\n前面三种学习方式是机器学习中最传统的三种方式，除此之外，通过对一个行为作出奖励或者惩罚，以此获得的输出，进而进行学习，这种学习方式称之为强化学习。\n\n常用的算法有：\n> 以后补充\n\n\n\n--------------------------------\n<br><br>\n\n## 3. Learning with Different Protocol\n通过不同的方式去提供数据到机器中：一次性给完（batch)，一点一点的输入（online），让机器主动提出问题（active）\n\n### 1) Batch\n\n批量（batch）学习就是将很多数据一次性的给算法进行学习，是最常见的方式\n\n### 2) Online\n\n在线（online）学习就是一点一点将数据传输进去，如增强学习；\n\n### 3) Active\n主动（active）学习是主动提出问题让算法解决，可以节省大量的训练和标记消耗。类似于让机器提问题，告诉我们机器有什么问题不会，从而教它\n\n--------------------------------\n<br><br>\n\n## 4. Learning with Different Input Space\n不同的输入空间:具体特征（Concrete Features），原始特征（Raw Features），抽象特征（Abstract Features）\n\n### 1) Concrete Features\n具体特征（Concrete Features），具体特征最大特点就是便于机器学习的处理，这种情况是人类或者机器通过一定的方式提取获得的，具有实用性。\n\n### 2) Raw Features\n原始特征（Raw Features），如图片的像素等等，是最为常见到的资料，但是需要经过处理，转换成具体特征，才容易使用，实用性不太大。\n\n### 3) Abstract Features\n抽象特征（Abstract Features），如一些ID之类的看似无意义的数据，这就更需要特征的转换、提取等工作（相对于原始特征而言），几乎没有实用性。\n\n\n--------------------------------\n<br><br>\n# Summary\nA collection of concept of different Learning types.\n\n<br><br>\n--------------------------------","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-04-3.When can Machine Learn - Types of Learning.md","raw":"---\ntitle: 3.When can Machine Learn? - Types of Learning\ndate: 2017-10-04 12:03:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# When can Machine Learn? - Types of Learning\n\n## 1. Learning with Different Output Space\n介绍类型的输出空间：二值输出（二元分类），多值输出（多元分类），实数输出（回归），结构输出\n\n### 1) Binary Classification\n前两章中提到的银行发信用卡问题就是一个典型的二元分类问题，其输出空间只包含两个标记+1和-1，分别对应着发卡与不发卡。\n用符号可以表示为：\n$$\ng(x) ∈ \\{value_1, value_2\\}\n\\tag{$1$}\n$$\n\n常用的算法有：\n> 以后补充\n\n### 2) Multiclass Classification\n有二元分类，就不难想到多元分类的问题，该类问题输出标签不止两种，而是{1,2,…,K}。这在人们的生活中非常常见，比如给病人症状的分类，购买物品的种类等等，其主要的应用场景就是模式识别。\n用符号可以表示为：\n$$\ng(x) ∈ \\{value_1, value_2, ..., value_n\\}\n\\tag{$2$}\n$$\n\n常用的算法有：\n> 以后补充\n\n### 3) Regression\n当输出的空间为实数的时候，就属于回归问题，这种输出与二元，多元分类的区别在于，我们无法提前打好标签到输出结果中。应用场景为：病人患病几率，给客户发信用卡的几率等。统计学中对回归问题有很多处理方法，以及评估的方法。\n用符号可以表示为：\n$$\ng(x) ∈ [ a, b ]\n\\tag{$$}\n$$\n\n常用的算法有：\n> 以后补充\n\n### 4) Structured Learning （不熟悉）\n结构化的学习，就是说输出的结果可能是一串特定的结构的数据，比如说语义识别中的语意结构。\n\n常用的算法有：\n> 以后补充\n\n\n\n--------------------------------\n\n<br><br>\n\n\n## 2. Learning with Different Data Label\n不同的数据标记: 标记了输入和输出（监督学习），标记部分数据的输入和输出（半监督学习），什么都不标记（无监督学习），训练模型根据后天的反馈进行调整（增强学习）\n\n常用的算法有：\n> 以后补充\n\n### 1) Supervised Learning\n知道数据输入的同时还知道数据的标记。就相当于告诉你题目的同时还告诉你答案，让你在这种环境下学习，称之为监督学习（supervised learning）或者叫有师学习（learning with a teacher），之前讨论的一些算法都是这类问题。\n\n常用的算法有：\n> 以后补充\n\n### 2) Semi-supervised Learning\n半监督学习，它通过少量有标记的训练点和大量无标记的训练点达到学习的目的。这种类型的例子也有很多，比如图像的识别，很多情况下我们不可能把每张图片都做上标记（因为做这种标记需要耗费大量的人力物力，是一种昂贵的行为），此时，使用半监督学习是一种不错的选择。\n\n常用的算法有：\n> 以后补充\n\n### 3) Unsupervised Learning\n这是一种没有标示（就是没有输出y）的问题，就是不告诉你题目的正确答案让你自己去做题。\n\n常用的算法有：\n> 以后补充\n\n### 4) Reinforcement Learning\n前面三种学习方式是机器学习中最传统的三种方式，除此之外，通过对一个行为作出奖励或者惩罚，以此获得的输出，进而进行学习，这种学习方式称之为强化学习。\n\n常用的算法有：\n> 以后补充\n\n\n\n--------------------------------\n<br><br>\n\n## 3. Learning with Different Protocol\n通过不同的方式去提供数据到机器中：一次性给完（batch)，一点一点的输入（online），让机器主动提出问题（active）\n\n### 1) Batch\n\n批量（batch）学习就是将很多数据一次性的给算法进行学习，是最常见的方式\n\n### 2) Online\n\n在线（online）学习就是一点一点将数据传输进去，如增强学习；\n\n### 3) Active\n主动（active）学习是主动提出问题让算法解决，可以节省大量的训练和标记消耗。类似于让机器提问题，告诉我们机器有什么问题不会，从而教它\n\n--------------------------------\n<br><br>\n\n## 4. Learning with Different Input Space\n不同的输入空间:具体特征（Concrete Features），原始特征（Raw Features），抽象特征（Abstract Features）\n\n### 1) Concrete Features\n具体特征（Concrete Features），具体特征最大特点就是便于机器学习的处理，这种情况是人类或者机器通过一定的方式提取获得的，具有实用性。\n\n### 2) Raw Features\n原始特征（Raw Features），如图片的像素等等，是最为常见到的资料，但是需要经过处理，转换成具体特征，才容易使用，实用性不太大。\n\n### 3) Abstract Features\n抽象特征（Abstract Features），如一些ID之类的看似无意义的数据，这就更需要特征的转换、提取等工作（相对于原始特征而言），几乎没有实用性。\n\n\n--------------------------------\n<br><br>\n# Summary\nA collection of concept of different Learning types.\n\n<br><br>\n--------------------------------","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-04-3.When can Machine Learn - Types of Learning","published":1,"updated":"2018-04-14T19:42:06.503Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2dw0014rwtjcycemgri","content":"<h1 id=\"When-can-Machine-Learn-Types-of-Learning\"><a href=\"#When-can-Machine-Learn-Types-of-Learning\" class=\"headerlink\" title=\"When can Machine Learn? - Types of Learning\"></a>When can Machine Learn? - Types of Learning</h1><h2 id=\"1-Learning-with-Different-Output-Space\"><a href=\"#1-Learning-with-Different-Output-Space\" class=\"headerlink\" title=\"1. Learning with Different Output Space\"></a>1. Learning with Different Output Space</h2><p>介绍类型的输出空间：二值输出（二元分类），多值输出（多元分类），实数输出（回归），结构输出</p>\n<h3 id=\"1-Binary-Classification\"><a href=\"#1-Binary-Classification\" class=\"headerlink\" title=\"1) Binary Classification\"></a>1) Binary Classification</h3><p>前两章中提到的银行发信用卡问题就是一个典型的二元分类问题，其输出空间只包含两个标记+1和-1，分别对应着发卡与不发卡。<br>用符号可以表示为：</p>\n<script type=\"math/tex; mode=display\">\ng(x) ∈ \\{value_1, value_2\\}\n\\tag{$1$}</script><p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"2-Multiclass-Classification\"><a href=\"#2-Multiclass-Classification\" class=\"headerlink\" title=\"2) Multiclass Classification\"></a>2) Multiclass Classification</h3><p>有二元分类，就不难想到多元分类的问题，该类问题输出标签不止两种，而是{1,2,…,K}。这在人们的生活中非常常见，比如给病人症状的分类，购买物品的种类等等，其主要的应用场景就是模式识别。<br>用符号可以表示为：</p>\n<script type=\"math/tex; mode=display\">\ng(x) ∈ \\{value_1, value_2, ..., value_n\\}\n\\tag{$2$}</script><p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"3-Regression\"><a href=\"#3-Regression\" class=\"headerlink\" title=\"3) Regression\"></a>3) Regression</h3><p>当输出的空间为实数的时候，就属于回归问题，这种输出与二元，多元分类的区别在于，我们无法提前打好标签到输出结果中。应用场景为：病人患病几率，给客户发信用卡的几率等。统计学中对回归问题有很多处理方法，以及评估的方法。<br>用符号可以表示为：</p>\n<script type=\"math/tex; mode=display\">\ng(x) ∈ [ a, b ]\n\\tag{$$}</script><p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"4-Structured-Learning-（不熟悉）\"><a href=\"#4-Structured-Learning-（不熟悉）\" class=\"headerlink\" title=\"4) Structured Learning （不熟悉）\"></a>4) Structured Learning （不熟悉）</h3><p>结构化的学习，就是说输出的结果可能是一串特定的结构的数据，比如说语义识别中的语意结构。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<hr>\n<p><br><br></p>\n<h2 id=\"2-Learning-with-Different-Data-Label\"><a href=\"#2-Learning-with-Different-Data-Label\" class=\"headerlink\" title=\"2. Learning with Different Data Label\"></a>2. Learning with Different Data Label</h2><p>不同的数据标记: 标记了输入和输出（监督学习），标记部分数据的输入和输出（半监督学习），什么都不标记（无监督学习），训练模型根据后天的反馈进行调整（增强学习）</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"1-Supervised-Learning\"><a href=\"#1-Supervised-Learning\" class=\"headerlink\" title=\"1) Supervised Learning\"></a>1) Supervised Learning</h3><p>知道数据输入的同时还知道数据的标记。就相当于告诉你题目的同时还告诉你答案，让你在这种环境下学习，称之为监督学习（supervised learning）或者叫有师学习（learning with a teacher），之前讨论的一些算法都是这类问题。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"2-Semi-supervised-Learning\"><a href=\"#2-Semi-supervised-Learning\" class=\"headerlink\" title=\"2) Semi-supervised Learning\"></a>2) Semi-supervised Learning</h3><p>半监督学习，它通过少量有标记的训练点和大量无标记的训练点达到学习的目的。这种类型的例子也有很多，比如图像的识别，很多情况下我们不可能把每张图片都做上标记（因为做这种标记需要耗费大量的人力物力，是一种昂贵的行为），此时，使用半监督学习是一种不错的选择。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"3-Unsupervised-Learning\"><a href=\"#3-Unsupervised-Learning\" class=\"headerlink\" title=\"3) Unsupervised Learning\"></a>3) Unsupervised Learning</h3><p>这是一种没有标示（就是没有输出y）的问题，就是不告诉你题目的正确答案让你自己去做题。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"4-Reinforcement-Learning\"><a href=\"#4-Reinforcement-Learning\" class=\"headerlink\" title=\"4) Reinforcement Learning\"></a>4) Reinforcement Learning</h3><p>前面三种学习方式是机器学习中最传统的三种方式，除此之外，通过对一个行为作出奖励或者惩罚，以此获得的输出，进而进行学习，这种学习方式称之为强化学习。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<hr>\n<p><br><br></p>\n<h2 id=\"3-Learning-with-Different-Protocol\"><a href=\"#3-Learning-with-Different-Protocol\" class=\"headerlink\" title=\"3. Learning with Different Protocol\"></a>3. Learning with Different Protocol</h2><p>通过不同的方式去提供数据到机器中：一次性给完（batch)，一点一点的输入（online），让机器主动提出问题（active）</p>\n<h3 id=\"1-Batch\"><a href=\"#1-Batch\" class=\"headerlink\" title=\"1) Batch\"></a>1) Batch</h3><p>批量（batch）学习就是将很多数据一次性的给算法进行学习，是最常见的方式</p>\n<h3 id=\"2-Online\"><a href=\"#2-Online\" class=\"headerlink\" title=\"2) Online\"></a>2) Online</h3><p>在线（online）学习就是一点一点将数据传输进去，如增强学习；</p>\n<h3 id=\"3-Active\"><a href=\"#3-Active\" class=\"headerlink\" title=\"3) Active\"></a>3) Active</h3><p>主动（active）学习是主动提出问题让算法解决，可以节省大量的训练和标记消耗。类似于让机器提问题，告诉我们机器有什么问题不会，从而教它</p>\n<hr>\n<p><br><br></p>\n<h2 id=\"4-Learning-with-Different-Input-Space\"><a href=\"#4-Learning-with-Different-Input-Space\" class=\"headerlink\" title=\"4. Learning with Different Input Space\"></a>4. Learning with Different Input Space</h2><p>不同的输入空间:具体特征（Concrete Features），原始特征（Raw Features），抽象特征（Abstract Features）</p>\n<h3 id=\"1-Concrete-Features\"><a href=\"#1-Concrete-Features\" class=\"headerlink\" title=\"1) Concrete Features\"></a>1) Concrete Features</h3><p>具体特征（Concrete Features），具体特征最大特点就是便于机器学习的处理，这种情况是人类或者机器通过一定的方式提取获得的，具有实用性。</p>\n<h3 id=\"2-Raw-Features\"><a href=\"#2-Raw-Features\" class=\"headerlink\" title=\"2) Raw Features\"></a>2) Raw Features</h3><p>原始特征（Raw Features），如图片的像素等等，是最为常见到的资料，但是需要经过处理，转换成具体特征，才容易使用，实用性不太大。</p>\n<h3 id=\"3-Abstract-Features\"><a href=\"#3-Abstract-Features\" class=\"headerlink\" title=\"3) Abstract Features\"></a>3) Abstract Features</h3><p>抽象特征（Abstract Features），如一些ID之类的看似无意义的数据，这就更需要特征的转换、提取等工作（相对于原始特征而言），几乎没有实用性。</p>\n<hr>\n<p><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><p>A collection of concept of different Learning types.</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"When-can-Machine-Learn-Types-of-Learning\"><a href=\"#When-can-Machine-Learn-Types-of-Learning\" class=\"headerlink\" title=\"When can Machine Learn? - Types of Learning\"></a>When can Machine Learn? - Types of Learning</h1><h2 id=\"1-Learning-with-Different-Output-Space\"><a href=\"#1-Learning-with-Different-Output-Space\" class=\"headerlink\" title=\"1. Learning with Different Output Space\"></a>1. Learning with Different Output Space</h2><p>介绍类型的输出空间：二值输出（二元分类），多值输出（多元分类），实数输出（回归），结构输出</p>\n<h3 id=\"1-Binary-Classification\"><a href=\"#1-Binary-Classification\" class=\"headerlink\" title=\"1) Binary Classification\"></a>1) Binary Classification</h3><p>前两章中提到的银行发信用卡问题就是一个典型的二元分类问题，其输出空间只包含两个标记+1和-1，分别对应着发卡与不发卡。<br>用符号可以表示为：</p>\n<script type=\"math/tex; mode=display\">\ng(x) ∈ \\{value_1, value_2\\}\n\\tag{$1$}</script><p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"2-Multiclass-Classification\"><a href=\"#2-Multiclass-Classification\" class=\"headerlink\" title=\"2) Multiclass Classification\"></a>2) Multiclass Classification</h3><p>有二元分类，就不难想到多元分类的问题，该类问题输出标签不止两种，而是{1,2,…,K}。这在人们的生活中非常常见，比如给病人症状的分类，购买物品的种类等等，其主要的应用场景就是模式识别。<br>用符号可以表示为：</p>\n<script type=\"math/tex; mode=display\">\ng(x) ∈ \\{value_1, value_2, ..., value_n\\}\n\\tag{$2$}</script><p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"3-Regression\"><a href=\"#3-Regression\" class=\"headerlink\" title=\"3) Regression\"></a>3) Regression</h3><p>当输出的空间为实数的时候，就属于回归问题，这种输出与二元，多元分类的区别在于，我们无法提前打好标签到输出结果中。应用场景为：病人患病几率，给客户发信用卡的几率等。统计学中对回归问题有很多处理方法，以及评估的方法。<br>用符号可以表示为：</p>\n<script type=\"math/tex; mode=display\">\ng(x) ∈ [ a, b ]\n\\tag{$$}</script><p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"4-Structured-Learning-（不熟悉）\"><a href=\"#4-Structured-Learning-（不熟悉）\" class=\"headerlink\" title=\"4) Structured Learning （不熟悉）\"></a>4) Structured Learning （不熟悉）</h3><p>结构化的学习，就是说输出的结果可能是一串特定的结构的数据，比如说语义识别中的语意结构。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<hr>\n<p><br><br></p>\n<h2 id=\"2-Learning-with-Different-Data-Label\"><a href=\"#2-Learning-with-Different-Data-Label\" class=\"headerlink\" title=\"2. Learning with Different Data Label\"></a>2. Learning with Different Data Label</h2><p>不同的数据标记: 标记了输入和输出（监督学习），标记部分数据的输入和输出（半监督学习），什么都不标记（无监督学习），训练模型根据后天的反馈进行调整（增强学习）</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"1-Supervised-Learning\"><a href=\"#1-Supervised-Learning\" class=\"headerlink\" title=\"1) Supervised Learning\"></a>1) Supervised Learning</h3><p>知道数据输入的同时还知道数据的标记。就相当于告诉你题目的同时还告诉你答案，让你在这种环境下学习，称之为监督学习（supervised learning）或者叫有师学习（learning with a teacher），之前讨论的一些算法都是这类问题。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"2-Semi-supervised-Learning\"><a href=\"#2-Semi-supervised-Learning\" class=\"headerlink\" title=\"2) Semi-supervised Learning\"></a>2) Semi-supervised Learning</h3><p>半监督学习，它通过少量有标记的训练点和大量无标记的训练点达到学习的目的。这种类型的例子也有很多，比如图像的识别，很多情况下我们不可能把每张图片都做上标记（因为做这种标记需要耗费大量的人力物力，是一种昂贵的行为），此时，使用半监督学习是一种不错的选择。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"3-Unsupervised-Learning\"><a href=\"#3-Unsupervised-Learning\" class=\"headerlink\" title=\"3) Unsupervised Learning\"></a>3) Unsupervised Learning</h3><p>这是一种没有标示（就是没有输出y）的问题，就是不告诉你题目的正确答案让你自己去做题。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<h3 id=\"4-Reinforcement-Learning\"><a href=\"#4-Reinforcement-Learning\" class=\"headerlink\" title=\"4) Reinforcement Learning\"></a>4) Reinforcement Learning</h3><p>前面三种学习方式是机器学习中最传统的三种方式，除此之外，通过对一个行为作出奖励或者惩罚，以此获得的输出，进而进行学习，这种学习方式称之为强化学习。</p>\n<p>常用的算法有：</p>\n<blockquote>\n<p>以后补充</p>\n</blockquote>\n<hr>\n<p><br><br></p>\n<h2 id=\"3-Learning-with-Different-Protocol\"><a href=\"#3-Learning-with-Different-Protocol\" class=\"headerlink\" title=\"3. Learning with Different Protocol\"></a>3. Learning with Different Protocol</h2><p>通过不同的方式去提供数据到机器中：一次性给完（batch)，一点一点的输入（online），让机器主动提出问题（active）</p>\n<h3 id=\"1-Batch\"><a href=\"#1-Batch\" class=\"headerlink\" title=\"1) Batch\"></a>1) Batch</h3><p>批量（batch）学习就是将很多数据一次性的给算法进行学习，是最常见的方式</p>\n<h3 id=\"2-Online\"><a href=\"#2-Online\" class=\"headerlink\" title=\"2) Online\"></a>2) Online</h3><p>在线（online）学习就是一点一点将数据传输进去，如增强学习；</p>\n<h3 id=\"3-Active\"><a href=\"#3-Active\" class=\"headerlink\" title=\"3) Active\"></a>3) Active</h3><p>主动（active）学习是主动提出问题让算法解决，可以节省大量的训练和标记消耗。类似于让机器提问题，告诉我们机器有什么问题不会，从而教它</p>\n<hr>\n<p><br><br></p>\n<h2 id=\"4-Learning-with-Different-Input-Space\"><a href=\"#4-Learning-with-Different-Input-Space\" class=\"headerlink\" title=\"4. Learning with Different Input Space\"></a>4. Learning with Different Input Space</h2><p>不同的输入空间:具体特征（Concrete Features），原始特征（Raw Features），抽象特征（Abstract Features）</p>\n<h3 id=\"1-Concrete-Features\"><a href=\"#1-Concrete-Features\" class=\"headerlink\" title=\"1) Concrete Features\"></a>1) Concrete Features</h3><p>具体特征（Concrete Features），具体特征最大特点就是便于机器学习的处理，这种情况是人类或者机器通过一定的方式提取获得的，具有实用性。</p>\n<h3 id=\"2-Raw-Features\"><a href=\"#2-Raw-Features\" class=\"headerlink\" title=\"2) Raw Features\"></a>2) Raw Features</h3><p>原始特征（Raw Features），如图片的像素等等，是最为常见到的资料，但是需要经过处理，转换成具体特征，才容易使用，实用性不太大。</p>\n<h3 id=\"3-Abstract-Features\"><a href=\"#3-Abstract-Features\" class=\"headerlink\" title=\"3) Abstract Features\"></a>3) Abstract Features</h3><p>抽象特征（Abstract Features），如一些ID之类的看似无意义的数据，这就更需要特征的转换、提取等工作（相对于原始特征而言），几乎没有实用性。</p>\n<hr>\n<p><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><p>A collection of concept of different Learning types.</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"4.When can Machine Learn? - Feasible of Learning","date":"2017-10-06T08:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n\n# When can Machine Learn? - Feasible of Learning\n\n> 这章主要讨论 Whether machine learning is possible or not.\n\n\n## 1. Learning is Impossible?\n在讨论之前，先看下面的一个问题\n\n![Learning Puzzle](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3fb303bc1e0eceffd4c6a3dc62dbb9b8bf078c43/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-1%20Learning%20Puzzle.png)\n\n<center>图一 Learning Puzzle <sup>[1]</sup></center>\n\n<br>\n这类似于一道智商题,却没有标准答案，根据你不同的一个视角，可以找到不同的规则。比如：\n\n1. 左上角的正方形是否涂黑\n2. 是否对称\n3. 正中间的正方形是否涂黑\n4. ...\n\n所以无论机器学到的是什么模型，其他人都能说机器说错了。也就是说机器不能真的学习了。\n\n----------------------------------------\n<br><br>\n\n## 2. Probability of the Rescue\n\n### 1) Hoeffding Inequity\n\n上面提到了机器不能学习，因为机器求出来的假设函数$g(x)$很难与目标函数$f(x)$一样：因为数据不一样。 但是根据Hoeffding不等式（公式1），可以证明，在数据量足够大的情况，可以保证$g(x)$很接近于$$f(x)$.\n\n$$\n\\rho \\left[ \\lvert \\nu - \\mu \\rvert  > \\epsilon\\right ] \\leq 2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$1$}\n$$\n\n>其中 $\\rho$为概率符号， $\\lvert \\nu - \\mu \\rvert$表示2个值的近似程度， \n\n$\\epsilon$是这个近似程度的下界，$N$为样本数量的大小，所以不等式的意思是两个值的差别大于 $\\epsilon$的概率小于等于 $2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)$。所以，如果当 $\\epsilon$一定的情况下， 随着样本数量 $N$越大，那么这个差距的可能性越小（参考 $e^{-n}$ 的曲线），当 $\\epsilon$很小且 $N$大到一定的程度的时候，$\\mu$和 $\\nu$差别很小的概率很低，即 $\\mu$和 $\\nu$相等是一个大概近似正确（Probably Approximately Correct, PAC）的情况。\n\n\n下面举例说明公式的意义（以概率统计中的从罐子中有放回的取球为例），如图二。\n\n![Sampling](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/36d1fe3ba6ea839628bebdfebf2dc55e1ca202d4/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-2%20Sampling.png)\n\n<center> 图二 Sampling<sup>[2]</sup></center>\n\n\n罐子中只有橙色和绿色的球，其中橙色球的概率为 $\\mu$,那么绿色球的概率为 $1-\\mu$， 如果通过抽样，得到橙色球的概率为 $\\nu$，那么绿色球的概率为$1-\\nu$（其中，$\\mu$是假设的，是未知的，$\\nu$而是通过抽样得到的，已知的)。因为抽样的罐子是均匀是，所以抽样得到的橙色球的概率 $\\nu$要近似于实际罐子中橙色球的概率 $\\mu$。这个近似值得范围就是Hoeffding Inequity所表示的。\n\n\n---------------------\n\n### 2) Connection Between Hoeffding Inequity and Learning\n\n下面引用PPT里面的对比图来进行解释。\n\n![Connection to Learning](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a1422a508266710d0f3a0b90c5227c7400307921/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-3%20Connection%20To%20Learning.png)\n\n<center> 图三 Connetion to Learning <sup>[3]</sup></center>\n\n\n上面的抽样调查中，我们关键有： 罐子中橙色球的实际概率 $\\mu$, 抽样出来的球 ∈ 罐子，抽样的橙色球概率，抽样得到的绿色球概率\n对应到实际学习中就是： 实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$, 训练样本$x$ ∈ 整个数据集 $X$，训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$, 训练过程中满足 $h(x) = g(x)$ 的概率 $1-E_{in}$,\n\n并且可以得到以下公式（2）和公式（3）\n$$\n\\nu = \\frac{1}{N} \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]   \\quad (x_i ∈ X)\n\\tag{$2$}\n$$\n\n$$\n\\mu = \\epsilon \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]\n\\tag{$3$}\n$$\n\n> $\\epsilon$表示为期望值，即实际样本中的错误率 $E_{out}$\n\n为了查看方便，列个表格进行说明\n\n罐子|机器学习\n-|-|\n罐子中橙色球的实际概率 $\\mu$    | 实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$\n抽样出来的球 ∈ 罐子            | 训练样本$x$ ∈ 整个数据集 $X$\n抽样的橙色球概率               |训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$\n抽样得到的绿色球概率           |训练过程中满足 $h(x) = g(x)$ 的概率 $1-E_{in}$,\n\n\n因此结合Hoeffding的理论支持后，我们可以扩展机器学习的流程图，如图四所示。\n\n![New Learning Diagram](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b89d9143e63c41c2d334d8471c8fa7730702458e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-4%20Learning%20Diagram.png)\n\n<center> 图四 New Learning Diagram <sup>[3]</sup></center>\n\n\n> 其中虚线表示未知概率 $\\rho$ 对随机抽样以及实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$  的影响，实线表示训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$。\n\n上面提到的$E_{in}$ $E_{out}$ 可以分别用下面的公式（4）（5）表示\n$$\nE_{in} = \\nu = \\frac{1}{N} \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]   \\quad (x_i ∈ X)\n\\tag{$4$}\n$$\n$$\nE_{out} = \\mu = \\epsilon \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]\n\\tag{$5$}\n$$\n\n所以Hoedding的不等式可以变成公式（6）\n接近于$f(x)$.\n$$\n\\rho \\left[ \\lvert E_{in} - E_{out} \\rvert  > \\epsilon\\right] \\leq 2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$6$}\n$$\n同样的，当 $\\epsilon$一定（一般都很小），并且训练样本 $N$足够大的情况下，我们有 $E_{in} \\approx E_{out}$，也就是说 $g(x) \\approx f(x)$，这时候的流程图就变成了一个 $h$对应一个 $f$的情况了，如图五所示。\n\n![A Verification Flow](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9007d438d4541ff836281cc7d1648bbcdbad927c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-5%20Learning%20Diagram%20-%20the%20verification%20flow.png)\n\n<center> 图五 A Verification Flow <sup>[3]</sup></center>\n\n<br>\n但这个并非是真正意义上的学习，因为只有一个Hypothesis(因为通过一个一段数据集，我们只能得到一个Hypothesis)，所以我们下面讨论多个。\n\n### 3) Connection Between Hoeffding Inequity and Real Learning\n上面我们讨论了根据Hoeffding Inequity，在 $N$无限大的时候，一个Hypothesis的 $\\lvert E_{in} - E_{out} \\rvert < \\epsilon$的概率会无限的小，所以同时满足 $\\epsilon$ 很小 且 $N$很大的情况下，我们可以得到 $E_{in} \\approx E_{out}$。\n\n> 疑问：但是在应对多个Hypothesis的时候，我们就有多个 $h(x)$ 和多个 $E_{in}$，那样我们真的能确定 $E_{in} \\approx E_{out}$ 吗？\n\n#### ① Introduction of Bad Data\n在连续抛5次硬币过程中，一个人抛到正面朝上的概率为 $\\frac{1}{32}$，如果现在有150个人，那么这150个人里面，至少有一个人5次都抛出正面朝上的几率为 $1- (\\frac{31}{32})^{150} > 99\\%$。\n> 分析： 因为一个人连续5次抛出正面朝上几率为 $\\frac{1}{32} < 3\\%$， 所以如果机器学习的话，在测试的时候他更偏向于预测“不能连续抛出5次正面朝上”（因为选择会偏向于选择概率更大的一边）。但是当150次试验的时候，一个人连续5次抛出正面朝上的几率 $E_{out}>99\\%$，而在训练的时候的概率$E_{in} < 3\\%$，也就是说机器学习了之后进行了错误的估计（即$E_{in} 远远\\neq E_{out}$。导致这种结果的几率就是所谓的 bad data（也就是Noise:偏差值，数据与所需数据不吻合等）。\n\n> 疑问：\n>\n> 1.如果说存在bad data，那样的话怎么区分呢？（下面会证明bad data出现的概率）。\n>\n> 2.如果万一 $E_{in}$ 很大的话，那么 $E_{out}$ 也很大，那么这机器学习还什么意义呢？(这个问题后面解释)\n\n#### ② Probability of Bad Data\n\n首先在单个假设的时候如图六所示\n\n![Bad Data for One h](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f7bb05cf9a34185056b07452b108bcd7b2f13f8d/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-6%20bad%20data%20for%20one%20H.png)\n\n<center> 图六 Bad Data for One h<sup>[4]</sup></center>\n\n而在多个假设的时候如图七所示\n\n![Bad Data for Many h](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f7bb05cf9a34185056b07452b108bcd7b2f13f8d/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-7%20bad%20data%20for%20many%20H.png)\n\n<center> 图七 Bad Data for Many h<sup>[4]</sup></center>\n\n最后计算 Bad Data发生的概率如图8所示。\n\n![Bound of Bad Data](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a02388463f51e20944466bd7718bf2921b68e970/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-8%20bound%20of%20bad%20data.png)\n\n<center> 图八 Bound of Bad Data<sup>[4]</sup></center>\n\n\n> 由计算结果可以发现，这里的M是一个有限的数，所以当训练样本 $N$ 越大，那么Bad Data出现的概率越低，$E_{in} \\approx E_{out}$；如果训练样本 $N$一定的情况下，M越大，也就是说Hypothesis越多，那样可以供我们用算法 $A$进行选择的越多，那么越有可能选到一个好的样本，使得 $E_{in} \\approx 0$\n\n总结如下表：\n\n-                              | M很小的时候 | M很大的时候 | N很小的时候 | N很大的时候 |\n-                              |:----------:|:----------:|:----------:|:----------:|\n$E_{in}(g) \\approx E_{out}(g)$ |Yes，Bad Data的数量也少了| No，Bad Data的数量也多了         |Yes，Bad Data出现的概率变小了 | No，Bad Data出现的概率变大了\n$E_{in}(g) \\approx 0$          |No，选到低错误率的可能性变小了|Yes，选到低错误率的可能性变大了|没必然联系，样本总数多于少，与错误率无关|没必然联系，样本总数多于少，与错误率无关\n\n最终我们的Learning Flow 就可以变成图9。\n\n![The Statistic Learning Flow](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a02388463f51e20944466bd7718bf2921b68e970/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-9%20statistic%20learning%20flow.png)\n\n<center> 图九 The Statistic Learning Flow<sup>[4]</sup></center>\n\n> 在足够样本的情况下，机器算是能学到东西了!\n\n----------------------------------------\n<br><br>\n\n\n# Summary\n这章主要讨论 Whether machine learning is possible or not.\n思路：\n1. 首先直线学习是不可能的：因为输入样本与实际测试样本差别可能很大\n2. 1中讨论的情况是事情，但是有根据统计学中的Hoeffding不等式，可以有补救的办法（某种程度上，可以使得学习后的机器在实际测试中有较小的错误），然后根据Hoeffding不等式，来联系机器学习，说明机器学习是某种程度上可行的\n\n\n----------------------------------------\n<br><br>\n\n\n# Reference\n1.机器学习基石(台湾大学-林轩田)\\4\\4 - 1 - Learning is Impossible- (13-32)\n\n2.机器学习基石(台湾大学-林轩田)\\4\\4 - 2 - Probability to the Rescue (11-33)\n\n3.机器学习基石(台湾大学-林轩田)\\4\\4 - 3 - Connection to Learning (16-46)\n\n4.机器学习基石(台湾大学-林轩田)\\4\\4 - 4 - Connection to Real Learning (18-06)\n\n<br><br>\n----------------------------------------","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-06-4.When can Machine Learn - Feasible of Learning.md","raw":"---\ntitle: 4.When can Machine Learn? - Feasible of Learning\ndate: 2017-10-06 16:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n\n# When can Machine Learn? - Feasible of Learning\n\n> 这章主要讨论 Whether machine learning is possible or not.\n\n\n## 1. Learning is Impossible?\n在讨论之前，先看下面的一个问题\n\n![Learning Puzzle](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3fb303bc1e0eceffd4c6a3dc62dbb9b8bf078c43/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-1%20Learning%20Puzzle.png)\n\n<center>图一 Learning Puzzle <sup>[1]</sup></center>\n\n<br>\n这类似于一道智商题,却没有标准答案，根据你不同的一个视角，可以找到不同的规则。比如：\n\n1. 左上角的正方形是否涂黑\n2. 是否对称\n3. 正中间的正方形是否涂黑\n4. ...\n\n所以无论机器学到的是什么模型，其他人都能说机器说错了。也就是说机器不能真的学习了。\n\n----------------------------------------\n<br><br>\n\n## 2. Probability of the Rescue\n\n### 1) Hoeffding Inequity\n\n上面提到了机器不能学习，因为机器求出来的假设函数$g(x)$很难与目标函数$f(x)$一样：因为数据不一样。 但是根据Hoeffding不等式（公式1），可以证明，在数据量足够大的情况，可以保证$g(x)$很接近于$$f(x)$.\n\n$$\n\\rho \\left[ \\lvert \\nu - \\mu \\rvert  > \\epsilon\\right ] \\leq 2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$1$}\n$$\n\n>其中 $\\rho$为概率符号， $\\lvert \\nu - \\mu \\rvert$表示2个值的近似程度， \n\n$\\epsilon$是这个近似程度的下界，$N$为样本数量的大小，所以不等式的意思是两个值的差别大于 $\\epsilon$的概率小于等于 $2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)$。所以，如果当 $\\epsilon$一定的情况下， 随着样本数量 $N$越大，那么这个差距的可能性越小（参考 $e^{-n}$ 的曲线），当 $\\epsilon$很小且 $N$大到一定的程度的时候，$\\mu$和 $\\nu$差别很小的概率很低，即 $\\mu$和 $\\nu$相等是一个大概近似正确（Probably Approximately Correct, PAC）的情况。\n\n\n下面举例说明公式的意义（以概率统计中的从罐子中有放回的取球为例），如图二。\n\n![Sampling](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/36d1fe3ba6ea839628bebdfebf2dc55e1ca202d4/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-2%20Sampling.png)\n\n<center> 图二 Sampling<sup>[2]</sup></center>\n\n\n罐子中只有橙色和绿色的球，其中橙色球的概率为 $\\mu$,那么绿色球的概率为 $1-\\mu$， 如果通过抽样，得到橙色球的概率为 $\\nu$，那么绿色球的概率为$1-\\nu$（其中，$\\mu$是假设的，是未知的，$\\nu$而是通过抽样得到的，已知的)。因为抽样的罐子是均匀是，所以抽样得到的橙色球的概率 $\\nu$要近似于实际罐子中橙色球的概率 $\\mu$。这个近似值得范围就是Hoeffding Inequity所表示的。\n\n\n---------------------\n\n### 2) Connection Between Hoeffding Inequity and Learning\n\n下面引用PPT里面的对比图来进行解释。\n\n![Connection to Learning](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a1422a508266710d0f3a0b90c5227c7400307921/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-3%20Connection%20To%20Learning.png)\n\n<center> 图三 Connetion to Learning <sup>[3]</sup></center>\n\n\n上面的抽样调查中，我们关键有： 罐子中橙色球的实际概率 $\\mu$, 抽样出来的球 ∈ 罐子，抽样的橙色球概率，抽样得到的绿色球概率\n对应到实际学习中就是： 实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$, 训练样本$x$ ∈ 整个数据集 $X$，训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$, 训练过程中满足 $h(x) = g(x)$ 的概率 $1-E_{in}$,\n\n并且可以得到以下公式（2）和公式（3）\n$$\n\\nu = \\frac{1}{N} \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]   \\quad (x_i ∈ X)\n\\tag{$2$}\n$$\n\n$$\n\\mu = \\epsilon \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]\n\\tag{$3$}\n$$\n\n> $\\epsilon$表示为期望值，即实际样本中的错误率 $E_{out}$\n\n为了查看方便，列个表格进行说明\n\n罐子|机器学习\n-|-|\n罐子中橙色球的实际概率 $\\mu$    | 实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$\n抽样出来的球 ∈ 罐子            | 训练样本$x$ ∈ 整个数据集 $X$\n抽样的橙色球概率               |训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$\n抽样得到的绿色球概率           |训练过程中满足 $h(x) = g(x)$ 的概率 $1-E_{in}$,\n\n\n因此结合Hoeffding的理论支持后，我们可以扩展机器学习的流程图，如图四所示。\n\n![New Learning Diagram](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b89d9143e63c41c2d334d8471c8fa7730702458e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-4%20Learning%20Diagram.png)\n\n<center> 图四 New Learning Diagram <sup>[3]</sup></center>\n\n\n> 其中虚线表示未知概率 $\\rho$ 对随机抽样以及实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$  的影响，实线表示训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$。\n\n上面提到的$E_{in}$ $E_{out}$ 可以分别用下面的公式（4）（5）表示\n$$\nE_{in} = \\nu = \\frac{1}{N} \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]   \\quad (x_i ∈ X)\n\\tag{$4$}\n$$\n$$\nE_{out} = \\mu = \\epsilon \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]\n\\tag{$5$}\n$$\n\n所以Hoedding的不等式可以变成公式（6）\n接近于$f(x)$.\n$$\n\\rho \\left[ \\lvert E_{in} - E_{out} \\rvert  > \\epsilon\\right] \\leq 2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$6$}\n$$\n同样的，当 $\\epsilon$一定（一般都很小），并且训练样本 $N$足够大的情况下，我们有 $E_{in} \\approx E_{out}$，也就是说 $g(x) \\approx f(x)$，这时候的流程图就变成了一个 $h$对应一个 $f$的情况了，如图五所示。\n\n![A Verification Flow](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9007d438d4541ff836281cc7d1648bbcdbad927c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-5%20Learning%20Diagram%20-%20the%20verification%20flow.png)\n\n<center> 图五 A Verification Flow <sup>[3]</sup></center>\n\n<br>\n但这个并非是真正意义上的学习，因为只有一个Hypothesis(因为通过一个一段数据集，我们只能得到一个Hypothesis)，所以我们下面讨论多个。\n\n### 3) Connection Between Hoeffding Inequity and Real Learning\n上面我们讨论了根据Hoeffding Inequity，在 $N$无限大的时候，一个Hypothesis的 $\\lvert E_{in} - E_{out} \\rvert < \\epsilon$的概率会无限的小，所以同时满足 $\\epsilon$ 很小 且 $N$很大的情况下，我们可以得到 $E_{in} \\approx E_{out}$。\n\n> 疑问：但是在应对多个Hypothesis的时候，我们就有多个 $h(x)$ 和多个 $E_{in}$，那样我们真的能确定 $E_{in} \\approx E_{out}$ 吗？\n\n#### ① Introduction of Bad Data\n在连续抛5次硬币过程中，一个人抛到正面朝上的概率为 $\\frac{1}{32}$，如果现在有150个人，那么这150个人里面，至少有一个人5次都抛出正面朝上的几率为 $1- (\\frac{31}{32})^{150} > 99\\%$。\n> 分析： 因为一个人连续5次抛出正面朝上几率为 $\\frac{1}{32} < 3\\%$， 所以如果机器学习的话，在测试的时候他更偏向于预测“不能连续抛出5次正面朝上”（因为选择会偏向于选择概率更大的一边）。但是当150次试验的时候，一个人连续5次抛出正面朝上的几率 $E_{out}>99\\%$，而在训练的时候的概率$E_{in} < 3\\%$，也就是说机器学习了之后进行了错误的估计（即$E_{in} 远远\\neq E_{out}$。导致这种结果的几率就是所谓的 bad data（也就是Noise:偏差值，数据与所需数据不吻合等）。\n\n> 疑问：\n>\n> 1.如果说存在bad data，那样的话怎么区分呢？（下面会证明bad data出现的概率）。\n>\n> 2.如果万一 $E_{in}$ 很大的话，那么 $E_{out}$ 也很大，那么这机器学习还什么意义呢？(这个问题后面解释)\n\n#### ② Probability of Bad Data\n\n首先在单个假设的时候如图六所示\n\n![Bad Data for One h](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f7bb05cf9a34185056b07452b108bcd7b2f13f8d/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-6%20bad%20data%20for%20one%20H.png)\n\n<center> 图六 Bad Data for One h<sup>[4]</sup></center>\n\n而在多个假设的时候如图七所示\n\n![Bad Data for Many h](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f7bb05cf9a34185056b07452b108bcd7b2f13f8d/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-7%20bad%20data%20for%20many%20H.png)\n\n<center> 图七 Bad Data for Many h<sup>[4]</sup></center>\n\n最后计算 Bad Data发生的概率如图8所示。\n\n![Bound of Bad Data](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a02388463f51e20944466bd7718bf2921b68e970/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-8%20bound%20of%20bad%20data.png)\n\n<center> 图八 Bound of Bad Data<sup>[4]</sup></center>\n\n\n> 由计算结果可以发现，这里的M是一个有限的数，所以当训练样本 $N$ 越大，那么Bad Data出现的概率越低，$E_{in} \\approx E_{out}$；如果训练样本 $N$一定的情况下，M越大，也就是说Hypothesis越多，那样可以供我们用算法 $A$进行选择的越多，那么越有可能选到一个好的样本，使得 $E_{in} \\approx 0$\n\n总结如下表：\n\n-                              | M很小的时候 | M很大的时候 | N很小的时候 | N很大的时候 |\n-                              |:----------:|:----------:|:----------:|:----------:|\n$E_{in}(g) \\approx E_{out}(g)$ |Yes，Bad Data的数量也少了| No，Bad Data的数量也多了         |Yes，Bad Data出现的概率变小了 | No，Bad Data出现的概率变大了\n$E_{in}(g) \\approx 0$          |No，选到低错误率的可能性变小了|Yes，选到低错误率的可能性变大了|没必然联系，样本总数多于少，与错误率无关|没必然联系，样本总数多于少，与错误率无关\n\n最终我们的Learning Flow 就可以变成图9。\n\n![The Statistic Learning Flow](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a02388463f51e20944466bd7718bf2921b68e970/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-9%20statistic%20learning%20flow.png)\n\n<center> 图九 The Statistic Learning Flow<sup>[4]</sup></center>\n\n> 在足够样本的情况下，机器算是能学到东西了!\n\n----------------------------------------\n<br><br>\n\n\n# Summary\n这章主要讨论 Whether machine learning is possible or not.\n思路：\n1. 首先直线学习是不可能的：因为输入样本与实际测试样本差别可能很大\n2. 1中讨论的情况是事情，但是有根据统计学中的Hoeffding不等式，可以有补救的办法（某种程度上，可以使得学习后的机器在实际测试中有较小的错误），然后根据Hoeffding不等式，来联系机器学习，说明机器学习是某种程度上可行的\n\n\n----------------------------------------\n<br><br>\n\n\n# Reference\n1.机器学习基石(台湾大学-林轩田)\\4\\4 - 1 - Learning is Impossible- (13-32)\n\n2.机器学习基石(台湾大学-林轩田)\\4\\4 - 2 - Probability to the Rescue (11-33)\n\n3.机器学习基石(台湾大学-林轩田)\\4\\4 - 3 - Connection to Learning (16-46)\n\n4.机器学习基石(台湾大学-林轩田)\\4\\4 - 4 - Connection to Real Learning (18-06)\n\n<br><br>\n----------------------------------------","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-06-4.When can Machine Learn - Feasible of Learning","published":1,"updated":"2018-04-14T19:42:06.503Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2dx0018rwtj9to7o9rc","content":"<h1 id=\"When-can-Machine-Learn-Feasible-of-Learning\"><a href=\"#When-can-Machine-Learn-Feasible-of-Learning\" class=\"headerlink\" title=\"When can Machine Learn? - Feasible of Learning\"></a>When can Machine Learn? - Feasible of Learning</h1><blockquote>\n<p>这章主要讨论 Whether machine learning is possible or not.</p>\n</blockquote>\n<h2 id=\"1-Learning-is-Impossible\"><a href=\"#1-Learning-is-Impossible\" class=\"headerlink\" title=\"1. Learning is Impossible?\"></a>1. Learning is Impossible?</h2><p>在讨论之前，先看下面的一个问题</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3fb303bc1e0eceffd4c6a3dc62dbb9b8bf078c43/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-1%20Learning%20Puzzle.png\" alt=\"Learning Puzzle\"></p>\n<center>图一 Learning Puzzle <sup>[1]</sup></center>\n\n<p><br><br>这类似于一道智商题,却没有标准答案，根据你不同的一个视角，可以找到不同的规则。比如：</p>\n<ol>\n<li>左上角的正方形是否涂黑</li>\n<li>是否对称</li>\n<li>正中间的正方形是否涂黑</li>\n<li>…</li>\n</ol>\n<p>所以无论机器学到的是什么模型，其他人都能说机器说错了。也就是说机器不能真的学习了。</p>\n<hr>\n<p><br><br></p>\n<h2 id=\"2-Probability-of-the-Rescue\"><a href=\"#2-Probability-of-the-Rescue\" class=\"headerlink\" title=\"2. Probability of the Rescue\"></a>2. Probability of the Rescue</h2><h3 id=\"1-Hoeffding-Inequity\"><a href=\"#1-Hoeffding-Inequity\" class=\"headerlink\" title=\"1) Hoeffding Inequity\"></a>1) Hoeffding Inequity</h3><p>上面提到了机器不能学习，因为机器求出来的假设函数$g(x)$很难与目标函数$f(x)$一样：因为数据不一样。 但是根据Hoeffding不等式（公式1），可以证明，在数据量足够大的情况，可以保证$g(x)$很接近于$$f(x)$.</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert \\nu - \\mu \\rvert  > \\epsilon\\right ] \\leq 2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$1$}</script><blockquote>\n<p>其中 $\\rho$为概率符号， $\\lvert \\nu - \\mu \\rvert$表示2个值的近似程度， </p>\n</blockquote>\n<p>$\\epsilon$是这个近似程度的下界，$N$为样本数量的大小，所以不等式的意思是两个值的差别大于 $\\epsilon$的概率小于等于 $2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)$。所以，如果当 $\\epsilon$一定的情况下， 随着样本数量 $N$越大，那么这个差距的可能性越小（参考 $e^{-n}$ 的曲线），当 $\\epsilon$很小且 $N$大到一定的程度的时候，$\\mu$和 $\\nu$差别很小的概率很低，即 $\\mu$和 $\\nu$相等是一个大概近似正确（Probably Approximately Correct, PAC）的情况。</p>\n<p>下面举例说明公式的意义（以概率统计中的从罐子中有放回的取球为例），如图二。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/36d1fe3ba6ea839628bebdfebf2dc55e1ca202d4/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-2%20Sampling.png\" alt=\"Sampling\"></p>\n<center> 图二 Sampling<sup>[2]</sup></center>\n\n\n<p>罐子中只有橙色和绿色的球，其中橙色球的概率为 $\\mu$,那么绿色球的概率为 $1-\\mu$， 如果通过抽样，得到橙色球的概率为 $\\nu$，那么绿色球的概率为$1-\\nu$（其中，$\\mu$是假设的，是未知的，$\\nu$而是通过抽样得到的，已知的)。因为抽样的罐子是均匀是，所以抽样得到的橙色球的概率 $\\nu$要近似于实际罐子中橙色球的概率 $\\mu$。这个近似值得范围就是Hoeffding Inequity所表示的。</p>\n<hr>\n<h3 id=\"2-Connection-Between-Hoeffding-Inequity-and-Learning\"><a href=\"#2-Connection-Between-Hoeffding-Inequity-and-Learning\" class=\"headerlink\" title=\"2) Connection Between Hoeffding Inequity and Learning\"></a>2) Connection Between Hoeffding Inequity and Learning</h3><p>下面引用PPT里面的对比图来进行解释。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a1422a508266710d0f3a0b90c5227c7400307921/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-3%20Connection%20To%20Learning.png\" alt=\"Connection to Learning\"></p>\n<center> 图三 Connetion to Learning <sup>[3]</sup></center>\n\n\n<p>上面的抽样调查中，我们关键有： 罐子中橙色球的实际概率 $\\mu$, 抽样出来的球 ∈ 罐子，抽样的橙色球概率，抽样得到的绿色球概率<br>对应到实际学习中就是： 实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$, 训练样本$x$ ∈ 整个数据集 $X$，训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$, 训练过程中满足 $h(x) = g(x)$ 的概率 $1-E_{in}$,</p>\n<p>并且可以得到以下公式（2）和公式（3）</p>\n<script type=\"math/tex; mode=display\">\n\\nu = \\frac{1}{N} \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]   \\quad (x_i ∈ X)\n\\tag{$2$}</script><script type=\"math/tex; mode=display\">\n\\mu = \\epsilon \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]\n\\tag{$3$}</script><blockquote>\n<p>$\\epsilon$表示为期望值，即实际样本中的错误率 $E_{out}$</p>\n</blockquote>\n<p>为了查看方便，列个表格进行说明</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>罐子</th>\n<th>机器学习</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>罐子中橙色球的实际概率 $\\mu$</td>\n<td>实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$</td>\n</tr>\n<tr>\n<td>抽样出来的球 ∈ 罐子</td>\n<td>训练样本$x$ ∈ 整个数据集 $X$</td>\n</tr>\n<tr>\n<td>抽样的橙色球概率</td>\n<td>训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$</td>\n</tr>\n<tr>\n<td>抽样得到的绿色球概率</td>\n<td>训练过程中满足 $h(x) = g(x)$ 的概率 $1-E_{in}$,</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>因此结合Hoeffding的理论支持后，我们可以扩展机器学习的流程图，如图四所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b89d9143e63c41c2d334d8471c8fa7730702458e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-4%20Learning%20Diagram.png\" alt=\"New Learning Diagram\"></p>\n<center> 图四 New Learning Diagram <sup>[3]</sup></center>\n\n\n<blockquote>\n<p>其中虚线表示未知概率 $\\rho$ 对随机抽样以及实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$  的影响，实线表示训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$。</p>\n</blockquote>\n<p>上面提到的$E_{in}$ $E_{out}$ 可以分别用下面的公式（4）（5）表示</p>\n<script type=\"math/tex; mode=display\">\nE_{in} = \\nu = \\frac{1}{N} \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]   \\quad (x_i ∈ X)\n\\tag{$4$}</script><script type=\"math/tex; mode=display\">\nE_{out} = \\mu = \\epsilon \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]\n\\tag{$5$}</script><p>所以Hoedding的不等式可以变成公式（6）<br>接近于$f(x)$.</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert E_{in} - E_{out} \\rvert  > \\epsilon\\right] \\leq 2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$6$}</script><p>同样的，当 $\\epsilon$一定（一般都很小），并且训练样本 $N$足够大的情况下，我们有 $E_{in} \\approx E_{out}$，也就是说 $g(x) \\approx f(x)$，这时候的流程图就变成了一个 $h$对应一个 $f$的情况了，如图五所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9007d438d4541ff836281cc7d1648bbcdbad927c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-5%20Learning%20Diagram%20-%20the%20verification%20flow.png\" alt=\"A Verification Flow\"></p>\n<center> 图五 A Verification Flow <sup>[3]</sup></center>\n\n<p><br><br>但这个并非是真正意义上的学习，因为只有一个Hypothesis(因为通过一个一段数据集，我们只能得到一个Hypothesis)，所以我们下面讨论多个。</p>\n<h3 id=\"3-Connection-Between-Hoeffding-Inequity-and-Real-Learning\"><a href=\"#3-Connection-Between-Hoeffding-Inequity-and-Real-Learning\" class=\"headerlink\" title=\"3) Connection Between Hoeffding Inequity and Real Learning\"></a>3) Connection Between Hoeffding Inequity and Real Learning</h3><p>上面我们讨论了根据Hoeffding Inequity，在 $N$无限大的时候，一个Hypothesis的 $\\lvert E_{in} - E_{out} \\rvert &lt; \\epsilon$的概率会无限的小，所以同时满足 $\\epsilon$ 很小 且 $N$很大的情况下，我们可以得到 $E_{in} \\approx E_{out}$。</p>\n<blockquote>\n<p>疑问：但是在应对多个Hypothesis的时候，我们就有多个 $h(x)$ 和多个 $E_{in}$，那样我们真的能确定 $E_{in} \\approx E_{out}$ 吗？</p>\n</blockquote>\n<h4 id=\"①-Introduction-of-Bad-Data\"><a href=\"#①-Introduction-of-Bad-Data\" class=\"headerlink\" title=\"① Introduction of Bad Data\"></a>① Introduction of Bad Data</h4><p>在连续抛5次硬币过程中，一个人抛到正面朝上的概率为 $\\frac{1}{32}$，如果现在有150个人，那么这150个人里面，至少有一个人5次都抛出正面朝上的几率为 $1- (\\frac{31}{32})^{150} &gt; 99\\%$。</p>\n<blockquote>\n<p>分析： 因为一个人连续5次抛出正面朝上几率为 $\\frac{1}{32} &lt; 3\\%$， 所以如果机器学习的话，在测试的时候他更偏向于预测“不能连续抛出5次正面朝上”（因为选择会偏向于选择概率更大的一边）。但是当150次试验的时候，一个人连续5次抛出正面朝上的几率 $E_{out}&gt;99\\%$，而在训练的时候的概率$E_{in} &lt; 3\\%$，也就是说机器学习了之后进行了错误的估计（即$E_{in} 远远\\neq E_{out}$。导致这种结果的几率就是所谓的 bad data（也就是Noise:偏差值，数据与所需数据不吻合等）。</p>\n<p>疑问：</p>\n<p>1.如果说存在bad data，那样的话怎么区分呢？（下面会证明bad data出现的概率）。</p>\n<p>2.如果万一 $E_{in}$ 很大的话，那么 $E_{out}$ 也很大，那么这机器学习还什么意义呢？(这个问题后面解释)</p>\n</blockquote>\n<h4 id=\"②-Probability-of-Bad-Data\"><a href=\"#②-Probability-of-Bad-Data\" class=\"headerlink\" title=\"② Probability of Bad Data\"></a>② Probability of Bad Data</h4><p>首先在单个假设的时候如图六所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f7bb05cf9a34185056b07452b108bcd7b2f13f8d/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-6%20bad%20data%20for%20one%20H.png\" alt=\"Bad Data for One h\"></p>\n<center> 图六 Bad Data for One h<sup>[4]</sup></center>\n\n<p>而在多个假设的时候如图七所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f7bb05cf9a34185056b07452b108bcd7b2f13f8d/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-7%20bad%20data%20for%20many%20H.png\" alt=\"Bad Data for Many h\"></p>\n<center> 图七 Bad Data for Many h<sup>[4]</sup></center>\n\n<p>最后计算 Bad Data发生的概率如图8所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a02388463f51e20944466bd7718bf2921b68e970/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-8%20bound%20of%20bad%20data.png\" alt=\"Bound of Bad Data\"></p>\n<center> 图八 Bound of Bad Data<sup>[4]</sup></center>\n\n\n<blockquote>\n<p>由计算结果可以发现，这里的M是一个有限的数，所以当训练样本 $N$ 越大，那么Bad Data出现的概率越低，$E_{in} \\approx E_{out}$；如果训练样本 $N$一定的情况下，M越大，也就是说Hypothesis越多，那样可以供我们用算法 $A$进行选择的越多，那么越有可能选到一个好的样本，使得 $E_{in} \\approx 0$</p>\n</blockquote>\n<p>总结如下表：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>-</th>\n<th style=\"text-align:center\">M很小的时候</th>\n<th style=\"text-align:center\">M很大的时候</th>\n<th style=\"text-align:center\">N很小的时候</th>\n<th style=\"text-align:center\">N很大的时候</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$E_{in}(g) \\approx E_{out}(g)$</td>\n<td style=\"text-align:center\">Yes，Bad Data的数量也少了</td>\n<td style=\"text-align:center\">No，Bad Data的数量也多了</td>\n<td style=\"text-align:center\">Yes，Bad Data出现的概率变小了</td>\n<td style=\"text-align:center\">No，Bad Data出现的概率变大了</td>\n</tr>\n<tr>\n<td>$E_{in}(g) \\approx 0$</td>\n<td style=\"text-align:center\">No，选到低错误率的可能性变小了</td>\n<td style=\"text-align:center\">Yes，选到低错误率的可能性变大了</td>\n<td style=\"text-align:center\">没必然联系，样本总数多于少，与错误率无关</td>\n<td style=\"text-align:center\">没必然联系，样本总数多于少，与错误率无关</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>最终我们的Learning Flow 就可以变成图9。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a02388463f51e20944466bd7718bf2921b68e970/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-9%20statistic%20learning%20flow.png\" alt=\"The Statistic Learning Flow\"></p>\n<center> 图九 The Statistic Learning Flow<sup>[4]</sup></center>\n\n<blockquote>\n<p>在足够样本的情况下，机器算是能学到东西了!</p>\n</blockquote>\n<hr>\n<p><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><p>这章主要讨论 Whether machine learning is possible or not.<br>思路：</p>\n<ol>\n<li>首先直线学习是不可能的：因为输入样本与实际测试样本差别可能很大</li>\n<li>1中讨论的情况是事情，但是有根据统计学中的Hoeffding不等式，可以有补救的办法（某种程度上，可以使得学习后的机器在实际测试中有较小的错误），然后根据Hoeffding不等式，来联系机器学习，说明机器学习是某种程度上可行的</li>\n</ol>\n<hr>\n<p><br><br></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>1.机器学习基石(台湾大学-林轩田)\\4\\4 - 1 - Learning is Impossible- (13-32)</p>\n<p>2.机器学习基石(台湾大学-林轩田)\\4\\4 - 2 - Probability to the Rescue (11-33)</p>\n<p>3.机器学习基石(台湾大学-林轩田)\\4\\4 - 3 - Connection to Learning (16-46)</p>\n<p>4.机器学习基石(台湾大学-林轩田)\\4\\4 - 4 - Connection to Real Learning (18-06)</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"When-can-Machine-Learn-Feasible-of-Learning\"><a href=\"#When-can-Machine-Learn-Feasible-of-Learning\" class=\"headerlink\" title=\"When can Machine Learn? - Feasible of Learning\"></a>When can Machine Learn? - Feasible of Learning</h1><blockquote>\n<p>这章主要讨论 Whether machine learning is possible or not.</p>\n</blockquote>\n<h2 id=\"1-Learning-is-Impossible\"><a href=\"#1-Learning-is-Impossible\" class=\"headerlink\" title=\"1. Learning is Impossible?\"></a>1. Learning is Impossible?</h2><p>在讨论之前，先看下面的一个问题</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3fb303bc1e0eceffd4c6a3dc62dbb9b8bf078c43/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-1%20Learning%20Puzzle.png\" alt=\"Learning Puzzle\"></p>\n<center>图一 Learning Puzzle <sup>[1]</sup></center>\n\n<p><br><br>这类似于一道智商题,却没有标准答案，根据你不同的一个视角，可以找到不同的规则。比如：</p>\n<ol>\n<li>左上角的正方形是否涂黑</li>\n<li>是否对称</li>\n<li>正中间的正方形是否涂黑</li>\n<li>…</li>\n</ol>\n<p>所以无论机器学到的是什么模型，其他人都能说机器说错了。也就是说机器不能真的学习了。</p>\n<hr>\n<p><br><br></p>\n<h2 id=\"2-Probability-of-the-Rescue\"><a href=\"#2-Probability-of-the-Rescue\" class=\"headerlink\" title=\"2. Probability of the Rescue\"></a>2. Probability of the Rescue</h2><h3 id=\"1-Hoeffding-Inequity\"><a href=\"#1-Hoeffding-Inequity\" class=\"headerlink\" title=\"1) Hoeffding Inequity\"></a>1) Hoeffding Inequity</h3><p>上面提到了机器不能学习，因为机器求出来的假设函数$g(x)$很难与目标函数$f(x)$一样：因为数据不一样。 但是根据Hoeffding不等式（公式1），可以证明，在数据量足够大的情况，可以保证$g(x)$很接近于$$f(x)$.</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert \\nu - \\mu \\rvert  > \\epsilon\\right ] \\leq 2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$1$}</script><blockquote>\n<p>其中 $\\rho$为概率符号， $\\lvert \\nu - \\mu \\rvert$表示2个值的近似程度， </p>\n</blockquote>\n<p>$\\epsilon$是这个近似程度的下界，$N$为样本数量的大小，所以不等式的意思是两个值的差别大于 $\\epsilon$的概率小于等于 $2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)$。所以，如果当 $\\epsilon$一定的情况下， 随着样本数量 $N$越大，那么这个差距的可能性越小（参考 $e^{-n}$ 的曲线），当 $\\epsilon$很小且 $N$大到一定的程度的时候，$\\mu$和 $\\nu$差别很小的概率很低，即 $\\mu$和 $\\nu$相等是一个大概近似正确（Probably Approximately Correct, PAC）的情况。</p>\n<p>下面举例说明公式的意义（以概率统计中的从罐子中有放回的取球为例），如图二。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/36d1fe3ba6ea839628bebdfebf2dc55e1ca202d4/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-2%20Sampling.png\" alt=\"Sampling\"></p>\n<center> 图二 Sampling<sup>[2]</sup></center>\n\n\n<p>罐子中只有橙色和绿色的球，其中橙色球的概率为 $\\mu$,那么绿色球的概率为 $1-\\mu$， 如果通过抽样，得到橙色球的概率为 $\\nu$，那么绿色球的概率为$1-\\nu$（其中，$\\mu$是假设的，是未知的，$\\nu$而是通过抽样得到的，已知的)。因为抽样的罐子是均匀是，所以抽样得到的橙色球的概率 $\\nu$要近似于实际罐子中橙色球的概率 $\\mu$。这个近似值得范围就是Hoeffding Inequity所表示的。</p>\n<hr>\n<h3 id=\"2-Connection-Between-Hoeffding-Inequity-and-Learning\"><a href=\"#2-Connection-Between-Hoeffding-Inequity-and-Learning\" class=\"headerlink\" title=\"2) Connection Between Hoeffding Inequity and Learning\"></a>2) Connection Between Hoeffding Inequity and Learning</h3><p>下面引用PPT里面的对比图来进行解释。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a1422a508266710d0f3a0b90c5227c7400307921/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-3%20Connection%20To%20Learning.png\" alt=\"Connection to Learning\"></p>\n<center> 图三 Connetion to Learning <sup>[3]</sup></center>\n\n\n<p>上面的抽样调查中，我们关键有： 罐子中橙色球的实际概率 $\\mu$, 抽样出来的球 ∈ 罐子，抽样的橙色球概率，抽样得到的绿色球概率<br>对应到实际学习中就是： 实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$, 训练样本$x$ ∈ 整个数据集 $X$，训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$, 训练过程中满足 $h(x) = g(x)$ 的概率 $1-E_{in}$,</p>\n<p>并且可以得到以下公式（2）和公式（3）</p>\n<script type=\"math/tex; mode=display\">\n\\nu = \\frac{1}{N} \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]   \\quad (x_i ∈ X)\n\\tag{$2$}</script><script type=\"math/tex; mode=display\">\n\\mu = \\epsilon \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]\n\\tag{$3$}</script><blockquote>\n<p>$\\epsilon$表示为期望值，即实际样本中的错误率 $E_{out}$</p>\n</blockquote>\n<p>为了查看方便，列个表格进行说明</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>罐子</th>\n<th>机器学习</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>罐子中橙色球的实际概率 $\\mu$</td>\n<td>实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$</td>\n</tr>\n<tr>\n<td>抽样出来的球 ∈ 罐子</td>\n<td>训练样本$x$ ∈ 整个数据集 $X$</td>\n</tr>\n<tr>\n<td>抽样的橙色球概率</td>\n<td>训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$</td>\n</tr>\n<tr>\n<td>抽样得到的绿色球概率</td>\n<td>训练过程中满足 $h(x) = g(x)$ 的概率 $1-E_{in}$,</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>因此结合Hoeffding的理论支持后，我们可以扩展机器学习的流程图，如图四所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b89d9143e63c41c2d334d8471c8fa7730702458e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-4%20Learning%20Diagram.png\" alt=\"New Learning Diagram\"></p>\n<center> 图四 New Learning Diagram <sup>[3]</sup></center>\n\n\n<blockquote>\n<p>其中虚线表示未知概率 $\\rho$ 对随机抽样以及实际测试中$h(x) \\neq g(x)$ 的概率 $E_{out}$  的影响，实线表示训练过程中满足 $h(x) \\neq g(x)$ 的概率 $E_{in}$。</p>\n</blockquote>\n<p>上面提到的$E_{in}$ $E_{out}$ 可以分别用下面的公式（4）（5）表示</p>\n<script type=\"math/tex; mode=display\">\nE_{in} = \\nu = \\frac{1}{N} \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]   \\quad (x_i ∈ X)\n\\tag{$4$}</script><script type=\"math/tex; mode=display\">\nE_{out} = \\mu = \\epsilon \\cdot \\sum\\limits_{i=1}^N \\left[\\left[ h(x_i) \\neq f(x_i) \\right]\\right]\n\\tag{$5$}</script><p>所以Hoedding的不等式可以变成公式（6）<br>接近于$f(x)$.</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert E_{in} - E_{out} \\rvert  > \\epsilon\\right] \\leq 2 \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$6$}</script><p>同样的，当 $\\epsilon$一定（一般都很小），并且训练样本 $N$足够大的情况下，我们有 $E_{in} \\approx E_{out}$，也就是说 $g(x) \\approx f(x)$，这时候的流程图就变成了一个 $h$对应一个 $f$的情况了，如图五所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9007d438d4541ff836281cc7d1648bbcdbad927c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-5%20Learning%20Diagram%20-%20the%20verification%20flow.png\" alt=\"A Verification Flow\"></p>\n<center> 图五 A Verification Flow <sup>[3]</sup></center>\n\n<p><br><br>但这个并非是真正意义上的学习，因为只有一个Hypothesis(因为通过一个一段数据集，我们只能得到一个Hypothesis)，所以我们下面讨论多个。</p>\n<h3 id=\"3-Connection-Between-Hoeffding-Inequity-and-Real-Learning\"><a href=\"#3-Connection-Between-Hoeffding-Inequity-and-Real-Learning\" class=\"headerlink\" title=\"3) Connection Between Hoeffding Inequity and Real Learning\"></a>3) Connection Between Hoeffding Inequity and Real Learning</h3><p>上面我们讨论了根据Hoeffding Inequity，在 $N$无限大的时候，一个Hypothesis的 $\\lvert E_{in} - E_{out} \\rvert &lt; \\epsilon$的概率会无限的小，所以同时满足 $\\epsilon$ 很小 且 $N$很大的情况下，我们可以得到 $E_{in} \\approx E_{out}$。</p>\n<blockquote>\n<p>疑问：但是在应对多个Hypothesis的时候，我们就有多个 $h(x)$ 和多个 $E_{in}$，那样我们真的能确定 $E_{in} \\approx E_{out}$ 吗？</p>\n</blockquote>\n<h4 id=\"①-Introduction-of-Bad-Data\"><a href=\"#①-Introduction-of-Bad-Data\" class=\"headerlink\" title=\"① Introduction of Bad Data\"></a>① Introduction of Bad Data</h4><p>在连续抛5次硬币过程中，一个人抛到正面朝上的概率为 $\\frac{1}{32}$，如果现在有150个人，那么这150个人里面，至少有一个人5次都抛出正面朝上的几率为 $1- (\\frac{31}{32})^{150} &gt; 99\\%$。</p>\n<blockquote>\n<p>分析： 因为一个人连续5次抛出正面朝上几率为 $\\frac{1}{32} &lt; 3\\%$， 所以如果机器学习的话，在测试的时候他更偏向于预测“不能连续抛出5次正面朝上”（因为选择会偏向于选择概率更大的一边）。但是当150次试验的时候，一个人连续5次抛出正面朝上的几率 $E_{out}&gt;99\\%$，而在训练的时候的概率$E_{in} &lt; 3\\%$，也就是说机器学习了之后进行了错误的估计（即$E_{in} 远远\\neq E_{out}$。导致这种结果的几率就是所谓的 bad data（也就是Noise:偏差值，数据与所需数据不吻合等）。</p>\n<p>疑问：</p>\n<p>1.如果说存在bad data，那样的话怎么区分呢？（下面会证明bad data出现的概率）。</p>\n<p>2.如果万一 $E_{in}$ 很大的话，那么 $E_{out}$ 也很大，那么这机器学习还什么意义呢？(这个问题后面解释)</p>\n</blockquote>\n<h4 id=\"②-Probability-of-Bad-Data\"><a href=\"#②-Probability-of-Bad-Data\" class=\"headerlink\" title=\"② Probability of Bad Data\"></a>② Probability of Bad Data</h4><p>首先在单个假设的时候如图六所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f7bb05cf9a34185056b07452b108bcd7b2f13f8d/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-6%20bad%20data%20for%20one%20H.png\" alt=\"Bad Data for One h\"></p>\n<center> 图六 Bad Data for One h<sup>[4]</sup></center>\n\n<p>而在多个假设的时候如图七所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f7bb05cf9a34185056b07452b108bcd7b2f13f8d/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-7%20bad%20data%20for%20many%20H.png\" alt=\"Bad Data for Many h\"></p>\n<center> 图七 Bad Data for Many h<sup>[4]</sup></center>\n\n<p>最后计算 Bad Data发生的概率如图8所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a02388463f51e20944466bd7718bf2921b68e970/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-8%20bound%20of%20bad%20data.png\" alt=\"Bound of Bad Data\"></p>\n<center> 图八 Bound of Bad Data<sup>[4]</sup></center>\n\n\n<blockquote>\n<p>由计算结果可以发现，这里的M是一个有限的数，所以当训练样本 $N$ 越大，那么Bad Data出现的概率越低，$E_{in} \\approx E_{out}$；如果训练样本 $N$一定的情况下，M越大，也就是说Hypothesis越多，那样可以供我们用算法 $A$进行选择的越多，那么越有可能选到一个好的样本，使得 $E_{in} \\approx 0$</p>\n</blockquote>\n<p>总结如下表：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>-</th>\n<th style=\"text-align:center\">M很小的时候</th>\n<th style=\"text-align:center\">M很大的时候</th>\n<th style=\"text-align:center\">N很小的时候</th>\n<th style=\"text-align:center\">N很大的时候</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$E_{in}(g) \\approx E_{out}(g)$</td>\n<td style=\"text-align:center\">Yes，Bad Data的数量也少了</td>\n<td style=\"text-align:center\">No，Bad Data的数量也多了</td>\n<td style=\"text-align:center\">Yes，Bad Data出现的概率变小了</td>\n<td style=\"text-align:center\">No，Bad Data出现的概率变大了</td>\n</tr>\n<tr>\n<td>$E_{in}(g) \\approx 0$</td>\n<td style=\"text-align:center\">No，选到低错误率的可能性变小了</td>\n<td style=\"text-align:center\">Yes，选到低错误率的可能性变大了</td>\n<td style=\"text-align:center\">没必然联系，样本总数多于少，与错误率无关</td>\n<td style=\"text-align:center\">没必然联系，样本总数多于少，与错误率无关</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>最终我们的Learning Flow 就可以变成图9。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a02388463f51e20944466bd7718bf2921b68e970/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter4-9%20statistic%20learning%20flow.png\" alt=\"The Statistic Learning Flow\"></p>\n<center> 图九 The Statistic Learning Flow<sup>[4]</sup></center>\n\n<blockquote>\n<p>在足够样本的情况下，机器算是能学到东西了!</p>\n</blockquote>\n<hr>\n<p><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><p>这章主要讨论 Whether machine learning is possible or not.<br>思路：</p>\n<ol>\n<li>首先直线学习是不可能的：因为输入样本与实际测试样本差别可能很大</li>\n<li>1中讨论的情况是事情，但是有根据统计学中的Hoeffding不等式，可以有补救的办法（某种程度上，可以使得学习后的机器在实际测试中有较小的错误），然后根据Hoeffding不等式，来联系机器学习，说明机器学习是某种程度上可行的</li>\n</ol>\n<hr>\n<p><br><br></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>1.机器学习基石(台湾大学-林轩田)\\4\\4 - 1 - Learning is Impossible- (13-32)</p>\n<p>2.机器学习基石(台湾大学-林轩田)\\4\\4 - 2 - Probability to the Rescue (11-33)</p>\n<p>3.机器学习基石(台湾大学-林轩田)\\4\\4 - 3 - Connection to Learning (16-46)</p>\n<p>4.机器学习基石(台湾大学-林轩田)\\4\\4 - 4 - Connection to Real Learning (18-06)</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"5.Why can Machine Learn?","date":"2017-10-08T08:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Why can Machine Learn?\n\n>这一节我的思路是把老师的第五，六、七节的内容结合起来了，并且思路不完全按照老师的授课来走。\n\n----------------------------------\n## 1. Preview of Last Chapter\n> 因为这一章的讨论是基于上一章最后一节得到的公式的，所以我们先Recap一下。\n\n上一节中，我们最后得出公式（1）（2）\n$$\n\\rho \\left[  BAD \\quad D \\right] \\leq 2M \\cdot \\exp \\left( -2 \\epsilon^2N  \\right) \\\\\n\\tag{$1$}\n$$\n$$\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 2M \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$2$}\n$$\n\n> 这里的M是一个有限的数，所以当训练样本 $N$ 越大，那么Bad Data出现的概率越低，$E_{in} \\approx E_{out}$；如果训练样本 $N$一定的情况下，M越大，也就是说Hypothesis越多，那样可以供我们用算法 $A$进行选择的越多，那么越有可能选到一个好的样本，使得 $E_{in} \\approx 0$\n\n总结如下表：\n\n-                              | M很小的时候 | M很大的时候 | N很小的时候 | N很大的时候 |\n-                              |:----------:|:----------:|:----------:|:----------:|\n$E_{in}(g) \\approx E_{out}(g)$ |Yes，Bad Data的数量也少了| No，Bad Data的数量也多了         |Yes，Bad Data出现的概率变小了 | No，Bad Data出现的概率变大了\n$E_{in}(g) \\approx 0$          |No，选到低错误率的可能性变小了|Yes，选到低错误率的可能性变大了|没必然联系，样本总数多于少，与错误率无关|没必然联系，样本总数多于少，与错误率无关\n\n>从表格中可以看出，$M$ 太大太小都会对机器学习的有效性造成影响，所以我们要进一步缩小$M$ 的取值范围。\n\n\n>问题：怎么缩小$M$的取值范围\n>\n>解决方案：在上一节中，我们再推导的过程中使用了联合上限（Union Bound)，造成实际的上限被放大了很多。因为在做集合的或运算的时候，我们单纯的把各个集合加起来，但是却没有减去他们的交集部分，所以造成了上限被放大的问题。\n\n关于Union Bound 的推导可以看这个链接[Boole's inequality](https://en.wikipedia.org/wiki/Boole%27s_inequality \"Boole's inequality\")\n\n总的来说就是因为我们推导公式（1）的时候使用了Union Bound，所以导致了不等式右边的值（上限）被放大了，所以现在我们可以把它进行缩减，求出有效的 $M$值(即 $M_H(N)$)，下面我们来推导这个有效值。\n\n## 2. VC Bound - A Upper Bound of Hoeffding Inequity\n### 1) Introduction\n场景：对于不同数量$N$的训练数据，有多少种不同的方法 $effective(N)$ 可以区分他们？\n\n当 $N=1$ 的时候，如图一所示，共有2种方法，$effective(N) = 2 = 2^1$。\n\n![N=1](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-1%20number%3D1.png)\n<center> 图一 N=1 <sup>[1]</sup></center>\n\n\n当 $N=2$ 的时候，如图二所示，共有4种方法，$effective(N) = 4 = 2^2$。\n\n![N=2](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-2%20number%3D2.png)\n<center> 图二 N=2 <sup>[1]</sup></center>\n\n\n当 $N=3$ 的时候，如图三、四所示，最多有8种方法，虽然说在特定的情形下，可能只有6中种方法，$effective(N) = 8 = 2^3$。\n\n![N=3 with error](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-4%20number%3D3%20with%20error.png)\n<center> 图三 N=3 with error <sup>[2]</sup></center>\n\n\n![N=3](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-3%20number%3D3.png)\n<center> 图四 N=3 <sup>[1]</sup></center>\n\n\n\n当 $N=4$ 的时候，如图五所示，无论怎么放着4个点，最多只有14种方法，$effective(N) = 14 < 2^4$。\n\n![N=4](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-5%20number%3D4.png)\n<center> 图五 N=4 <sup>[1]</sup></center>\n\n\n当 $N=5$ 的时候，很显然 $effective(N) = 32 <  2^5$，就不再继续讨论了。\n\n总结如下表：\n\nN|$effctive(N)$\n-  |-          |\n1  |$2 = 2^1$  |\n2  |$4 = 2^2$  |\n3  |$8 = 2^3$  |\n4  |$14 < 2^5$ |\n5  |$32 << 2^6$|\n...|...|\nN|$effctive(N) << 2^N$|\n\n总结如图六，可以看出当 $N>4$的时候，$effective(N) < 2^N$，也就是说我们把 $M和N$ 的关系构建了起来，所以我们可以用 $effective(N)$ 去替换 $M$，得到公式(3)\n\n$$\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 2 \\cdot effective(N) \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$3$}\n$$\n\n![Summary](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-6%20summary.png)\n<center> 图六 Summary <sup>[1]</sup></center>\n\n\n\n### 2) Growth function\n上面Binary Clasification的分类方法叫做二分类法（dichotomy)，为了更好地表示 $N和effective(N)$ 的关系，我们引入成长函数（Growth Function) $M_H(N)$ 来表示，具体的数学表达如公式（4）所示。\n$$\nM_H(N) = \\max\\limits_{x_1,x_2,...,x_N ∈ X}  \\lvert H(x_1,x_2,...,x_N) \\rvert\n\\tag{$4$}  \\quad(其中，上限为2^N)\n$$\n\n### 3) Different Types of Growth function\n#### ① Growth Function for Positive Rays\n> Positive Rays 是用一个一维向量作用于一维坐标上，与该向量同方向的值为+1，反方向为-1\n如图七所示，Postives Rays 的成长函数为 $M_H(N) = (N-1)$，当 $N \\geq 2$的时候，$M_H(N) < 2^N = O(N)$\n\n![Growth Function for Positive Rays](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-7%20Growth%20Function%20for%20Positive%20Rays.png)\n<center> 图七 Growth Function for Positive Rays <sup>[2]</sup></center>\n\n\n\n#### ② Growth Function for Positive Interval\n> Positive Interval 是用一个一维“线段”作用于一维坐标上，与在线段里面的值为+1，外面的为-1\n如图八所示，Positive Interval 的成长函数为 $M_H(N) = C_{N+1}^2 + 1 = \\frac{1}{2}N^2 + \\frac{1}{2}N + 1$，当 $N \\geq 3$的时候，$M_H(N) < 2^N = O(N^2)$\n\n![Growth Function for Positive Interval](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-8%20Growth%20Function%20for%20Positive%20Intervals.png)\n<center> 图八 Growth Function for Positive Interval <sup>[2]</sup></center>\n\n\n\n#### ③ Growth Function for Convex Sets\n> Convex Sets 不太好理解。可以理解成在二维坐标上，用凸多边形去把所需要的点串起来。在多边形顶点上的点的值为+1，不在的为-1。因为我们讨论的是最大的可能性，所以当我们把所有的点都放在一个圆上的时候，必定存在一个凸多边形可以连接任意多个点（即可以画出任意多边形），然后再把所有点的组合情况加起来\n如图九所示，Convex Setsl 的成长函数为 $M_H(N) = \\sum\\limits_{i=0}^{N}C_{i}^{N} = 2^N$\n\n![Growth Function for Convex Sets](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-9%20Growth%20Function%20for%20Convex%20Sets.png)\n<center> 图九 Growth Function for Convex Sets <sup>[2]</sup></center>\n\n\n\n### 4) Break Point of Growth function\n我们称能满足完全二分类(出现不同种类的数量为$2^N$)的情况为shattered,能shattered的最大的点为突破点(break point)。\n然后根据上面对N从1-5的尝试，得到的最大可能性如下表\n\nN|$effctive(N)$\n-|-|\n1|$2 = 2^1$|\n2|$4 = 2^2$|\n3|$8 = 2^3$|\n4|$14 < 2^5$|\n5|$32 << 2^6$|\n...|...|\nN|$effctive(N) << 2^N$|\n\n可以推断出公式（3）\n$$\neffctive(N): M_H(N) \\leq \\max( possible \\quad M_H(N) \\quad Given \\quad break- point \\quad(K)) \\leq 2^N\n\\tag{$3$}\n$$\n\n\n上面关于Growth Function讨论的几种情况的Break Point 如图十所示，我们可以看出成长函数的复杂度与Break Point的大小存在一定的关系。\n\n![Break Point of Growth function](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/033cce3063c6d35c7ac55af137821cf97819be44/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-10%20Break%20Point.png)\n<center> 图十 Break Point of Growth function <sup>[3]</sup></center>\n\n\n### 5) Bounding Function\n#### ① Introduction of Bounding Function\n根据上一节的Break Point $K$和样本点 $N$的关系，我们引入一个新概念，上限函数(Bounding Function) $B(N,K)$。这个函数表示有$N$个样本点且成长函数的突破点是$K$的时候，最多有多少种组合情况，比如说$B(3,2) = 3$（这个比较容易想象，这里就不展开讨论了）。并且这个上限函数满足公式（4），因为这是采用而分类的方法来进行的，最大值为$2^N$。\n$$\nB(N,K) \\leq 2^N\n\\tag{$3$}\n$$\n\n但是显然在上面的例子中，我们可以看到$B(N,K) < 2^N \\quad(N \\geq K)$，所以我们下面进一步确定这个上限函数的最大值。\n\n#### ② Proof of Bounding Function\n\n我们下面用表格的方式来表示$B(N,K)$ 表格如下表，我们下面将会填满这个表格来找出相应的规律\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      |                        |                        |                        |                        |                        |                        | ... |\nN=2      |                        |                        |                        |                        |                        |                        | ... |\nN=3      |                        |                        |                        |                        |                        |                        | ... |\nN=4      |                        |                        |                        |                        |                        |                        | ... |\nN=5      |                        |                        |                        |                        |                        |                        | ... |\nN=6      |                        |                        |                        |                        |                        |                        | ... |\n...      |                        |                        |                        |                        |                        |                        | ... |\n\n1.根据上面的规律，我们知道在$N<K$的时候，$B(N,K) = 2^N$，所以表格更新如下\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      |                        | <font color=#FF0000>2</font>  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      |                        |                        | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      |                        |                        |                        | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      |                        |                        |                        |                        | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      |                        |                        |                        |                        |                        | <font color=#FF0000>32 | ... |\nN=6      |                        |                        |                        |                        |                        |                        | ... |\n...      |                        |                        |                        |                        |                        |                        | ... |\n\n2.然后当$K=1$的时候，我们至少有一种分法（全正或者全负），所以第一列全部为1，表格更新如下。\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      | <font color=#0000FF>1  | <font color=#FF0000>2  | <font color=#FF0120>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      | <font color=#0000FF>1  |                        | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      | <font color=#0000FF>1  |                        |                        | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      | <font color=#0000FF>1  |                        |                        |                        | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      | <font color=#0000FF>1  |                        |                        |                        |                        | <font color=#FF0000>32 | ... |\nN=6      | <font color=#0000FF>1  |                        |                        |                        |                        |                        | ... |\n...      | <font color=#0000FF>1  |                        |                        |                        |                        |                        | ... |\n\n3.接着，当$N=K$的时候，我们上面也可以看到，所有的值最大都等于$2^N-1$(因为不能所有情况都出现一次)，所以表格更新如下。\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      | <font color=#0000FF>1  | <font color=#FF0000>2  | <font color=#FF0120>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      | <font color=#0000FF>1  | <font color=#FF0000>3  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      | <font color=#0000FF>1  |                        | <font color=#FF0000>7  | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      | <font color=#0000FF>1  |                        |                        | <font color=#FF0000>15 | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      | <font color=#0000FF>1  |                        |                        |                        | <font color=#FF0000>31 | <font color=#FF0000>32 | ... |\nN=6      | <font color=#0000FF>1  |                        |                        |                        |                        | <font color=#FF0000>63 | ... |\n...      | <font color=#0000FF>1  |                        |                        |                        |                        |                        | ... |\n\n4.在之前的章节，我们也数过$B(3,2) = 4$，其实看到这里我们已经大概有一些规律了：下面一项为上面两项之和， $B(N,K) = B(N-1, K) + B(N-1, K-1)$。当然我们只是猜测，下面我们继续证明。表格更新如下：\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      | <font color=#0000FF>1  | <font color=#FF0000>2  | <font color=#FF0120>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      | <font color=#0000FF>1  | <font color=#FF0000>3  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      | <font color=#0000FF>1  | <font color=#F0FFF0)>4 | <font color=#FF0000>7  | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      | <font color=#0000FF>1  |                        |                        | <font color=#FF0000>15 | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      | <font color=#0000FF>1  |                        |                        |                        | <font color=#FF0000>31 | <font color=#FF0000>32 | ... |\nN=6      | <font color=#0000FF>1  |                        |                        |                        |                        | <font color=#FF0000>63 | ... |\n...      | <font color=#0000FF>1  |                        |                        |                        |                        |                        | ... |\n\n\n5.我们证明上面的猜想$B(N,K) = B(N-1, K) + B(N-1, K-1)$\n1）首先我们遍历B(4,3)，可以得到图十一的结果，然后我们整理了一下结果的顺序，可以发现橙色区域 {$x_1,x_2,x_3$}结果分别出现了2次，而紫色区域的{$x_1,x_2,x_3$}结果只出现了1次。\n\n![Reorganized Dichotomies of B(4,3) - 1][11]\n\n[11]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\n\n<center> 图十一 Reorganized Dichotomies of B(4,3) - 1 <sup>[4]</sup></center>\n\n\n2）所以我们单独把{$x_1,x_2,x_3$}，提出来看，并把橙色区域的个数设为 $\\alpha$，紫色区域的个数为 $\\beta$，那么原来4个点的情况 $B(4,3) = 2 \\alpha + \\beta$，而3个点的情况 $B(3,3) = \\alpha + \\beta$，如下图十二所示。\n\n![Reorganized Dichotomies of B(4,3) - 2][12]\n\n[12]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%202.png\n\n<center> 图十二 Reorganized Dichotomies of B(4,3) - 2 <sup>[4]</sup></center>\n\n3)接着我们单独看 $\\alpha$可以发现这个刚好是 $B(3,2)$ 的最大可能性，也就是说$ \\alpha \\leq B(3,2) = 4 $，如图十三所示。\n\n![Reorganized Dichotomies of B(4,3) - 3][13]\n\n[13]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%203.png\n\n<center> 图十三 Reorganized Dichotomies of B(4,3) - 3 <sup>[4]</sup></center>\n\n\n4） 根据上面的分析，我们目前得到三个公式，如下面的公式（4）（5）（6）。\n$$\nB(4,5) = 2 \\cdot \\alpha + \\beta\n\\tag{$4$}\n$$\n$$\n\\alpha + \\beta \\leq B(3,3)\n\\tag{$5$}\n$$\n$$\n\\alpha \\leq B(3,2)\n\\tag{$6$}\n$$\n\n所以把公式(5)(6)加起来，我们可以更新公式（4）为公式（7）\n$$\nB(4,5) \\leq 2 \\cdot \\alpha + \\beta\n\\tag{$7$}\n$$\n\n5）最后我们用同样的方法来研究$B(N,K)$可以很容易证明到公式（8）（9）（10）\n$$\nB(N - 1,K) \\leq \\sum\\limits_{i=0}^{k-1} C_{N-1}^{i}\n\\tag{$8$}\n$$\n$$\nB(N - 1,K - 1) \\leq \\sum\\limits_{i=0}^{k-2} C_{N-1}^{i} = \\sum\\limits_{i=1}^{k-1} C_{N-1}^{i}\n\\tag{$9$}\n$$\n$$\n\\begin{align}\nB(N,K)  & \\leq {B(N-1,K) + B(N-1,K-1)} \\\\\n        & \\leq \\sum\\limits_{i=0}^{k-1} C_{N-1}^{i} + \\sum\\limits_{i=1}^{k-1} C_{N-1}^{i} \\\\\n        & = C_{N-1}^0 + \\sum\\limits_{i=1}^{k-1} \\left( C_{N-1}^{i} + C_{N-1}^{i}\\right)  \\\\\n        & = 1 + \\sum\\limits_{i=1}^{k-1} C_{N}^{i}  \\\\\n        & = C_{N}^0 + \\sum\\limits_{i=1}^{k-1} C_{N}^{i}  \\\\\n        & = \\sum\\limits_{i=0}^{k-1} C_{N}^{i}\n\\end{align}\n\\tag{$10$}\n$$\n\n所以我们的表格更新如下：\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      | <font color=#0000FF>1  | <font color=#FF0000>2  | <font color=#FF0120>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      | <font color=#0000FF>1  | <font color=#FF0000>3  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      | <font color=#0000FF>1  | <font color=#F0FFF0)>4 | <font color=#FF0000>7  | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      | <font color=#0000FF>1  | <font color=#FFA500)>$\\leq5$  | <font color=#FFA500>11  | <font color=#FF0000>15 | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      | <font color=#0000FF>1  | <font color=#FFA500)>$\\leq6$  | <font color=#FFA500)>$\\leq16$  | <font color=#FFA500)>$\\leq15$  | <font color=#FF0000>31 | <font color=#FF0000>32 | ... |\nN=6      | <font color=#0000FF>1  | <font color=#FFA500)>$\\leq7$  | <font color=#FFA500)>$\\leq22$  | <font color=#FFA500)>$\\leq26$  | <font color=#FFA500)>$\\leq57$  | <font color=#FF0000>63 | ... |\n...      | <font color=#0000FF>1  |              ...              |              ...              |              ...              |              ...              |              ...              | ... |\n\n我们再把$N^{K-1}$的表格整理如下：\n\n$N^{N-1}$|          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      |           1            |           1            |           1            |           1            |           1            |           1            | ... |\nN=2      |           1            |           2            |           4            |           8            |           8            |           16           | ... |\nN=3      |           1            |           3            |           9            |           27           |           27           |           81           | ... |\nN=4      |           1            |           4            |           16           |           64           |           64           |           256          | ... |\nN=5      |           1            |           5            |           25           |           125          |           125          |           625          | ... |\nN=6      |           1            |           6            |           36           |           216          |           216          |           1296         | ... |\n...      |          ...           |          ...           |          ...           |          ...           |          ...           |          ...           | ... |\n\n对比图参考老师上课的PPT，如图十四所示。\n\n![Comparision of B(N,K) and N^(K-1)][14]\n\n[14]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-14%20Comparision%20of%20B(N%2CK)%20and%20N%5E(K-1).png\n\n<center> 图十四 Comparision of B(N,K) and N^(K-1) <sup>[6]</sup></center>\n\n\n总结起来就是：在$N \\geq 2, K \\geq 3$的时候，总有公式（11）的情况。\n$$\nM_H(N) \\leq B(N,K) = \\sum\\limits_{i=0}^{K-1} \\leq N^{K-1}\n\\tag{$11$}\n$$\n\n\n### 6) Vapnik-Chervonenkis (VC) bound\n\n@TODO: 这一节主要是证明从数学的角度上证明VC Bound 并以此更新Hoeffding Inequity。目前听得失一知半解，所以只贴出结论，后面再补充。\n\n\n结论如图十五所示。\n\n![Vapnik-Chervonenkis (VC) bound](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/738603b7f8e8f10505a791140c32f4677d4e7d84/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-13%20VC%20bound.png)\n\n<center> 图十五 Vapnik-Chervonenkis (VC) bound <sup>[5]</sup></center>\n\n这个VC Bound的作用是把之前Hoeffding的参数 $M$替换成这里引入的成长函数 $M_H(N)$，并构建出成长函数与样本数量（N）的关系这样的话，我们就可以容易的得到结论：在样本N足够大时候，发生Bad Data的概率小于 $epsilon$ ($E_{in} \\approx E_{out}$)，可以得出错误率也低($E_{in} \\approx 0$)，说明机器学习是可能的。\n\n也就是说要说机器可以学习必须满足下面的条件：\n1. 假设空间的成长函数 $M_H(N)$ 存在Break Point K （即有一个好的假设空间$H$)\n2. 输入数据的样本 $N$ 足够大（有一个好的数据集 $D$）\n3. 存在一个算法，能够找出能在假设空间 $H$ 中找到一个值使得错误率 $E_{in}$ 足够小 （有一个好的算法 $A$ -->也就是我们后面会研究的重点）\n\n其中：条件1和2通过VC Bound保证了 $E_{in} \\approx E_{out}$，条件3保证了 $E_{in} \\approx 0$ $\\Longrightarrow$ Machine Can Learn.\n\n\n\n------------------------------------\n<br><br>\n\n\n## 3. The VC Dimension\n### 1) Definition of VC Dimension\nVC Dimension( $d_{vc}$ )指的是能够使得成长函数可以被shatter的最大值（即 Break Point - 1)，用符号表示为公式（12）。\n$$\nd_{vc} = min(K) -1\n\\tag{$12$}\n$$\n\n上面的Hoeffding Inequity可以变成公式（13）\n$$\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 4 \\cdot (2N)^{d_{vc}} \\cdot \\exp \\left( -\\frac{1}{8} \\epsilon^2 N \\right)\n\\tag{$13$}\n$$\n\n因此，根据这个特点，我们只要确保一个成长函数存在 VC Dimension，我们就可以确定他存在Break Point，是一个好的假设空间。\n\n### 2) Generalization Error\n我们引入泛化误差 $\\delta$ 表示 $E_{in}(g) 和 E_{out}(g)$ 的接近程度，即 $\\delta = E_{in}(g) - E_{out}(g)$ ，根据公式（13），我们稍作化简，如公式（14）。\n$$\n\\begin{align}\n\\delta                           &= 4 \\cdot (2N)^{d_{vc}} \\cdot \\exp \\left( -\\frac{1}{8} \\epsilon^2 N \\right) \\\\\n\\frac{4 (2N)^{d_{vc}}}{\\delta}   &= \\exp(\\frac{1}{8} \\epsilon^2 N)  \\\\\n\\epsilon                         &= \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}\n\\end{align}\n\\tag{$14$}\n$$\n\n也就是说 $E_{in}(g) - E_{out}(g)$ 的误差会小于等于 $\\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}$。 所以我们可以求得 $E_{out}$ 的范围如公式（15）\n$$\nE_{in}(g) -  \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )} \\leq E_{out}(g) \\leq E_{in}(g) + \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )})\n\\tag{$15$}\n$$\n\n### 3) Model Complexity\n上一节，我们求出了 $E_{in}(g) - E_{out}(g)$ 的误差，为了方便引用，我们引入了新的概念：模型复杂度（Model Complexitiy）来表示这个误差值，数学表示如公式（16）\n$$\n\\Omega(N,H,\\delta) =  \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}\n\\tag{$15$}\n$$\n可以看出，随着VC Dimension的增大，$\\Omega$也会变大，然后 $E_{in}(g)$ 也会随着VC Dimension的增大而变小（因为选择的假设空间大了），但是 $E_{out}(g)$ 却不是一个单调函数，因为公式（15），然后这2个值一个变大一个变小，但是最终的话，$E_{out}$ 的曲线是先下降，然后上升（遍历一边就可以得到结果了），所以找到 $d_{vc}^{*}$ 很重要。(因为 $E_{out}$ 才是我们机器学习最重要的指标）\n结果如图十四所示。\n![Error and VC dimension](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bab59f769a00e4a303287ff809ff1df7a2916f55/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-15%20Error%20and%20VC%20dimension.png)\n<center> 图十六 Error and VC dimension <sup>[6]</sup></center>\n\n\n### 4) How much Data We need Theoretically and Practically\n问题：假如现在老板给员工下达了一个任务，要求这个模型的 ${\\epsilon = 0.1，\\delta = 0.1，  d_{vc} = 3}$ ，那样的话，我们需要多少个样本 $N$ 才能满足要求呢？\n回答：根据上面的公式（14），分别代入参数到等式中，可以求得样本数量 $N$ 如图十五的橙色区域所示。但是实际上，我们只需要 $10d_{vc}$就足够了。\n\n![How much Data We need Theoretically and Practically](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bab59f769a00e4a303287ff809ff1df7a2916f55/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-15%20Error%20and%20VC%20dimension.png)\n<center> 图十七 How much Data We need Theoretically and Practically <sup>[6]</sup></center>\n<br>\n\n因为我们在计算的时候同样的把上限给放大了，放大的原因如下所示:\n1. Hoeffding Inequity 不需要知道未知的情况，但是VC Bound可以用于各种分布，各种目标函数(因为 VC Bound的推导是基于不同的N和K)；\n2. 在给Binary Classification 强行装上成长函数本身就是一个宽松的上界，但是VC Bound可以用于各种数据样本；\n3. 使用二项式 $N^{d_{vc}}$ 作为成长函数的上界使得约束更加宽松，但是VC Bound可以用于任意具有相同VC维的假设空间；\n4. 联合限制（union bound）并不是一定会选择出现不好事情的假设函数，但是VC Bound可以用于任意算法。\n\n\n---------------------------------------------\n<br>\n<br>\n\n# Summary\n1. 我们首先通过回顾上一节的内容，得到结论是根据Hoeffding Inquity: 要使得机器可以学习的条件是 ① $E_{in} \\approx E_{out}$ ② $E_{in} \\approx 0$\n2. 接着我们讨论了什么情况下才能保证这2个条件满足，进行了讨论，最终我们通过引入① Growth Function ② Break Point ③ VC Bound, VC Dimension 更改Hoeffding Inequity的上限，最终得到我们需要的答案：\n    - 假设空间的成长函数 $M_H(N)$ 存在Break Point $K$ （即有一个好的假设空间 $H$)\n    - 输入数据的样本 $N$ 足够大（有一个好的数据集 $D$）\n    - 存在一个算法，能够找出能在假设空间 $H$ 中找到一个值使得错误率 $E_{in}$ 足够小 （有一个好的算法 $A$ -->也就是我们后面会研究的重点）\n    - 其中：条件1和2通过VC Bound保证了 $E_{in} \\approx E_{out}$，条件3保证了 $E_{in} \\approx 0$ ⟹ Machine Can Learn.\n3. 之后我们讨论了理论上 $N \\approx 10000 d_{vc}$ 而实际上只需要 $N \\approx 10 d_{vc}$ 的能使得 $E_{out}$ 最小，并且分析了为什么理论上和实际上差别这么大\n\n\n---------------------------------------------\n<br>\n<br>\n\n\n\n# Reference\n[1]机器学习基石(台湾大学-林轩田)\\5\\5 - 2 - Effective Number of Lines (15-26)\n\n[2]机器学习基石(台湾大学-林轩田)\\5\\5 - 3 - Effective Number of Hypotheses (16-17)\n\n[3]机器学习基石(台湾大学-林轩田)\\5\\5 - 4 - Break Point (07-44)\n\n[4]机器学习基石(台湾大学-林轩田)\\6\\6 - 3 - Bounding Function- Inductive Cases (14-47)\n\n[5]机器学习基石(台湾大学-林轩田)\\6\\6 - 4 - A Pictorial Proof (16-01)\n\n[6]机器学习基石(台湾大学-林轩田)\\7\\7 - 4 - Interpreting VC Dimension (17-13)\n\n<br>\n<br>\n---------------------------------------------\n","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-08-5.Why Can Machine Learn.md","raw":"---\ntitle: 5.Why can Machine Learn?\ndate: 2017-10-08 16:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Why can Machine Learn?\n\n>这一节我的思路是把老师的第五，六、七节的内容结合起来了，并且思路不完全按照老师的授课来走。\n\n----------------------------------\n## 1. Preview of Last Chapter\n> 因为这一章的讨论是基于上一章最后一节得到的公式的，所以我们先Recap一下。\n\n上一节中，我们最后得出公式（1）（2）\n$$\n\\rho \\left[  BAD \\quad D \\right] \\leq 2M \\cdot \\exp \\left( -2 \\epsilon^2N  \\right) \\\\\n\\tag{$1$}\n$$\n$$\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 2M \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$2$}\n$$\n\n> 这里的M是一个有限的数，所以当训练样本 $N$ 越大，那么Bad Data出现的概率越低，$E_{in} \\approx E_{out}$；如果训练样本 $N$一定的情况下，M越大，也就是说Hypothesis越多，那样可以供我们用算法 $A$进行选择的越多，那么越有可能选到一个好的样本，使得 $E_{in} \\approx 0$\n\n总结如下表：\n\n-                              | M很小的时候 | M很大的时候 | N很小的时候 | N很大的时候 |\n-                              |:----------:|:----------:|:----------:|:----------:|\n$E_{in}(g) \\approx E_{out}(g)$ |Yes，Bad Data的数量也少了| No，Bad Data的数量也多了         |Yes，Bad Data出现的概率变小了 | No，Bad Data出现的概率变大了\n$E_{in}(g) \\approx 0$          |No，选到低错误率的可能性变小了|Yes，选到低错误率的可能性变大了|没必然联系，样本总数多于少，与错误率无关|没必然联系，样本总数多于少，与错误率无关\n\n>从表格中可以看出，$M$ 太大太小都会对机器学习的有效性造成影响，所以我们要进一步缩小$M$ 的取值范围。\n\n\n>问题：怎么缩小$M$的取值范围\n>\n>解决方案：在上一节中，我们再推导的过程中使用了联合上限（Union Bound)，造成实际的上限被放大了很多。因为在做集合的或运算的时候，我们单纯的把各个集合加起来，但是却没有减去他们的交集部分，所以造成了上限被放大的问题。\n\n关于Union Bound 的推导可以看这个链接[Boole's inequality](https://en.wikipedia.org/wiki/Boole%27s_inequality \"Boole's inequality\")\n\n总的来说就是因为我们推导公式（1）的时候使用了Union Bound，所以导致了不等式右边的值（上限）被放大了，所以现在我们可以把它进行缩减，求出有效的 $M$值(即 $M_H(N)$)，下面我们来推导这个有效值。\n\n## 2. VC Bound - A Upper Bound of Hoeffding Inequity\n### 1) Introduction\n场景：对于不同数量$N$的训练数据，有多少种不同的方法 $effective(N)$ 可以区分他们？\n\n当 $N=1$ 的时候，如图一所示，共有2种方法，$effective(N) = 2 = 2^1$。\n\n![N=1](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-1%20number%3D1.png)\n<center> 图一 N=1 <sup>[1]</sup></center>\n\n\n当 $N=2$ 的时候，如图二所示，共有4种方法，$effective(N) = 4 = 2^2$。\n\n![N=2](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-2%20number%3D2.png)\n<center> 图二 N=2 <sup>[1]</sup></center>\n\n\n当 $N=3$ 的时候，如图三、四所示，最多有8种方法，虽然说在特定的情形下，可能只有6中种方法，$effective(N) = 8 = 2^3$。\n\n![N=3 with error](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-4%20number%3D3%20with%20error.png)\n<center> 图三 N=3 with error <sup>[2]</sup></center>\n\n\n![N=3](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-3%20number%3D3.png)\n<center> 图四 N=3 <sup>[1]</sup></center>\n\n\n\n当 $N=4$ 的时候，如图五所示，无论怎么放着4个点，最多只有14种方法，$effective(N) = 14 < 2^4$。\n\n![N=4](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-5%20number%3D4.png)\n<center> 图五 N=4 <sup>[1]</sup></center>\n\n\n当 $N=5$ 的时候，很显然 $effective(N) = 32 <  2^5$，就不再继续讨论了。\n\n总结如下表：\n\nN|$effctive(N)$\n-  |-          |\n1  |$2 = 2^1$  |\n2  |$4 = 2^2$  |\n3  |$8 = 2^3$  |\n4  |$14 < 2^5$ |\n5  |$32 << 2^6$|\n...|...|\nN|$effctive(N) << 2^N$|\n\n总结如图六，可以看出当 $N>4$的时候，$effective(N) < 2^N$，也就是说我们把 $M和N$ 的关系构建了起来，所以我们可以用 $effective(N)$ 去替换 $M$，得到公式(3)\n\n$$\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 2 \\cdot effective(N) \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$3$}\n$$\n\n![Summary](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-6%20summary.png)\n<center> 图六 Summary <sup>[1]</sup></center>\n\n\n\n### 2) Growth function\n上面Binary Clasification的分类方法叫做二分类法（dichotomy)，为了更好地表示 $N和effective(N)$ 的关系，我们引入成长函数（Growth Function) $M_H(N)$ 来表示，具体的数学表达如公式（4）所示。\n$$\nM_H(N) = \\max\\limits_{x_1,x_2,...,x_N ∈ X}  \\lvert H(x_1,x_2,...,x_N) \\rvert\n\\tag{$4$}  \\quad(其中，上限为2^N)\n$$\n\n### 3) Different Types of Growth function\n#### ① Growth Function for Positive Rays\n> Positive Rays 是用一个一维向量作用于一维坐标上，与该向量同方向的值为+1，反方向为-1\n如图七所示，Postives Rays 的成长函数为 $M_H(N) = (N-1)$，当 $N \\geq 2$的时候，$M_H(N) < 2^N = O(N)$\n\n![Growth Function for Positive Rays](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-7%20Growth%20Function%20for%20Positive%20Rays.png)\n<center> 图七 Growth Function for Positive Rays <sup>[2]</sup></center>\n\n\n\n#### ② Growth Function for Positive Interval\n> Positive Interval 是用一个一维“线段”作用于一维坐标上，与在线段里面的值为+1，外面的为-1\n如图八所示，Positive Interval 的成长函数为 $M_H(N) = C_{N+1}^2 + 1 = \\frac{1}{2}N^2 + \\frac{1}{2}N + 1$，当 $N \\geq 3$的时候，$M_H(N) < 2^N = O(N^2)$\n\n![Growth Function for Positive Interval](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-8%20Growth%20Function%20for%20Positive%20Intervals.png)\n<center> 图八 Growth Function for Positive Interval <sup>[2]</sup></center>\n\n\n\n#### ③ Growth Function for Convex Sets\n> Convex Sets 不太好理解。可以理解成在二维坐标上，用凸多边形去把所需要的点串起来。在多边形顶点上的点的值为+1，不在的为-1。因为我们讨论的是最大的可能性，所以当我们把所有的点都放在一个圆上的时候，必定存在一个凸多边形可以连接任意多个点（即可以画出任意多边形），然后再把所有点的组合情况加起来\n如图九所示，Convex Setsl 的成长函数为 $M_H(N) = \\sum\\limits_{i=0}^{N}C_{i}^{N} = 2^N$\n\n![Growth Function for Convex Sets](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-9%20Growth%20Function%20for%20Convex%20Sets.png)\n<center> 图九 Growth Function for Convex Sets <sup>[2]</sup></center>\n\n\n\n### 4) Break Point of Growth function\n我们称能满足完全二分类(出现不同种类的数量为$2^N$)的情况为shattered,能shattered的最大的点为突破点(break point)。\n然后根据上面对N从1-5的尝试，得到的最大可能性如下表\n\nN|$effctive(N)$\n-|-|\n1|$2 = 2^1$|\n2|$4 = 2^2$|\n3|$8 = 2^3$|\n4|$14 < 2^5$|\n5|$32 << 2^6$|\n...|...|\nN|$effctive(N) << 2^N$|\n\n可以推断出公式（3）\n$$\neffctive(N): M_H(N) \\leq \\max( possible \\quad M_H(N) \\quad Given \\quad break- point \\quad(K)) \\leq 2^N\n\\tag{$3$}\n$$\n\n\n上面关于Growth Function讨论的几种情况的Break Point 如图十所示，我们可以看出成长函数的复杂度与Break Point的大小存在一定的关系。\n\n![Break Point of Growth function](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/033cce3063c6d35c7ac55af137821cf97819be44/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-10%20Break%20Point.png)\n<center> 图十 Break Point of Growth function <sup>[3]</sup></center>\n\n\n### 5) Bounding Function\n#### ① Introduction of Bounding Function\n根据上一节的Break Point $K$和样本点 $N$的关系，我们引入一个新概念，上限函数(Bounding Function) $B(N,K)$。这个函数表示有$N$个样本点且成长函数的突破点是$K$的时候，最多有多少种组合情况，比如说$B(3,2) = 3$（这个比较容易想象，这里就不展开讨论了）。并且这个上限函数满足公式（4），因为这是采用而分类的方法来进行的，最大值为$2^N$。\n$$\nB(N,K) \\leq 2^N\n\\tag{$3$}\n$$\n\n但是显然在上面的例子中，我们可以看到$B(N,K) < 2^N \\quad(N \\geq K)$，所以我们下面进一步确定这个上限函数的最大值。\n\n#### ② Proof of Bounding Function\n\n我们下面用表格的方式来表示$B(N,K)$ 表格如下表，我们下面将会填满这个表格来找出相应的规律\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      |                        |                        |                        |                        |                        |                        | ... |\nN=2      |                        |                        |                        |                        |                        |                        | ... |\nN=3      |                        |                        |                        |                        |                        |                        | ... |\nN=4      |                        |                        |                        |                        |                        |                        | ... |\nN=5      |                        |                        |                        |                        |                        |                        | ... |\nN=6      |                        |                        |                        |                        |                        |                        | ... |\n...      |                        |                        |                        |                        |                        |                        | ... |\n\n1.根据上面的规律，我们知道在$N<K$的时候，$B(N,K) = 2^N$，所以表格更新如下\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      |                        | <font color=#FF0000>2</font>  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      |                        |                        | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      |                        |                        |                        | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      |                        |                        |                        |                        | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      |                        |                        |                        |                        |                        | <font color=#FF0000>32 | ... |\nN=6      |                        |                        |                        |                        |                        |                        | ... |\n...      |                        |                        |                        |                        |                        |                        | ... |\n\n2.然后当$K=1$的时候，我们至少有一种分法（全正或者全负），所以第一列全部为1，表格更新如下。\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      | <font color=#0000FF>1  | <font color=#FF0000>2  | <font color=#FF0120>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      | <font color=#0000FF>1  |                        | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      | <font color=#0000FF>1  |                        |                        | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      | <font color=#0000FF>1  |                        |                        |                        | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      | <font color=#0000FF>1  |                        |                        |                        |                        | <font color=#FF0000>32 | ... |\nN=6      | <font color=#0000FF>1  |                        |                        |                        |                        |                        | ... |\n...      | <font color=#0000FF>1  |                        |                        |                        |                        |                        | ... |\n\n3.接着，当$N=K$的时候，我们上面也可以看到，所有的值最大都等于$2^N-1$(因为不能所有情况都出现一次)，所以表格更新如下。\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      | <font color=#0000FF>1  | <font color=#FF0000>2  | <font color=#FF0120>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      | <font color=#0000FF>1  | <font color=#FF0000>3  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      | <font color=#0000FF>1  |                        | <font color=#FF0000>7  | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      | <font color=#0000FF>1  |                        |                        | <font color=#FF0000>15 | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      | <font color=#0000FF>1  |                        |                        |                        | <font color=#FF0000>31 | <font color=#FF0000>32 | ... |\nN=6      | <font color=#0000FF>1  |                        |                        |                        |                        | <font color=#FF0000>63 | ... |\n...      | <font color=#0000FF>1  |                        |                        |                        |                        |                        | ... |\n\n4.在之前的章节，我们也数过$B(3,2) = 4$，其实看到这里我们已经大概有一些规律了：下面一项为上面两项之和， $B(N,K) = B(N-1, K) + B(N-1, K-1)$。当然我们只是猜测，下面我们继续证明。表格更新如下：\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      | <font color=#0000FF>1  | <font color=#FF0000>2  | <font color=#FF0120>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      | <font color=#0000FF>1  | <font color=#FF0000>3  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      | <font color=#0000FF>1  | <font color=#F0FFF0)>4 | <font color=#FF0000>7  | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      | <font color=#0000FF>1  |                        |                        | <font color=#FF0000>15 | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      | <font color=#0000FF>1  |                        |                        |                        | <font color=#FF0000>31 | <font color=#FF0000>32 | ... |\nN=6      | <font color=#0000FF>1  |                        |                        |                        |                        | <font color=#FF0000>63 | ... |\n...      | <font color=#0000FF>1  |                        |                        |                        |                        |                        | ... |\n\n\n5.我们证明上面的猜想$B(N,K) = B(N-1, K) + B(N-1, K-1)$\n1）首先我们遍历B(4,3)，可以得到图十一的结果，然后我们整理了一下结果的顺序，可以发现橙色区域 {$x_1,x_2,x_3$}结果分别出现了2次，而紫色区域的{$x_1,x_2,x_3$}结果只出现了1次。\n\n![Reorganized Dichotomies of B(4,3) - 1][11]\n\n[11]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\n\n<center> 图十一 Reorganized Dichotomies of B(4,3) - 1 <sup>[4]</sup></center>\n\n\n2）所以我们单独把{$x_1,x_2,x_3$}，提出来看，并把橙色区域的个数设为 $\\alpha$，紫色区域的个数为 $\\beta$，那么原来4个点的情况 $B(4,3) = 2 \\alpha + \\beta$，而3个点的情况 $B(3,3) = \\alpha + \\beta$，如下图十二所示。\n\n![Reorganized Dichotomies of B(4,3) - 2][12]\n\n[12]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%202.png\n\n<center> 图十二 Reorganized Dichotomies of B(4,3) - 2 <sup>[4]</sup></center>\n\n3)接着我们单独看 $\\alpha$可以发现这个刚好是 $B(3,2)$ 的最大可能性，也就是说$ \\alpha \\leq B(3,2) = 4 $，如图十三所示。\n\n![Reorganized Dichotomies of B(4,3) - 3][13]\n\n[13]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%203.png\n\n<center> 图十三 Reorganized Dichotomies of B(4,3) - 3 <sup>[4]</sup></center>\n\n\n4） 根据上面的分析，我们目前得到三个公式，如下面的公式（4）（5）（6）。\n$$\nB(4,5) = 2 \\cdot \\alpha + \\beta\n\\tag{$4$}\n$$\n$$\n\\alpha + \\beta \\leq B(3,3)\n\\tag{$5$}\n$$\n$$\n\\alpha \\leq B(3,2)\n\\tag{$6$}\n$$\n\n所以把公式(5)(6)加起来，我们可以更新公式（4）为公式（7）\n$$\nB(4,5) \\leq 2 \\cdot \\alpha + \\beta\n\\tag{$7$}\n$$\n\n5）最后我们用同样的方法来研究$B(N,K)$可以很容易证明到公式（8）（9）（10）\n$$\nB(N - 1,K) \\leq \\sum\\limits_{i=0}^{k-1} C_{N-1}^{i}\n\\tag{$8$}\n$$\n$$\nB(N - 1,K - 1) \\leq \\sum\\limits_{i=0}^{k-2} C_{N-1}^{i} = \\sum\\limits_{i=1}^{k-1} C_{N-1}^{i}\n\\tag{$9$}\n$$\n$$\n\\begin{align}\nB(N,K)  & \\leq {B(N-1,K) + B(N-1,K-1)} \\\\\n        & \\leq \\sum\\limits_{i=0}^{k-1} C_{N-1}^{i} + \\sum\\limits_{i=1}^{k-1} C_{N-1}^{i} \\\\\n        & = C_{N-1}^0 + \\sum\\limits_{i=1}^{k-1} \\left( C_{N-1}^{i} + C_{N-1}^{i}\\right)  \\\\\n        & = 1 + \\sum\\limits_{i=1}^{k-1} C_{N}^{i}  \\\\\n        & = C_{N}^0 + \\sum\\limits_{i=1}^{k-1} C_{N}^{i}  \\\\\n        & = \\sum\\limits_{i=0}^{k-1} C_{N}^{i}\n\\end{align}\n\\tag{$10$}\n$$\n\n所以我们的表格更新如下：\n\n$B(N,K)$ |          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      | <font color=#0000FF>1  | <font color=#FF0000>2  | <font color=#FF0120>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | <font color=#FF0000>2  | ... |\nN=2      | <font color=#0000FF>1  | <font color=#FF0000>3  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | <font color=#FF0000>4  | ... |\nN=3      | <font color=#0000FF>1  | <font color=#F0FFF0)>4 | <font color=#FF0000>7  | <font color=#FF0000>8  | <font color=#FF0000>8  | <font color=#FF0000>8  | ... |\nN=4      | <font color=#0000FF>1  | <font color=#FFA500)>$\\leq5$  | <font color=#FFA500>11  | <font color=#FF0000>15 | <font color=#FF0000>16 | <font color=#FF0000>16 | ... |\nN=5      | <font color=#0000FF>1  | <font color=#FFA500)>$\\leq6$  | <font color=#FFA500)>$\\leq16$  | <font color=#FFA500)>$\\leq15$  | <font color=#FF0000>31 | <font color=#FF0000>32 | ... |\nN=6      | <font color=#0000FF>1  | <font color=#FFA500)>$\\leq7$  | <font color=#FFA500)>$\\leq22$  | <font color=#FFA500)>$\\leq26$  | <font color=#FFA500)>$\\leq57$  | <font color=#FF0000>63 | ... |\n...      | <font color=#0000FF>1  |              ...              |              ...              |              ...              |              ...              |              ...              | ... |\n\n我们再把$N^{K-1}$的表格整理如下：\n\n$N^{N-1}$|          K=1           |           K=2          |           K=3          |           K=4          |           K=5          |           K=6          | ... |\n:-------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------------:|:---:|\nN=1      |           1            |           1            |           1            |           1            |           1            |           1            | ... |\nN=2      |           1            |           2            |           4            |           8            |           8            |           16           | ... |\nN=3      |           1            |           3            |           9            |           27           |           27           |           81           | ... |\nN=4      |           1            |           4            |           16           |           64           |           64           |           256          | ... |\nN=5      |           1            |           5            |           25           |           125          |           125          |           625          | ... |\nN=6      |           1            |           6            |           36           |           216          |           216          |           1296         | ... |\n...      |          ...           |          ...           |          ...           |          ...           |          ...           |          ...           | ... |\n\n对比图参考老师上课的PPT，如图十四所示。\n\n![Comparision of B(N,K) and N^(K-1)][14]\n\n[14]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-14%20Comparision%20of%20B(N%2CK)%20and%20N%5E(K-1).png\n\n<center> 图十四 Comparision of B(N,K) and N^(K-1) <sup>[6]</sup></center>\n\n\n总结起来就是：在$N \\geq 2, K \\geq 3$的时候，总有公式（11）的情况。\n$$\nM_H(N) \\leq B(N,K) = \\sum\\limits_{i=0}^{K-1} \\leq N^{K-1}\n\\tag{$11$}\n$$\n\n\n### 6) Vapnik-Chervonenkis (VC) bound\n\n@TODO: 这一节主要是证明从数学的角度上证明VC Bound 并以此更新Hoeffding Inequity。目前听得失一知半解，所以只贴出结论，后面再补充。\n\n\n结论如图十五所示。\n\n![Vapnik-Chervonenkis (VC) bound](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/738603b7f8e8f10505a791140c32f4677d4e7d84/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-13%20VC%20bound.png)\n\n<center> 图十五 Vapnik-Chervonenkis (VC) bound <sup>[5]</sup></center>\n\n这个VC Bound的作用是把之前Hoeffding的参数 $M$替换成这里引入的成长函数 $M_H(N)$，并构建出成长函数与样本数量（N）的关系这样的话，我们就可以容易的得到结论：在样本N足够大时候，发生Bad Data的概率小于 $epsilon$ ($E_{in} \\approx E_{out}$)，可以得出错误率也低($E_{in} \\approx 0$)，说明机器学习是可能的。\n\n也就是说要说机器可以学习必须满足下面的条件：\n1. 假设空间的成长函数 $M_H(N)$ 存在Break Point K （即有一个好的假设空间$H$)\n2. 输入数据的样本 $N$ 足够大（有一个好的数据集 $D$）\n3. 存在一个算法，能够找出能在假设空间 $H$ 中找到一个值使得错误率 $E_{in}$ 足够小 （有一个好的算法 $A$ -->也就是我们后面会研究的重点）\n\n其中：条件1和2通过VC Bound保证了 $E_{in} \\approx E_{out}$，条件3保证了 $E_{in} \\approx 0$ $\\Longrightarrow$ Machine Can Learn.\n\n\n\n------------------------------------\n<br><br>\n\n\n## 3. The VC Dimension\n### 1) Definition of VC Dimension\nVC Dimension( $d_{vc}$ )指的是能够使得成长函数可以被shatter的最大值（即 Break Point - 1)，用符号表示为公式（12）。\n$$\nd_{vc} = min(K) -1\n\\tag{$12$}\n$$\n\n上面的Hoeffding Inequity可以变成公式（13）\n$$\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 4 \\cdot (2N)^{d_{vc}} \\cdot \\exp \\left( -\\frac{1}{8} \\epsilon^2 N \\right)\n\\tag{$13$}\n$$\n\n因此，根据这个特点，我们只要确保一个成长函数存在 VC Dimension，我们就可以确定他存在Break Point，是一个好的假设空间。\n\n### 2) Generalization Error\n我们引入泛化误差 $\\delta$ 表示 $E_{in}(g) 和 E_{out}(g)$ 的接近程度，即 $\\delta = E_{in}(g) - E_{out}(g)$ ，根据公式（13），我们稍作化简，如公式（14）。\n$$\n\\begin{align}\n\\delta                           &= 4 \\cdot (2N)^{d_{vc}} \\cdot \\exp \\left( -\\frac{1}{8} \\epsilon^2 N \\right) \\\\\n\\frac{4 (2N)^{d_{vc}}}{\\delta}   &= \\exp(\\frac{1}{8} \\epsilon^2 N)  \\\\\n\\epsilon                         &= \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}\n\\end{align}\n\\tag{$14$}\n$$\n\n也就是说 $E_{in}(g) - E_{out}(g)$ 的误差会小于等于 $\\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}$。 所以我们可以求得 $E_{out}$ 的范围如公式（15）\n$$\nE_{in}(g) -  \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )} \\leq E_{out}(g) \\leq E_{in}(g) + \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )})\n\\tag{$15$}\n$$\n\n### 3) Model Complexity\n上一节，我们求出了 $E_{in}(g) - E_{out}(g)$ 的误差，为了方便引用，我们引入了新的概念：模型复杂度（Model Complexitiy）来表示这个误差值，数学表示如公式（16）\n$$\n\\Omega(N,H,\\delta) =  \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}\n\\tag{$15$}\n$$\n可以看出，随着VC Dimension的增大，$\\Omega$也会变大，然后 $E_{in}(g)$ 也会随着VC Dimension的增大而变小（因为选择的假设空间大了），但是 $E_{out}(g)$ 却不是一个单调函数，因为公式（15），然后这2个值一个变大一个变小，但是最终的话，$E_{out}$ 的曲线是先下降，然后上升（遍历一边就可以得到结果了），所以找到 $d_{vc}^{*}$ 很重要。(因为 $E_{out}$ 才是我们机器学习最重要的指标）\n结果如图十四所示。\n![Error and VC dimension](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bab59f769a00e4a303287ff809ff1df7a2916f55/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-15%20Error%20and%20VC%20dimension.png)\n<center> 图十六 Error and VC dimension <sup>[6]</sup></center>\n\n\n### 4) How much Data We need Theoretically and Practically\n问题：假如现在老板给员工下达了一个任务，要求这个模型的 ${\\epsilon = 0.1，\\delta = 0.1，  d_{vc} = 3}$ ，那样的话，我们需要多少个样本 $N$ 才能满足要求呢？\n回答：根据上面的公式（14），分别代入参数到等式中，可以求得样本数量 $N$ 如图十五的橙色区域所示。但是实际上，我们只需要 $10d_{vc}$就足够了。\n\n![How much Data We need Theoretically and Practically](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bab59f769a00e4a303287ff809ff1df7a2916f55/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-15%20Error%20and%20VC%20dimension.png)\n<center> 图十七 How much Data We need Theoretically and Practically <sup>[6]</sup></center>\n<br>\n\n因为我们在计算的时候同样的把上限给放大了，放大的原因如下所示:\n1. Hoeffding Inequity 不需要知道未知的情况，但是VC Bound可以用于各种分布，各种目标函数(因为 VC Bound的推导是基于不同的N和K)；\n2. 在给Binary Classification 强行装上成长函数本身就是一个宽松的上界，但是VC Bound可以用于各种数据样本；\n3. 使用二项式 $N^{d_{vc}}$ 作为成长函数的上界使得约束更加宽松，但是VC Bound可以用于任意具有相同VC维的假设空间；\n4. 联合限制（union bound）并不是一定会选择出现不好事情的假设函数，但是VC Bound可以用于任意算法。\n\n\n---------------------------------------------\n<br>\n<br>\n\n# Summary\n1. 我们首先通过回顾上一节的内容，得到结论是根据Hoeffding Inquity: 要使得机器可以学习的条件是 ① $E_{in} \\approx E_{out}$ ② $E_{in} \\approx 0$\n2. 接着我们讨论了什么情况下才能保证这2个条件满足，进行了讨论，最终我们通过引入① Growth Function ② Break Point ③ VC Bound, VC Dimension 更改Hoeffding Inequity的上限，最终得到我们需要的答案：\n    - 假设空间的成长函数 $M_H(N)$ 存在Break Point $K$ （即有一个好的假设空间 $H$)\n    - 输入数据的样本 $N$ 足够大（有一个好的数据集 $D$）\n    - 存在一个算法，能够找出能在假设空间 $H$ 中找到一个值使得错误率 $E_{in}$ 足够小 （有一个好的算法 $A$ -->也就是我们后面会研究的重点）\n    - 其中：条件1和2通过VC Bound保证了 $E_{in} \\approx E_{out}$，条件3保证了 $E_{in} \\approx 0$ ⟹ Machine Can Learn.\n3. 之后我们讨论了理论上 $N \\approx 10000 d_{vc}$ 而实际上只需要 $N \\approx 10 d_{vc}$ 的能使得 $E_{out}$ 最小，并且分析了为什么理论上和实际上差别这么大\n\n\n---------------------------------------------\n<br>\n<br>\n\n\n\n# Reference\n[1]机器学习基石(台湾大学-林轩田)\\5\\5 - 2 - Effective Number of Lines (15-26)\n\n[2]机器学习基石(台湾大学-林轩田)\\5\\5 - 3 - Effective Number of Hypotheses (16-17)\n\n[3]机器学习基石(台湾大学-林轩田)\\5\\5 - 4 - Break Point (07-44)\n\n[4]机器学习基石(台湾大学-林轩田)\\6\\6 - 3 - Bounding Function- Inductive Cases (14-47)\n\n[5]机器学习基石(台湾大学-林轩田)\\6\\6 - 4 - A Pictorial Proof (16-01)\n\n[6]机器学习基石(台湾大学-林轩田)\\7\\7 - 4 - Interpreting VC Dimension (17-13)\n\n<br>\n<br>\n---------------------------------------------\n","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-08-5.Why Can Machine Learn","published":1,"updated":"2018-04-14T19:42:06.504Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2e7001brwtj3kbs3k5c","content":"<h1 id=\"Why-can-Machine-Learn\"><a href=\"#Why-can-Machine-Learn\" class=\"headerlink\" title=\"Why can Machine Learn?\"></a>Why can Machine Learn?</h1><blockquote>\n<p>这一节我的思路是把老师的第五，六、七节的内容结合起来了，并且思路不完全按照老师的授课来走。</p>\n</blockquote>\n<hr>\n<h2 id=\"1-Preview-of-Last-Chapter\"><a href=\"#1-Preview-of-Last-Chapter\" class=\"headerlink\" title=\"1. Preview of Last Chapter\"></a>1. Preview of Last Chapter</h2><blockquote>\n<p>因为这一章的讨论是基于上一章最后一节得到的公式的，所以我们先Recap一下。</p>\n</blockquote>\n<p>上一节中，我们最后得出公式（1）（2）</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[  BAD \\quad D \\right] \\leq 2M \\cdot \\exp \\left( -2 \\epsilon^2N  \\right) \\\\\n\\tag{$1$}</script><script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 2M \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$2$}</script><blockquote>\n<p>这里的M是一个有限的数，所以当训练样本 $N$ 越大，那么Bad Data出现的概率越低，$E_{in} \\approx E_{out}$；如果训练样本 $N$一定的情况下，M越大，也就是说Hypothesis越多，那样可以供我们用算法 $A$进行选择的越多，那么越有可能选到一个好的样本，使得 $E_{in} \\approx 0$</p>\n</blockquote>\n<p>总结如下表：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>-</th>\n<th style=\"text-align:center\">M很小的时候</th>\n<th style=\"text-align:center\">M很大的时候</th>\n<th style=\"text-align:center\">N很小的时候</th>\n<th style=\"text-align:center\">N很大的时候</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$E_{in}(g) \\approx E_{out}(g)$</td>\n<td style=\"text-align:center\">Yes，Bad Data的数量也少了</td>\n<td style=\"text-align:center\">No，Bad Data的数量也多了</td>\n<td style=\"text-align:center\">Yes，Bad Data出现的概率变小了</td>\n<td style=\"text-align:center\">No，Bad Data出现的概率变大了</td>\n</tr>\n<tr>\n<td>$E_{in}(g) \\approx 0$</td>\n<td style=\"text-align:center\">No，选到低错误率的可能性变小了</td>\n<td style=\"text-align:center\">Yes，选到低错误率的可能性变大了</td>\n<td style=\"text-align:center\">没必然联系，样本总数多于少，与错误率无关</td>\n<td style=\"text-align:center\">没必然联系，样本总数多于少，与错误率无关</td>\n</tr>\n</tbody>\n</table>\n</div>\n<blockquote>\n<p>从表格中可以看出，$M$ 太大太小都会对机器学习的有效性造成影响，所以我们要进一步缩小$M$ 的取值范围。</p>\n<p>问题：怎么缩小$M$的取值范围</p>\n<p>解决方案：在上一节中，我们再推导的过程中使用了联合上限（Union Bound)，造成实际的上限被放大了很多。因为在做集合的或运算的时候，我们单纯的把各个集合加起来，但是却没有减去他们的交集部分，所以造成了上限被放大的问题。</p>\n</blockquote>\n<p>关于Union Bound 的推导可以看这个链接<a href=\"https://en.wikipedia.org/wiki/Boole%27s_inequality\" title=\"Boole&#39;s inequality\" target=\"_blank\" rel=\"external\">Boole’s inequality</a></p>\n<p>总的来说就是因为我们推导公式（1）的时候使用了Union Bound，所以导致了不等式右边的值（上限）被放大了，所以现在我们可以把它进行缩减，求出有效的 $M$值(即 $M_H(N)$)，下面我们来推导这个有效值。</p>\n<h2 id=\"2-VC-Bound-A-Upper-Bound-of-Hoeffding-Inequity\"><a href=\"#2-VC-Bound-A-Upper-Bound-of-Hoeffding-Inequity\" class=\"headerlink\" title=\"2. VC Bound - A Upper Bound of Hoeffding Inequity\"></a>2. VC Bound - A Upper Bound of Hoeffding Inequity</h2><h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1) Introduction\"></a>1) Introduction</h3><p>场景：对于不同数量$N$的训练数据，有多少种不同的方法 $effective(N)$ 可以区分他们？</p>\n<p>当 $N=1$ 的时候，如图一所示，共有2种方法，$effective(N) = 2 = 2^1$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-1%20number%3D1.png\" alt=\"N=1\"></p>\n<center> 图一 N=1 <sup>[1]</sup></center>\n\n\n<p>当 $N=2$ 的时候，如图二所示，共有4种方法，$effective(N) = 4 = 2^2$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-2%20number%3D2.png\" alt=\"N=2\"></p>\n<center> 图二 N=2 <sup>[1]</sup></center>\n\n\n<p>当 $N=3$ 的时候，如图三、四所示，最多有8种方法，虽然说在特定的情形下，可能只有6中种方法，$effective(N) = 8 = 2^3$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-4%20number%3D3%20with%20error.png\" alt=\"N=3 with error\"></p>\n<center> 图三 N=3 with error <sup>[2]</sup></center>\n\n\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-3%20number%3D3.png\" alt=\"N=3\"></p>\n<center> 图四 N=3 <sup>[1]</sup></center>\n\n\n\n<p>当 $N=4$ 的时候，如图五所示，无论怎么放着4个点，最多只有14种方法，$effective(N) = 14 &lt; 2^4$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-5%20number%3D4.png\" alt=\"N=4\"></p>\n<center> 图五 N=4 <sup>[1]</sup></center>\n\n\n<p>当 $N=5$ 的时候，很显然 $effective(N) = 32 &lt;  2^5$，就不再继续讨论了。</p>\n<p>总结如下表：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>N</th>\n<th>$effctive(N)$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>$2 = 2^1$</td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td>$4 = 2^2$</td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td>$8 = 2^3$</td>\n<td></td>\n</tr>\n<tr>\n<td>4</td>\n<td>$14 &lt; 2^5$</td>\n<td></td>\n</tr>\n<tr>\n<td>5</td>\n<td>$32 &lt;&lt; 2^6$</td>\n<td></td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n<td></td>\n</tr>\n<tr>\n<td>N</td>\n<td>$effctive(N) &lt;&lt; 2^N$</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>总结如图六，可以看出当 $N&gt;4$的时候，$effective(N) &lt; 2^N$，也就是说我们把 $M和N$ 的关系构建了起来，所以我们可以用 $effective(N)$ 去替换 $M$，得到公式(3)</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 2 \\cdot effective(N) \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$3$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-6%20summary.png\" alt=\"Summary\"></p>\n<center> 图六 Summary <sup>[1]</sup></center>\n\n\n\n<h3 id=\"2-Growth-function\"><a href=\"#2-Growth-function\" class=\"headerlink\" title=\"2) Growth function\"></a>2) Growth function</h3><p>上面Binary Clasification的分类方法叫做二分类法（dichotomy)，为了更好地表示 $N和effective(N)$ 的关系，我们引入成长函数（Growth Function) $M_H(N)$ 来表示，具体的数学表达如公式（4）所示。</p>\n<script type=\"math/tex; mode=display\">\nM_H(N) = \\max\\limits_{x_1,x_2,...,x_N ∈ X}  \\lvert H(x_1,x_2,...,x_N) \\rvert\n\\tag{$4$}  \\quad(其中，上限为2^N)</script><h3 id=\"3-Different-Types-of-Growth-function\"><a href=\"#3-Different-Types-of-Growth-function\" class=\"headerlink\" title=\"3) Different Types of Growth function\"></a>3) Different Types of Growth function</h3><h4 id=\"①-Growth-Function-for-Positive-Rays\"><a href=\"#①-Growth-Function-for-Positive-Rays\" class=\"headerlink\" title=\"① Growth Function for Positive Rays\"></a>① Growth Function for Positive Rays</h4><blockquote>\n<p>Positive Rays 是用一个一维向量作用于一维坐标上，与该向量同方向的值为+1，反方向为-1<br>如图七所示，Postives Rays 的成长函数为 $M_H(N) = (N-1)$，当 $N \\geq 2$的时候，$M_H(N) &lt; 2^N = O(N)$</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-7%20Growth%20Function%20for%20Positive%20Rays.png\" alt=\"Growth Function for Positive Rays\"></p>\n<center> 图七 Growth Function for Positive Rays <sup>[2]</sup></center>\n\n\n\n<h4 id=\"②-Growth-Function-for-Positive-Interval\"><a href=\"#②-Growth-Function-for-Positive-Interval\" class=\"headerlink\" title=\"② Growth Function for Positive Interval\"></a>② Growth Function for Positive Interval</h4><blockquote>\n<p>Positive Interval 是用一个一维“线段”作用于一维坐标上，与在线段里面的值为+1，外面的为-1<br>如图八所示，Positive Interval 的成长函数为 $M_H(N) = C_{N+1}^2 + 1 = \\frac{1}{2}N^2 + \\frac{1}{2}N + 1$，当 $N \\geq 3$的时候，$M_H(N) &lt; 2^N = O(N^2)$</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-8%20Growth%20Function%20for%20Positive%20Intervals.png\" alt=\"Growth Function for Positive Interval\"></p>\n<center> 图八 Growth Function for Positive Interval <sup>[2]</sup></center>\n\n\n\n<h4 id=\"③-Growth-Function-for-Convex-Sets\"><a href=\"#③-Growth-Function-for-Convex-Sets\" class=\"headerlink\" title=\"③ Growth Function for Convex Sets\"></a>③ Growth Function for Convex Sets</h4><blockquote>\n<p>Convex Sets 不太好理解。可以理解成在二维坐标上，用凸多边形去把所需要的点串起来。在多边形顶点上的点的值为+1，不在的为-1。因为我们讨论的是最大的可能性，所以当我们把所有的点都放在一个圆上的时候，必定存在一个凸多边形可以连接任意多个点（即可以画出任意多边形），然后再把所有点的组合情况加起来<br>如图九所示，Convex Setsl 的成长函数为 $M_H(N) = \\sum\\limits_{i=0}^{N}C_{i}^{N} = 2^N$</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-9%20Growth%20Function%20for%20Convex%20Sets.png\" alt=\"Growth Function for Convex Sets\"></p>\n<center> 图九 Growth Function for Convex Sets <sup>[2]</sup></center>\n\n\n\n<h3 id=\"4-Break-Point-of-Growth-function\"><a href=\"#4-Break-Point-of-Growth-function\" class=\"headerlink\" title=\"4) Break Point of Growth function\"></a>4) Break Point of Growth function</h3><p>我们称能满足完全二分类(出现不同种类的数量为$2^N$)的情况为shattered,能shattered的最大的点为突破点(break point)。<br>然后根据上面对N从1-5的尝试，得到的最大可能性如下表</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>N</th>\n<th>$effctive(N)$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>$2 = 2^1$</td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td>$4 = 2^2$</td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td>$8 = 2^3$</td>\n<td></td>\n</tr>\n<tr>\n<td>4</td>\n<td>$14 &lt; 2^5$</td>\n<td></td>\n</tr>\n<tr>\n<td>5</td>\n<td>$32 &lt;&lt; 2^6$</td>\n<td></td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n<td></td>\n</tr>\n<tr>\n<td>N</td>\n<td>$effctive(N) &lt;&lt; 2^N$</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>可以推断出公式（3）</p>\n<script type=\"math/tex; mode=display\">\neffctive(N): M_H(N) \\leq \\max( possible \\quad M_H(N) \\quad Given \\quad break- point \\quad(K)) \\leq 2^N\n\\tag{$3$}</script><p>上面关于Growth Function讨论的几种情况的Break Point 如图十所示，我们可以看出成长函数的复杂度与Break Point的大小存在一定的关系。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/033cce3063c6d35c7ac55af137821cf97819be44/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-10%20Break%20Point.png\" alt=\"Break Point of Growth function\"></p>\n<center> 图十 Break Point of Growth function <sup>[3]</sup></center>\n\n\n<h3 id=\"5-Bounding-Function\"><a href=\"#5-Bounding-Function\" class=\"headerlink\" title=\"5) Bounding Function\"></a>5) Bounding Function</h3><h4 id=\"①-Introduction-of-Bounding-Function\"><a href=\"#①-Introduction-of-Bounding-Function\" class=\"headerlink\" title=\"① Introduction of Bounding Function\"></a>① Introduction of Bounding Function</h4><p>根据上一节的Break Point $K$和样本点 $N$的关系，我们引入一个新概念，上限函数(Bounding Function) $B(N,K)$。这个函数表示有$N$个样本点且成长函数的突破点是$K$的时候，最多有多少种组合情况，比如说$B(3,2) = 3$（这个比较容易想象，这里就不展开讨论了）。并且这个上限函数满足公式（4），因为这是采用而分类的方法来进行的，最大值为$2^N$。</p>\n<script type=\"math/tex; mode=display\">\nB(N,K) \\leq 2^N\n\\tag{$3$}</script><p>但是显然在上面的例子中，我们可以看到$B(N,K) &lt; 2^N \\quad(N \\geq K)$，所以我们下面进一步确定这个上限函数的最大值。</p>\n<h4 id=\"②-Proof-of-Bounding-Function\"><a href=\"#②-Proof-of-Bounding-Function\" class=\"headerlink\" title=\"② Proof of Bounding Function\"></a>② Proof of Bounding Function</h4><p>我们下面用表格的方式来表示$B(N,K)$ 表格如下表，我们下面将会填满这个表格来找出相应的规律</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>1.根据上面的规律，我们知道在$N&lt;K$的时候，$B(N,K) = 2^N$，所以表格更新如下</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>2.然后当$K=1$的时候，我们至少有一种分法（全正或者全负），所以第一列全部为1，表格更新如下。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0120\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>3.接着，当$N=K$的时候，我们上面也可以看到，所有的值最大都等于$2^N-1$(因为不能所有情况都出现一次)，所以表格更新如下。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0120\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">3</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">7</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">15</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">31</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">63</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>4.在之前的章节，我们也数过$B(3,2) = 4$，其实看到这里我们已经大概有一些规律了：下面一项为上面两项之和， $B(N,K) = B(N-1, K) + B(N-1, K-1)$。当然我们只是猜测，下面我们继续证明。表格更新如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0120\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">3</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#F0FFF0)\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">7</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">15</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">31</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">63</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>5.我们证明上面的猜想$B(N,K) = B(N-1, K) + B(N-1, K-1)$<br>1）首先我们遍历B(4,3)，可以得到图十一的结果，然后我们整理了一下结果的顺序，可以发现橙色区域 {$x_1,x_2,x_3$}结果分别出现了2次，而紫色区域的{$x_1,x_2,x_3$}结果只出现了1次。</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\" alt=\"Reorganized Dichotomies of B(4,3) - 1\"></p>\n<center> 图十一 Reorganized Dichotomies of B(4,3) - 1 <sup>[4]</sup></center>\n\n\n<p>2）所以我们单独把{$x_1,x_2,x_3$}，提出来看，并把橙色区域的个数设为 $\\alpha$，紫色区域的个数为 $\\beta$，那么原来4个点的情况 $B(4,3) = 2 \\alpha + \\beta$，而3个点的情况 $B(3,3) = \\alpha + \\beta$，如下图十二所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%202.png\" alt=\"Reorganized Dichotomies of B(4,3) - 2\"></p>\n<center> 图十二 Reorganized Dichotomies of B(4,3) - 2 <sup>[4]</sup></center>\n\n<p>3)接着我们单独看 $\\alpha$可以发现这个刚好是 $B(3,2)$ 的最大可能性，也就是说$ \\alpha \\leq B(3,2) = 4 $，如图十三所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%203.png\" alt=\"Reorganized Dichotomies of B(4,3) - 3\"></p>\n<center> 图十三 Reorganized Dichotomies of B(4,3) - 3 <sup>[4]</sup></center>\n\n\n<p>4） 根据上面的分析，我们目前得到三个公式，如下面的公式（4）（5）（6）。</p>\n<script type=\"math/tex; mode=display\">\nB(4,5) = 2 \\cdot \\alpha + \\beta\n\\tag{$4$}</script><script type=\"math/tex; mode=display\">\n\\alpha + \\beta \\leq B(3,3)\n\\tag{$5$}</script><script type=\"math/tex; mode=display\">\n\\alpha \\leq B(3,2)\n\\tag{$6$}</script><p>所以把公式(5)(6)加起来，我们可以更新公式（4）为公式（7）</p>\n<script type=\"math/tex; mode=display\">\nB(4,5) \\leq 2 \\cdot \\alpha + \\beta\n\\tag{$7$}</script><p>5）最后我们用同样的方法来研究$B(N,K)$可以很容易证明到公式（8）（9）（10）</p>\n<script type=\"math/tex; mode=display\">\nB(N - 1,K) \\leq \\sum\\limits_{i=0}^{k-1} C_{N-1}^{i}\n\\tag{$8$}</script><script type=\"math/tex; mode=display\">\nB(N - 1,K - 1) \\leq \\sum\\limits_{i=0}^{k-2} C_{N-1}^{i} = \\sum\\limits_{i=1}^{k-1} C_{N-1}^{i}\n\\tag{$9$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\nB(N,K)  & \\leq {B(N-1,K) + B(N-1,K-1)} \\\\\n        & \\leq \\sum\\limits_{i=0}^{k-1} C_{N-1}^{i} + \\sum\\limits_{i=1}^{k-1} C_{N-1}^{i} \\\\\n        & = C_{N-1}^0 + \\sum\\limits_{i=1}^{k-1} \\left( C_{N-1}^{i} + C_{N-1}^{i}\\right)  \\\\\n        & = 1 + \\sum\\limits_{i=1}^{k-1} C_{N}^{i}  \\\\\n        & = C_{N}^0 + \\sum\\limits_{i=1}^{k-1} C_{N}^{i}  \\\\\n        & = \\sum\\limits_{i=0}^{k-1} C_{N}^{i}\n\\end{align}\n\\tag{$10$}</script><p>所以我们的表格更新如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0120\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">3</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#F0FFF0)\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">7</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq5$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500\">11</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">15</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq6$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq16$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq15$</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">31</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq7$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq22$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq26$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq57$</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">63</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>我们再把$N^{K-1}$的表格整理如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$N^{N-1}$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">9</td>\n<td style=\"text-align:center\">27</td>\n<td style=\"text-align:center\">27</td>\n<td style=\"text-align:center\">81</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">64</td>\n<td style=\"text-align:center\">64</td>\n<td style=\"text-align:center\">256</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">25</td>\n<td style=\"text-align:center\">125</td>\n<td style=\"text-align:center\">125</td>\n<td style=\"text-align:center\">625</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">36</td>\n<td style=\"text-align:center\">216</td>\n<td style=\"text-align:center\">216</td>\n<td style=\"text-align:center\">1296</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>对比图参考老师上课的PPT，如图十四所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-14%20Comparision%20of%20B(N%2CK)%20and%20N%5E(K-1).png\" alt=\"Comparision of B(N,K) and N^(K-1)\"></p>\n<center> 图十四 Comparision of B(N,K) and N^(K-1) <sup>[6]</sup></center>\n\n\n<p>总结起来就是：在$N \\geq 2, K \\geq 3$的时候，总有公式（11）的情况。</p>\n<script type=\"math/tex; mode=display\">\nM_H(N) \\leq B(N,K) = \\sum\\limits_{i=0}^{K-1} \\leq N^{K-1}\n\\tag{$11$}</script><h3 id=\"6-Vapnik-Chervonenkis-VC-bound\"><a href=\"#6-Vapnik-Chervonenkis-VC-bound\" class=\"headerlink\" title=\"6) Vapnik-Chervonenkis (VC) bound\"></a>6) Vapnik-Chervonenkis (VC) bound</h3><p>@TODO: 这一节主要是证明从数学的角度上证明VC Bound 并以此更新Hoeffding Inequity。目前听得失一知半解，所以只贴出结论，后面再补充。</p>\n<p>结论如图十五所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/738603b7f8e8f10505a791140c32f4677d4e7d84/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-13%20VC%20bound.png\" alt=\"Vapnik-Chervonenkis (VC) bound\"></p>\n<center> 图十五 Vapnik-Chervonenkis (VC) bound <sup>[5]</sup></center>\n\n<p>这个VC Bound的作用是把之前Hoeffding的参数 $M$替换成这里引入的成长函数 $M_H(N)$，并构建出成长函数与样本数量（N）的关系这样的话，我们就可以容易的得到结论：在样本N足够大时候，发生Bad Data的概率小于 $epsilon$ ($E_{in} \\approx E_{out}$)，可以得出错误率也低($E_{in} \\approx 0$)，说明机器学习是可能的。</p>\n<p>也就是说要说机器可以学习必须满足下面的条件：</p>\n<ol>\n<li>假设空间的成长函数 $M_H(N)$ 存在Break Point K （即有一个好的假设空间$H$)</li>\n<li>输入数据的样本 $N$ 足够大（有一个好的数据集 $D$）</li>\n<li>存在一个算法，能够找出能在假设空间 $H$ 中找到一个值使得错误率 $E_{in}$ 足够小 （有一个好的算法 $A$ —&gt;也就是我们后面会研究的重点）</li>\n</ol>\n<p>其中：条件1和2通过VC Bound保证了 $E_{in} \\approx E_{out}$，条件3保证了 $E_{in} \\approx 0$ $\\Longrightarrow$ Machine Can Learn.</p>\n<hr>\n<p><br><br></p>\n<h2 id=\"3-The-VC-Dimension\"><a href=\"#3-The-VC-Dimension\" class=\"headerlink\" title=\"3. The VC Dimension\"></a>3. The VC Dimension</h2><h3 id=\"1-Definition-of-VC-Dimension\"><a href=\"#1-Definition-of-VC-Dimension\" class=\"headerlink\" title=\"1) Definition of VC Dimension\"></a>1) Definition of VC Dimension</h3><p>VC Dimension( $d_{vc}$ )指的是能够使得成长函数可以被shatter的最大值（即 Break Point - 1)，用符号表示为公式（12）。</p>\n<script type=\"math/tex; mode=display\">\nd_{vc} = min(K) -1\n\\tag{$12$}</script><p>上面的Hoeffding Inequity可以变成公式（13）</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 4 \\cdot (2N)^{d_{vc}} \\cdot \\exp \\left( -\\frac{1}{8} \\epsilon^2 N \\right)\n\\tag{$13$}</script><p>因此，根据这个特点，我们只要确保一个成长函数存在 VC Dimension，我们就可以确定他存在Break Point，是一个好的假设空间。</p>\n<h3 id=\"2-Generalization-Error\"><a href=\"#2-Generalization-Error\" class=\"headerlink\" title=\"2) Generalization Error\"></a>2) Generalization Error</h3><p>我们引入泛化误差 $\\delta$ 表示 $E_{in}(g) 和 E_{out}(g)$ 的接近程度，即 $\\delta = E_{in}(g) - E_{out}(g)$ ，根据公式（13），我们稍作化简，如公式（14）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\delta                           &= 4 \\cdot (2N)^{d_{vc}} \\cdot \\exp \\left( -\\frac{1}{8} \\epsilon^2 N \\right) \\\\\n\\frac{4 (2N)^{d_{vc}}}{\\delta}   &= \\exp(\\frac{1}{8} \\epsilon^2 N)  \\\\\n\\epsilon                         &= \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}\n\\end{align}\n\\tag{$14$}</script><p>也就是说 $E_{in}(g) - E_{out}(g)$ 的误差会小于等于 $\\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}$。 所以我们可以求得 $E_{out}$ 的范围如公式（15）</p>\n<script type=\"math/tex; mode=display\">\nE_{in}(g) -  \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )} \\leq E_{out}(g) \\leq E_{in}(g) + \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )})\n\\tag{$15$}</script><h3 id=\"3-Model-Complexity\"><a href=\"#3-Model-Complexity\" class=\"headerlink\" title=\"3) Model Complexity\"></a>3) Model Complexity</h3><p>上一节，我们求出了 $E_{in}(g) - E_{out}(g)$ 的误差，为了方便引用，我们引入了新的概念：模型复杂度（Model Complexitiy）来表示这个误差值，数学表示如公式（16）</p>\n<script type=\"math/tex; mode=display\">\n\\Omega(N,H,\\delta) =  \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}\n\\tag{$15$}</script><p>可以看出，随着VC Dimension的增大，$\\Omega$也会变大，然后 $E_{in}(g)$ 也会随着VC Dimension的增大而变小（因为选择的假设空间大了），但是 $E_{out}(g)$ 却不是一个单调函数，因为公式（15），然后这2个值一个变大一个变小，但是最终的话，$E_{out}$ 的曲线是先下降，然后上升（遍历一边就可以得到结果了），所以找到 $d_{vc}^{*}$ 很重要。(因为 $E_{out}$ 才是我们机器学习最重要的指标）<br>结果如图十四所示。<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bab59f769a00e4a303287ff809ff1df7a2916f55/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-15%20Error%20and%20VC%20dimension.png\" alt=\"Error and VC dimension\"></p>\n<center> 图十六 Error and VC dimension <sup>[6]</sup></center>\n\n\n<h3 id=\"4-How-much-Data-We-need-Theoretically-and-Practically\"><a href=\"#4-How-much-Data-We-need-Theoretically-and-Practically\" class=\"headerlink\" title=\"4) How much Data We need Theoretically and Practically\"></a>4) How much Data We need Theoretically and Practically</h3><p>问题：假如现在老板给员工下达了一个任务，要求这个模型的 ${\\epsilon = 0.1，\\delta = 0.1，  d_{vc} = 3}$ ，那样的话，我们需要多少个样本 $N$ 才能满足要求呢？<br>回答：根据上面的公式（14），分别代入参数到等式中，可以求得样本数量 $N$ 如图十五的橙色区域所示。但是实际上，我们只需要 $10d_{vc}$就足够了。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bab59f769a00e4a303287ff809ff1df7a2916f55/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-15%20Error%20and%20VC%20dimension.png\" alt=\"How much Data We need Theoretically and Practically\"></p>\n<p><center> 图十七 How much Data We need Theoretically and Practically <sup>[6]</sup></center><br><br></p>\n<p>因为我们在计算的时候同样的把上限给放大了，放大的原因如下所示:</p>\n<ol>\n<li>Hoeffding Inequity 不需要知道未知的情况，但是VC Bound可以用于各种分布，各种目标函数(因为 VC Bound的推导是基于不同的N和K)；</li>\n<li>在给Binary Classification 强行装上成长函数本身就是一个宽松的上界，但是VC Bound可以用于各种数据样本；</li>\n<li>使用二项式 $N^{d_{vc}}$ 作为成长函数的上界使得约束更加宽松，但是VC Bound可以用于任意具有相同VC维的假设空间；</li>\n<li>联合限制（union bound）并不是一定会选择出现不好事情的假设函数，但是VC Bound可以用于任意算法。</li>\n</ol>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>我们首先通过回顾上一节的内容，得到结论是根据Hoeffding Inquity: 要使得机器可以学习的条件是 ① $E_{in} \\approx E_{out}$ ② $E_{in} \\approx 0$</li>\n<li>接着我们讨论了什么情况下才能保证这2个条件满足，进行了讨论，最终我们通过引入① Growth Function ② Break Point ③ VC Bound, VC Dimension 更改Hoeffding Inequity的上限，最终得到我们需要的答案：<ul>\n<li>假设空间的成长函数 $M_H(N)$ 存在Break Point $K$ （即有一个好的假设空间 $H$)</li>\n<li>输入数据的样本 $N$ 足够大（有一个好的数据集 $D$）</li>\n<li>存在一个算法，能够找出能在假设空间 $H$ 中找到一个值使得错误率 $E_{in}$ 足够小 （有一个好的算法 $A$ —&gt;也就是我们后面会研究的重点）</li>\n<li>其中：条件1和2通过VC Bound保证了 $E_{in} \\approx E_{out}$，条件3保证了 $E_{in} \\approx 0$ ⟹ Machine Can Learn.</li>\n</ul>\n</li>\n<li>之后我们讨论了理论上 $N \\approx 10000 d_{vc}$ 而实际上只需要 $N \\approx 10 d_{vc}$ 的能使得 $E_{out}$ 最小，并且分析了为什么理论上和实际上差别这么大</li>\n</ol>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1]机器学习基石(台湾大学-林轩田)\\5\\5 - 2 - Effective Number of Lines (15-26)</p>\n<p>[2]机器学习基石(台湾大学-林轩田)\\5\\5 - 3 - Effective Number of Hypotheses (16-17)</p>\n<p>[3]机器学习基石(台湾大学-林轩田)\\5\\5 - 4 - Break Point (07-44)</p>\n<p>[4]机器学习基石(台湾大学-林轩田)\\6\\6 - 3 - Bounding Function- Inductive Cases (14-47)</p>\n<p>[5]机器学习基石(台湾大学-林轩田)\\6\\6 - 4 - A Pictorial Proof (16-01)</p>\n<p>[6]机器学习基石(台湾大学-林轩田)\\7\\7 - 4 - Interpreting VC Dimension (17-13)</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Why-can-Machine-Learn\"><a href=\"#Why-can-Machine-Learn\" class=\"headerlink\" title=\"Why can Machine Learn?\"></a>Why can Machine Learn?</h1><blockquote>\n<p>这一节我的思路是把老师的第五，六、七节的内容结合起来了，并且思路不完全按照老师的授课来走。</p>\n</blockquote>\n<hr>\n<h2 id=\"1-Preview-of-Last-Chapter\"><a href=\"#1-Preview-of-Last-Chapter\" class=\"headerlink\" title=\"1. Preview of Last Chapter\"></a>1. Preview of Last Chapter</h2><blockquote>\n<p>因为这一章的讨论是基于上一章最后一节得到的公式的，所以我们先Recap一下。</p>\n</blockquote>\n<p>上一节中，我们最后得出公式（1）（2）</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[  BAD \\quad D \\right] \\leq 2M \\cdot \\exp \\left( -2 \\epsilon^2N  \\right) \\\\\n\\tag{$1$}</script><script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 2M \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$2$}</script><blockquote>\n<p>这里的M是一个有限的数，所以当训练样本 $N$ 越大，那么Bad Data出现的概率越低，$E_{in} \\approx E_{out}$；如果训练样本 $N$一定的情况下，M越大，也就是说Hypothesis越多，那样可以供我们用算法 $A$进行选择的越多，那么越有可能选到一个好的样本，使得 $E_{in} \\approx 0$</p>\n</blockquote>\n<p>总结如下表：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>-</th>\n<th style=\"text-align:center\">M很小的时候</th>\n<th style=\"text-align:center\">M很大的时候</th>\n<th style=\"text-align:center\">N很小的时候</th>\n<th style=\"text-align:center\">N很大的时候</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$E_{in}(g) \\approx E_{out}(g)$</td>\n<td style=\"text-align:center\">Yes，Bad Data的数量也少了</td>\n<td style=\"text-align:center\">No，Bad Data的数量也多了</td>\n<td style=\"text-align:center\">Yes，Bad Data出现的概率变小了</td>\n<td style=\"text-align:center\">No，Bad Data出现的概率变大了</td>\n</tr>\n<tr>\n<td>$E_{in}(g) \\approx 0$</td>\n<td style=\"text-align:center\">No，选到低错误率的可能性变小了</td>\n<td style=\"text-align:center\">Yes，选到低错误率的可能性变大了</td>\n<td style=\"text-align:center\">没必然联系，样本总数多于少，与错误率无关</td>\n<td style=\"text-align:center\">没必然联系，样本总数多于少，与错误率无关</td>\n</tr>\n</tbody>\n</table>\n</div>\n<blockquote>\n<p>从表格中可以看出，$M$ 太大太小都会对机器学习的有效性造成影响，所以我们要进一步缩小$M$ 的取值范围。</p>\n<p>问题：怎么缩小$M$的取值范围</p>\n<p>解决方案：在上一节中，我们再推导的过程中使用了联合上限（Union Bound)，造成实际的上限被放大了很多。因为在做集合的或运算的时候，我们单纯的把各个集合加起来，但是却没有减去他们的交集部分，所以造成了上限被放大的问题。</p>\n</blockquote>\n<p>关于Union Bound 的推导可以看这个链接<a href=\"https://en.wikipedia.org/wiki/Boole%27s_inequality\" title=\"Boole&#39;s inequality\" target=\"_blank\" rel=\"external\">Boole’s inequality</a></p>\n<p>总的来说就是因为我们推导公式（1）的时候使用了Union Bound，所以导致了不等式右边的值（上限）被放大了，所以现在我们可以把它进行缩减，求出有效的 $M$值(即 $M_H(N)$)，下面我们来推导这个有效值。</p>\n<h2 id=\"2-VC-Bound-A-Upper-Bound-of-Hoeffding-Inequity\"><a href=\"#2-VC-Bound-A-Upper-Bound-of-Hoeffding-Inequity\" class=\"headerlink\" title=\"2. VC Bound - A Upper Bound of Hoeffding Inequity\"></a>2. VC Bound - A Upper Bound of Hoeffding Inequity</h2><h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1) Introduction\"></a>1) Introduction</h3><p>场景：对于不同数量$N$的训练数据，有多少种不同的方法 $effective(N)$ 可以区分他们？</p>\n<p>当 $N=1$ 的时候，如图一所示，共有2种方法，$effective(N) = 2 = 2^1$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-1%20number%3D1.png\" alt=\"N=1\"></p>\n<center> 图一 N=1 <sup>[1]</sup></center>\n\n\n<p>当 $N=2$ 的时候，如图二所示，共有4种方法，$effective(N) = 4 = 2^2$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-2%20number%3D2.png\" alt=\"N=2\"></p>\n<center> 图二 N=2 <sup>[1]</sup></center>\n\n\n<p>当 $N=3$ 的时候，如图三、四所示，最多有8种方法，虽然说在特定的情形下，可能只有6中种方法，$effective(N) = 8 = 2^3$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-4%20number%3D3%20with%20error.png\" alt=\"N=3 with error\"></p>\n<center> 图三 N=3 with error <sup>[2]</sup></center>\n\n\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-3%20number%3D3.png\" alt=\"N=3\"></p>\n<center> 图四 N=3 <sup>[1]</sup></center>\n\n\n\n<p>当 $N=4$ 的时候，如图五所示，无论怎么放着4个点，最多只有14种方法，$effective(N) = 14 &lt; 2^4$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-5%20number%3D4.png\" alt=\"N=4\"></p>\n<center> 图五 N=4 <sup>[1]</sup></center>\n\n\n<p>当 $N=5$ 的时候，很显然 $effective(N) = 32 &lt;  2^5$，就不再继续讨论了。</p>\n<p>总结如下表：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>N</th>\n<th>$effctive(N)$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>$2 = 2^1$</td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td>$4 = 2^2$</td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td>$8 = 2^3$</td>\n<td></td>\n</tr>\n<tr>\n<td>4</td>\n<td>$14 &lt; 2^5$</td>\n<td></td>\n</tr>\n<tr>\n<td>5</td>\n<td>$32 &lt;&lt; 2^6$</td>\n<td></td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n<td></td>\n</tr>\n<tr>\n<td>N</td>\n<td>$effctive(N) &lt;&lt; 2^N$</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>总结如图六，可以看出当 $N&gt;4$的时候，$effective(N) &lt; 2^N$，也就是说我们把 $M和N$ 的关系构建了起来，所以我们可以用 $effective(N)$ 去替换 $M$，得到公式(3)</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 2 \\cdot effective(N) \\cdot \\exp \\left( -2 \\epsilon^2 N \\right)\n\\tag{$3$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c26b572d17e970051568f6781b5420330ce9892e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-6%20summary.png\" alt=\"Summary\"></p>\n<center> 图六 Summary <sup>[1]</sup></center>\n\n\n\n<h3 id=\"2-Growth-function\"><a href=\"#2-Growth-function\" class=\"headerlink\" title=\"2) Growth function\"></a>2) Growth function</h3><p>上面Binary Clasification的分类方法叫做二分类法（dichotomy)，为了更好地表示 $N和effective(N)$ 的关系，我们引入成长函数（Growth Function) $M_H(N)$ 来表示，具体的数学表达如公式（4）所示。</p>\n<script type=\"math/tex; mode=display\">\nM_H(N) = \\max\\limits_{x_1,x_2,...,x_N ∈ X}  \\lvert H(x_1,x_2,...,x_N) \\rvert\n\\tag{$4$}  \\quad(其中，上限为2^N)</script><h3 id=\"3-Different-Types-of-Growth-function\"><a href=\"#3-Different-Types-of-Growth-function\" class=\"headerlink\" title=\"3) Different Types of Growth function\"></a>3) Different Types of Growth function</h3><h4 id=\"①-Growth-Function-for-Positive-Rays\"><a href=\"#①-Growth-Function-for-Positive-Rays\" class=\"headerlink\" title=\"① Growth Function for Positive Rays\"></a>① Growth Function for Positive Rays</h4><blockquote>\n<p>Positive Rays 是用一个一维向量作用于一维坐标上，与该向量同方向的值为+1，反方向为-1<br>如图七所示，Postives Rays 的成长函数为 $M_H(N) = (N-1)$，当 $N \\geq 2$的时候，$M_H(N) &lt; 2^N = O(N)$</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-7%20Growth%20Function%20for%20Positive%20Rays.png\" alt=\"Growth Function for Positive Rays\"></p>\n<center> 图七 Growth Function for Positive Rays <sup>[2]</sup></center>\n\n\n\n<h4 id=\"②-Growth-Function-for-Positive-Interval\"><a href=\"#②-Growth-Function-for-Positive-Interval\" class=\"headerlink\" title=\"② Growth Function for Positive Interval\"></a>② Growth Function for Positive Interval</h4><blockquote>\n<p>Positive Interval 是用一个一维“线段”作用于一维坐标上，与在线段里面的值为+1，外面的为-1<br>如图八所示，Positive Interval 的成长函数为 $M_H(N) = C_{N+1}^2 + 1 = \\frac{1}{2}N^2 + \\frac{1}{2}N + 1$，当 $N \\geq 3$的时候，$M_H(N) &lt; 2^N = O(N^2)$</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-8%20Growth%20Function%20for%20Positive%20Intervals.png\" alt=\"Growth Function for Positive Interval\"></p>\n<center> 图八 Growth Function for Positive Interval <sup>[2]</sup></center>\n\n\n\n<h4 id=\"③-Growth-Function-for-Convex-Sets\"><a href=\"#③-Growth-Function-for-Convex-Sets\" class=\"headerlink\" title=\"③ Growth Function for Convex Sets\"></a>③ Growth Function for Convex Sets</h4><blockquote>\n<p>Convex Sets 不太好理解。可以理解成在二维坐标上，用凸多边形去把所需要的点串起来。在多边形顶点上的点的值为+1，不在的为-1。因为我们讨论的是最大的可能性，所以当我们把所有的点都放在一个圆上的时候，必定存在一个凸多边形可以连接任意多个点（即可以画出任意多边形），然后再把所有点的组合情况加起来<br>如图九所示，Convex Setsl 的成长函数为 $M_H(N) = \\sum\\limits_{i=0}^{N}C_{i}^{N} = 2^N$</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/5e76112bcd38118a6046de5b5a666dff2b5eedee/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-9%20Growth%20Function%20for%20Convex%20Sets.png\" alt=\"Growth Function for Convex Sets\"></p>\n<center> 图九 Growth Function for Convex Sets <sup>[2]</sup></center>\n\n\n\n<h3 id=\"4-Break-Point-of-Growth-function\"><a href=\"#4-Break-Point-of-Growth-function\" class=\"headerlink\" title=\"4) Break Point of Growth function\"></a>4) Break Point of Growth function</h3><p>我们称能满足完全二分类(出现不同种类的数量为$2^N$)的情况为shattered,能shattered的最大的点为突破点(break point)。<br>然后根据上面对N从1-5的尝试，得到的最大可能性如下表</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>N</th>\n<th>$effctive(N)$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>$2 = 2^1$</td>\n<td></td>\n</tr>\n<tr>\n<td>2</td>\n<td>$4 = 2^2$</td>\n<td></td>\n</tr>\n<tr>\n<td>3</td>\n<td>$8 = 2^3$</td>\n<td></td>\n</tr>\n<tr>\n<td>4</td>\n<td>$14 &lt; 2^5$</td>\n<td></td>\n</tr>\n<tr>\n<td>5</td>\n<td>$32 &lt;&lt; 2^6$</td>\n<td></td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n<td></td>\n</tr>\n<tr>\n<td>N</td>\n<td>$effctive(N) &lt;&lt; 2^N$</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>可以推断出公式（3）</p>\n<script type=\"math/tex; mode=display\">\neffctive(N): M_H(N) \\leq \\max( possible \\quad M_H(N) \\quad Given \\quad break- point \\quad(K)) \\leq 2^N\n\\tag{$3$}</script><p>上面关于Growth Function讨论的几种情况的Break Point 如图十所示，我们可以看出成长函数的复杂度与Break Point的大小存在一定的关系。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/033cce3063c6d35c7ac55af137821cf97819be44/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-10%20Break%20Point.png\" alt=\"Break Point of Growth function\"></p>\n<center> 图十 Break Point of Growth function <sup>[3]</sup></center>\n\n\n<h3 id=\"5-Bounding-Function\"><a href=\"#5-Bounding-Function\" class=\"headerlink\" title=\"5) Bounding Function\"></a>5) Bounding Function</h3><h4 id=\"①-Introduction-of-Bounding-Function\"><a href=\"#①-Introduction-of-Bounding-Function\" class=\"headerlink\" title=\"① Introduction of Bounding Function\"></a>① Introduction of Bounding Function</h4><p>根据上一节的Break Point $K$和样本点 $N$的关系，我们引入一个新概念，上限函数(Bounding Function) $B(N,K)$。这个函数表示有$N$个样本点且成长函数的突破点是$K$的时候，最多有多少种组合情况，比如说$B(3,2) = 3$（这个比较容易想象，这里就不展开讨论了）。并且这个上限函数满足公式（4），因为这是采用而分类的方法来进行的，最大值为$2^N$。</p>\n<script type=\"math/tex; mode=display\">\nB(N,K) \\leq 2^N\n\\tag{$3$}</script><p>但是显然在上面的例子中，我们可以看到$B(N,K) &lt; 2^N \\quad(N \\geq K)$，所以我们下面进一步确定这个上限函数的最大值。</p>\n<h4 id=\"②-Proof-of-Bounding-Function\"><a href=\"#②-Proof-of-Bounding-Function\" class=\"headerlink\" title=\"② Proof of Bounding Function\"></a>② Proof of Bounding Function</h4><p>我们下面用表格的方式来表示$B(N,K)$ 表格如下表，我们下面将会填满这个表格来找出相应的规律</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>1.根据上面的规律，我们知道在$N&lt;K$的时候，$B(N,K) = 2^N$，所以表格更新如下</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>2.然后当$K=1$的时候，我们至少有一种分法（全正或者全负），所以第一列全部为1，表格更新如下。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0120\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>3.接着，当$N=K$的时候，我们上面也可以看到，所有的值最大都等于$2^N-1$(因为不能所有情况都出现一次)，所以表格更新如下。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0120\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">3</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">7</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">15</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">31</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">63</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>4.在之前的章节，我们也数过$B(3,2) = 4$，其实看到这里我们已经大概有一些规律了：下面一项为上面两项之和， $B(N,K) = B(N-1, K) + B(N-1, K-1)$。当然我们只是猜测，下面我们继续证明。表格更新如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0120\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">3</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#F0FFF0)\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">7</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">15</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">31</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">63</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\"></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>5.我们证明上面的猜想$B(N,K) = B(N-1, K) + B(N-1, K-1)$<br>1）首先我们遍历B(4,3)，可以得到图十一的结果，然后我们整理了一下结果的顺序，可以发现橙色区域 {$x_1,x_2,x_3$}结果分别出现了2次，而紫色区域的{$x_1,x_2,x_3$}结果只出现了1次。</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\" alt=\"Reorganized Dichotomies of B(4,3) - 1\"></p>\n<center> 图十一 Reorganized Dichotomies of B(4,3) - 1 <sup>[4]</sup></center>\n\n\n<p>2）所以我们单独把{$x_1,x_2,x_3$}，提出来看，并把橙色区域的个数设为 $\\alpha$，紫色区域的个数为 $\\beta$，那么原来4个点的情况 $B(4,3) = 2 \\alpha + \\beta$，而3个点的情况 $B(3,3) = \\alpha + \\beta$，如下图十二所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%202.png\" alt=\"Reorganized Dichotomies of B(4,3) - 2\"></p>\n<center> 图十二 Reorganized Dichotomies of B(4,3) - 2 <sup>[4]</sup></center>\n\n<p>3)接着我们单独看 $\\alpha$可以发现这个刚好是 $B(3,2)$ 的最大可能性，也就是说$ \\alpha \\leq B(3,2) = 4 $，如图十三所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%203.png\" alt=\"Reorganized Dichotomies of B(4,3) - 3\"></p>\n<center> 图十三 Reorganized Dichotomies of B(4,3) - 3 <sup>[4]</sup></center>\n\n\n<p>4） 根据上面的分析，我们目前得到三个公式，如下面的公式（4）（5）（6）。</p>\n<script type=\"math/tex; mode=display\">\nB(4,5) = 2 \\cdot \\alpha + \\beta\n\\tag{$4$}</script><script type=\"math/tex; mode=display\">\n\\alpha + \\beta \\leq B(3,3)\n\\tag{$5$}</script><script type=\"math/tex; mode=display\">\n\\alpha \\leq B(3,2)\n\\tag{$6$}</script><p>所以把公式(5)(6)加起来，我们可以更新公式（4）为公式（7）</p>\n<script type=\"math/tex; mode=display\">\nB(4,5) \\leq 2 \\cdot \\alpha + \\beta\n\\tag{$7$}</script><p>5）最后我们用同样的方法来研究$B(N,K)$可以很容易证明到公式（8）（9）（10）</p>\n<script type=\"math/tex; mode=display\">\nB(N - 1,K) \\leq \\sum\\limits_{i=0}^{k-1} C_{N-1}^{i}\n\\tag{$8$}</script><script type=\"math/tex; mode=display\">\nB(N - 1,K - 1) \\leq \\sum\\limits_{i=0}^{k-2} C_{N-1}^{i} = \\sum\\limits_{i=1}^{k-1} C_{N-1}^{i}\n\\tag{$9$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\nB(N,K)  & \\leq {B(N-1,K) + B(N-1,K-1)} \\\\\n        & \\leq \\sum\\limits_{i=0}^{k-1} C_{N-1}^{i} + \\sum\\limits_{i=1}^{k-1} C_{N-1}^{i} \\\\\n        & = C_{N-1}^0 + \\sum\\limits_{i=1}^{k-1} \\left( C_{N-1}^{i} + C_{N-1}^{i}\\right)  \\\\\n        & = 1 + \\sum\\limits_{i=1}^{k-1} C_{N}^{i}  \\\\\n        & = C_{N}^0 + \\sum\\limits_{i=1}^{k-1} C_{N}^{i}  \\\\\n        & = \\sum\\limits_{i=0}^{k-1} C_{N}^{i}\n\\end{align}\n\\tag{$10$}</script><p>所以我们的表格更新如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$B(N,K)$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0120\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">2</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">3</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">4</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#F0FFF0)\">4</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">7</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">8</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq5$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500\">11</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">15</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">16</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq6$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq16$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq15$</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">31</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">32</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq7$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq22$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq26$</font></td>\n<td style=\"text-align:center\"><font color=\"#FFA500)\">$\\leq57$</font></td>\n<td style=\"text-align:center\"><font color=\"#FF0000\">63</font></td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\"><font color=\"#0000FF\">1</font></td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>我们再把$N^{K-1}$的表格整理如下：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">$N^{N-1}$</th>\n<th style=\"text-align:center\">K=1</th>\n<th style=\"text-align:center\">K=2</th>\n<th style=\"text-align:center\">K=3</th>\n<th style=\"text-align:center\">K=4</th>\n<th style=\"text-align:center\">K=5</th>\n<th style=\"text-align:center\">K=6</th>\n<th style=\"text-align:center\">…</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">N=1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=2</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=3</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">9</td>\n<td style=\"text-align:center\">27</td>\n<td style=\"text-align:center\">27</td>\n<td style=\"text-align:center\">81</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=4</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">64</td>\n<td style=\"text-align:center\">64</td>\n<td style=\"text-align:center\">256</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=5</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">25</td>\n<td style=\"text-align:center\">125</td>\n<td style=\"text-align:center\">125</td>\n<td style=\"text-align:center\">625</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">N=6</td>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">36</td>\n<td style=\"text-align:center\">216</td>\n<td style=\"text-align:center\">216</td>\n<td style=\"text-align:center\">1296</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td style=\"text-align:center\">…</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>对比图参考老师上课的PPT，如图十四所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-14%20Comparision%20of%20B(N%2CK)%20and%20N%5E(K-1).png\" alt=\"Comparision of B(N,K) and N^(K-1)\"></p>\n<center> 图十四 Comparision of B(N,K) and N^(K-1) <sup>[6]</sup></center>\n\n\n<p>总结起来就是：在$N \\geq 2, K \\geq 3$的时候，总有公式（11）的情况。</p>\n<script type=\"math/tex; mode=display\">\nM_H(N) \\leq B(N,K) = \\sum\\limits_{i=0}^{K-1} \\leq N^{K-1}\n\\tag{$11$}</script><h3 id=\"6-Vapnik-Chervonenkis-VC-bound\"><a href=\"#6-Vapnik-Chervonenkis-VC-bound\" class=\"headerlink\" title=\"6) Vapnik-Chervonenkis (VC) bound\"></a>6) Vapnik-Chervonenkis (VC) bound</h3><p>@TODO: 这一节主要是证明从数学的角度上证明VC Bound 并以此更新Hoeffding Inequity。目前听得失一知半解，所以只贴出结论，后面再补充。</p>\n<p>结论如图十五所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/738603b7f8e8f10505a791140c32f4677d4e7d84/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-13%20VC%20bound.png\" alt=\"Vapnik-Chervonenkis (VC) bound\"></p>\n<center> 图十五 Vapnik-Chervonenkis (VC) bound <sup>[5]</sup></center>\n\n<p>这个VC Bound的作用是把之前Hoeffding的参数 $M$替换成这里引入的成长函数 $M_H(N)$，并构建出成长函数与样本数量（N）的关系这样的话，我们就可以容易的得到结论：在样本N足够大时候，发生Bad Data的概率小于 $epsilon$ ($E_{in} \\approx E_{out}$)，可以得出错误率也低($E_{in} \\approx 0$)，说明机器学习是可能的。</p>\n<p>也就是说要说机器可以学习必须满足下面的条件：</p>\n<ol>\n<li>假设空间的成长函数 $M_H(N)$ 存在Break Point K （即有一个好的假设空间$H$)</li>\n<li>输入数据的样本 $N$ 足够大（有一个好的数据集 $D$）</li>\n<li>存在一个算法，能够找出能在假设空间 $H$ 中找到一个值使得错误率 $E_{in}$ 足够小 （有一个好的算法 $A$ —&gt;也就是我们后面会研究的重点）</li>\n</ol>\n<p>其中：条件1和2通过VC Bound保证了 $E_{in} \\approx E_{out}$，条件3保证了 $E_{in} \\approx 0$ $\\Longrightarrow$ Machine Can Learn.</p>\n<hr>\n<p><br><br></p>\n<h2 id=\"3-The-VC-Dimension\"><a href=\"#3-The-VC-Dimension\" class=\"headerlink\" title=\"3. The VC Dimension\"></a>3. The VC Dimension</h2><h3 id=\"1-Definition-of-VC-Dimension\"><a href=\"#1-Definition-of-VC-Dimension\" class=\"headerlink\" title=\"1) Definition of VC Dimension\"></a>1) Definition of VC Dimension</h3><p>VC Dimension( $d_{vc}$ )指的是能够使得成长函数可以被shatter的最大值（即 Break Point - 1)，用符号表示为公式（12）。</p>\n<script type=\"math/tex; mode=display\">\nd_{vc} = min(K) -1\n\\tag{$12$}</script><p>上面的Hoeffding Inequity可以变成公式（13）</p>\n<script type=\"math/tex; mode=display\">\n\\rho \\left[ \\lvert E_{in}(g) - E_{out}(g) \\rvert  > \\epsilon\\right] \\leq 4 \\cdot (2N)^{d_{vc}} \\cdot \\exp \\left( -\\frac{1}{8} \\epsilon^2 N \\right)\n\\tag{$13$}</script><p>因此，根据这个特点，我们只要确保一个成长函数存在 VC Dimension，我们就可以确定他存在Break Point，是一个好的假设空间。</p>\n<h3 id=\"2-Generalization-Error\"><a href=\"#2-Generalization-Error\" class=\"headerlink\" title=\"2) Generalization Error\"></a>2) Generalization Error</h3><p>我们引入泛化误差 $\\delta$ 表示 $E_{in}(g) 和 E_{out}(g)$ 的接近程度，即 $\\delta = E_{in}(g) - E_{out}(g)$ ，根据公式（13），我们稍作化简，如公式（14）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\delta                           &= 4 \\cdot (2N)^{d_{vc}} \\cdot \\exp \\left( -\\frac{1}{8} \\epsilon^2 N \\right) \\\\\n\\frac{4 (2N)^{d_{vc}}}{\\delta}   &= \\exp(\\frac{1}{8} \\epsilon^2 N)  \\\\\n\\epsilon                         &= \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}\n\\end{align}\n\\tag{$14$}</script><p>也就是说 $E_{in}(g) - E_{out}(g)$ 的误差会小于等于 $\\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}$。 所以我们可以求得 $E_{out}$ 的范围如公式（15）</p>\n<script type=\"math/tex; mode=display\">\nE_{in}(g) -  \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )} \\leq E_{out}(g) \\leq E_{in}(g) + \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )})\n\\tag{$15$}</script><h3 id=\"3-Model-Complexity\"><a href=\"#3-Model-Complexity\" class=\"headerlink\" title=\"3) Model Complexity\"></a>3) Model Complexity</h3><p>上一节，我们求出了 $E_{in}(g) - E_{out}(g)$ 的误差，为了方便引用，我们引入了新的概念：模型复杂度（Model Complexitiy）来表示这个误差值，数学表示如公式（16）</p>\n<script type=\"math/tex; mode=display\">\n\\Omega(N,H,\\delta) =  \\sqrt{\\frac{8}{N} \\cdot \\ln( \\frac{4(2N)^{d_{vc}}}{\\delta} )}\n\\tag{$15$}</script><p>可以看出，随着VC Dimension的增大，$\\Omega$也会变大，然后 $E_{in}(g)$ 也会随着VC Dimension的增大而变小（因为选择的假设空间大了），但是 $E_{out}(g)$ 却不是一个单调函数，因为公式（15），然后这2个值一个变大一个变小，但是最终的话，$E_{out}$ 的曲线是先下降，然后上升（遍历一边就可以得到结果了），所以找到 $d_{vc}^{*}$ 很重要。(因为 $E_{out}$ 才是我们机器学习最重要的指标）<br>结果如图十四所示。<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bab59f769a00e4a303287ff809ff1df7a2916f55/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-15%20Error%20and%20VC%20dimension.png\" alt=\"Error and VC dimension\"></p>\n<center> 图十六 Error and VC dimension <sup>[6]</sup></center>\n\n\n<h3 id=\"4-How-much-Data-We-need-Theoretically-and-Practically\"><a href=\"#4-How-much-Data-We-need-Theoretically-and-Practically\" class=\"headerlink\" title=\"4) How much Data We need Theoretically and Practically\"></a>4) How much Data We need Theoretically and Practically</h3><p>问题：假如现在老板给员工下达了一个任务，要求这个模型的 ${\\epsilon = 0.1，\\delta = 0.1，  d_{vc} = 3}$ ，那样的话，我们需要多少个样本 $N$ 才能满足要求呢？<br>回答：根据上面的公式（14），分别代入参数到等式中，可以求得样本数量 $N$ 如图十五的橙色区域所示。但是实际上，我们只需要 $10d_{vc}$就足够了。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bab59f769a00e4a303287ff809ff1df7a2916f55/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-15%20Error%20and%20VC%20dimension.png\" alt=\"How much Data We need Theoretically and Practically\"></p>\n<p><center> 图十七 How much Data We need Theoretically and Practically <sup>[6]</sup></center><br><br></p>\n<p>因为我们在计算的时候同样的把上限给放大了，放大的原因如下所示:</p>\n<ol>\n<li>Hoeffding Inequity 不需要知道未知的情况，但是VC Bound可以用于各种分布，各种目标函数(因为 VC Bound的推导是基于不同的N和K)；</li>\n<li>在给Binary Classification 强行装上成长函数本身就是一个宽松的上界，但是VC Bound可以用于各种数据样本；</li>\n<li>使用二项式 $N^{d_{vc}}$ 作为成长函数的上界使得约束更加宽松，但是VC Bound可以用于任意具有相同VC维的假设空间；</li>\n<li>联合限制（union bound）并不是一定会选择出现不好事情的假设函数，但是VC Bound可以用于任意算法。</li>\n</ol>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>我们首先通过回顾上一节的内容，得到结论是根据Hoeffding Inquity: 要使得机器可以学习的条件是 ① $E_{in} \\approx E_{out}$ ② $E_{in} \\approx 0$</li>\n<li>接着我们讨论了什么情况下才能保证这2个条件满足，进行了讨论，最终我们通过引入① Growth Function ② Break Point ③ VC Bound, VC Dimension 更改Hoeffding Inequity的上限，最终得到我们需要的答案：<ul>\n<li>假设空间的成长函数 $M_H(N)$ 存在Break Point $K$ （即有一个好的假设空间 $H$)</li>\n<li>输入数据的样本 $N$ 足够大（有一个好的数据集 $D$）</li>\n<li>存在一个算法，能够找出能在假设空间 $H$ 中找到一个值使得错误率 $E_{in}$ 足够小 （有一个好的算法 $A$ —&gt;也就是我们后面会研究的重点）</li>\n<li>其中：条件1和2通过VC Bound保证了 $E_{in} \\approx E_{out}$，条件3保证了 $E_{in} \\approx 0$ ⟹ Machine Can Learn.</li>\n</ul>\n</li>\n<li>之后我们讨论了理论上 $N \\approx 10000 d_{vc}$ 而实际上只需要 $N \\approx 10 d_{vc}$ 的能使得 $E_{out}$ 最小，并且分析了为什么理论上和实际上差别这么大</li>\n</ol>\n<hr>\n<p><br><br><br></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1]机器学习基石(台湾大学-林轩田)\\5\\5 - 2 - Effective Number of Lines (15-26)</p>\n<p>[2]机器学习基石(台湾大学-林轩田)\\5\\5 - 3 - Effective Number of Hypotheses (16-17)</p>\n<p>[3]机器学习基石(台湾大学-林轩田)\\5\\5 - 4 - Break Point (07-44)</p>\n<p>[4]机器学习基石(台湾大学-林轩田)\\6\\6 - 3 - Bounding Function- Inductive Cases (14-47)</p>\n<p>[5]机器学习基石(台湾大学-林轩田)\\6\\6 - 4 - A Pictorial Proof (16-01)</p>\n<p>[6]机器学习基石(台湾大学-林轩田)\\7\\7 - 4 - Interpreting VC Dimension (17-13)</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"6.How can Machine Learn? - Noice and Error","date":"2017-10-09T08:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n\n# How can Machine Learn? - Noice and Error\n\n> 这一节主要讨论存在噪音的情况下，上一节中讨论的VC Bound 还能否适用，我们能不能相信机器能学到东西。\n\n## 1. Noise\n### 1) Introduction of Noise\n噪音数据指的是：在实际情况中，与所希望的值不一样的数据。\n\n噪声有几种错误：\n1. noise in y: mislabeled data;\n2. noise in y: different labels for same x;\n3. noise in x: error x.\n4. ...\n\n造成噪声的原因也有很多种，比如：\n1. 数据在测量时产生误差\n2. 数据录入错误\n3. 数据在处理过程中处理不当\n4. ...\n\n\n### 2) The Influence of Noise\n继续以之前抽球的例子来说明，在一个罐子中有很多小球，要么是绿色，要么是橙色，假设存在一个会不断变色的小球，颜色不停的快速变化着（即噪音），我们称之为概率性小球（Probabilitic Ball），其他的小球颜色都固定，我们称为确定性小球（Deterministic Ball）。那么确定性的小球满足公式（1）的概率，然后我们再假设这个概率性小球为+1的概率为0.7，那么满足下面公式（2）。\n$$\n\\begin{align}\n&\\rho \\left( +1|x \\right) = 1   &\\quad(当 y = f(x)) \\\\\n&\\rho \\left( -1|x \\right) = 0  &\\quad(当 y \\neq f(x))\n\\end{align}\n\\tag{$1$}\n$$\n\n$$\n\\begin{align}\n&\\rho \\left( +1|x \\right) = 0.7   &\\quad(当 y = f(x)) \\\\\n&\\rho \\left( -1|x \\right) = 0.3  &\\quad(当 y \\neq f(x))\n\\end{align}\n\\tag{$2$}\n$$\n\n两个分布函数 $\\rho(x)$ 和 $\\rho(y|x)$ ，其中 $\\rho(x)$ 越大意味着 $x$ 被选为中的概率越大，$\\rho(y|x)$越大意味着该样本属于某一类的概率越大，所以联合概率指的是样本点上分类尽量正确的可能性。这种含有噪音的输入样本以及标记分别服从 $\\rho(x)$ 和 $\\rho(y|x)$，即 $(x,y)$ 服从 $\\rho(x,y)$ 的联合概率分布。所以VC限制依然适用。\n\n所以我们的流程图更新为图一所示。\n\n![Learning Flow](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a4a0aeb4e07f1c860787df98a08d247319527acf/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-1%20Noice.png)\n<center>图一 Learning Flow<sup>[1]</sup></center>\n<br>\n\n## 2. Error\n这里主要介绍 Pointwise Error Measure的方法\n### 1) Pointwise Error Measure\n我们用每个点的误差衡量来衡量整体误差，用 $err()$ 表示。\n对应 $E_{in}(g), E_{out}(g)$可以用下面公式（3）和（4）表示。\n\n$$\nE_{in}(g) = \\frac{1}{N} \\cdot \\sum\\limits_{n=1}^N err(g(x_n), f(x_n))\n\\tag{$3$}\n$$\n\n$$\nE_{out}(g) = \\epsilon \\cdot err(g(x), f(x))\n\\tag{$4$}\n$$\n\n\n### 2) Two Important Pointwise Error Measures\n我们用0/1错误来衡量分类问题，用平方错误来衡量回归问题。其数学表达式，如公式（5）（6）所示\n$$\nerr( \\tilde{y}, y) = \\left[\\left[ \\tilde{y} \\neq y \\right]\\right]\n\\tag{$5$}\n$$\n\n$$\nerr( \\tilde{y}, y) = \\left( \\tilde{y} - y \\right)^2\n\\tag{$6$}\n$$\n\n### 3) Choice of Error Measures\n对于二元分类问题错误的类型也分为两类，如图二所示。\n\n![Error Types of Binary Classification](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/82fc93d617c6aec7addbbc0bcff9207f127e42f9/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-2%20Error.png)\n<center>图二 Error Types of Binary Classification<sup>[2]</sup></center>\n<br>\n\n在目标函数f为+1时，假设函数g却给出了-1，这种错误叫做错误的拒绝（false reject），另一种是在目标函数f为-1时，假设函数g却给出了+1，这种错误叫做错误的接受（false accept）。\n\n然后在不同的应用场合下，如果我们要给错误加上权重，那么这个权重会不一样。比如说：\n\n> 1. 超市指纹识别的例子\n\n如果在超市中通过指纹识别来进行打折活动，如果是vip用户，之前有指纹录入的话，就应该有优惠活动，否则没有。\n如果发生false reject的情况，那么顾客可能会不高兴，这样就会损失了一部分未来的生意；而如果发生false accept的话，超市只不过损失了一点小钱。\n所以对于超市的成本表如图三所示，false reject会牺牲成本比较大，而false accept牺牲的成本会较小。所以，我们应该尽量避免false reject的情形。\n\n![Finger Verification for Supermarket](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/055e02432ef3bf9402663249ae71198c78ecca15/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-3%20example-supermarket.png)\n<center>图三 Finger Verification for Supermarket<sup>[3]</sup></center>\n<br>\n\n\n> 2. CIA指纹识别的例子\n\n如果美国中情局，用指纹识别来判断该人是否有权限进入系统查看重要资料。\n那么，发生false accept的情况会导致很严重的后果，而false reject的话，就不会有太大的影响。\n所以对于CIA的成本表如图四所示，应该尽量避免false accept的情形。\n\n![Finger Verification for CIA](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/055e02432ef3bf9402663249ae71198c78ecca15/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-4%20example-cia.png)\n<center>图四 Finger Verification for CIA<sup>[3]</sup></center>\n<br>\n\n<br><br>\n----------------------------------\n# Summary\n1. 首先介绍了Noise，并且讨论了Noise对我们的VC Bound不会产生影响，即Noise不影响机器学习的可行性。\n2. 然后介绍了Error，然后讨论了0/1错误来衡量二元分类问题，以及用平方错误来衡量回归问题。\n3. 最后讨论了不同应用场景下的Error应该加上不同的权重，比如超市和CIA的指纹识别系统的侧重点不同。\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1]机器学习基石(台湾大学-林轩田)\\8\\8 - 1 - Noise and Probabilistic Target (17-01)\n\n[2]机器学习基石(台湾大学-林轩田)\\8\\8 - 2 - Error Measure (15-10)\n\n[3]机器学习基石(台湾大学-林轩田)\\8\\8 - 3 - Algorithmic Error Measure (13-46)\n\n<br>\n<br>\n---------------------------------------------","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-09-6.How can Machine Learn - Noice and Error.md","raw":"---\ntitle: 6.How can Machine Learn? - Noice and Error\ndate: 2017-10-09 16:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n\n# How can Machine Learn? - Noice and Error\n\n> 这一节主要讨论存在噪音的情况下，上一节中讨论的VC Bound 还能否适用，我们能不能相信机器能学到东西。\n\n## 1. Noise\n### 1) Introduction of Noise\n噪音数据指的是：在实际情况中，与所希望的值不一样的数据。\n\n噪声有几种错误：\n1. noise in y: mislabeled data;\n2. noise in y: different labels for same x;\n3. noise in x: error x.\n4. ...\n\n造成噪声的原因也有很多种，比如：\n1. 数据在测量时产生误差\n2. 数据录入错误\n3. 数据在处理过程中处理不当\n4. ...\n\n\n### 2) The Influence of Noise\n继续以之前抽球的例子来说明，在一个罐子中有很多小球，要么是绿色，要么是橙色，假设存在一个会不断变色的小球，颜色不停的快速变化着（即噪音），我们称之为概率性小球（Probabilitic Ball），其他的小球颜色都固定，我们称为确定性小球（Deterministic Ball）。那么确定性的小球满足公式（1）的概率，然后我们再假设这个概率性小球为+1的概率为0.7，那么满足下面公式（2）。\n$$\n\\begin{align}\n&\\rho \\left( +1|x \\right) = 1   &\\quad(当 y = f(x)) \\\\\n&\\rho \\left( -1|x \\right) = 0  &\\quad(当 y \\neq f(x))\n\\end{align}\n\\tag{$1$}\n$$\n\n$$\n\\begin{align}\n&\\rho \\left( +1|x \\right) = 0.7   &\\quad(当 y = f(x)) \\\\\n&\\rho \\left( -1|x \\right) = 0.3  &\\quad(当 y \\neq f(x))\n\\end{align}\n\\tag{$2$}\n$$\n\n两个分布函数 $\\rho(x)$ 和 $\\rho(y|x)$ ，其中 $\\rho(x)$ 越大意味着 $x$ 被选为中的概率越大，$\\rho(y|x)$越大意味着该样本属于某一类的概率越大，所以联合概率指的是样本点上分类尽量正确的可能性。这种含有噪音的输入样本以及标记分别服从 $\\rho(x)$ 和 $\\rho(y|x)$，即 $(x,y)$ 服从 $\\rho(x,y)$ 的联合概率分布。所以VC限制依然适用。\n\n所以我们的流程图更新为图一所示。\n\n![Learning Flow](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a4a0aeb4e07f1c860787df98a08d247319527acf/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-1%20Noice.png)\n<center>图一 Learning Flow<sup>[1]</sup></center>\n<br>\n\n## 2. Error\n这里主要介绍 Pointwise Error Measure的方法\n### 1) Pointwise Error Measure\n我们用每个点的误差衡量来衡量整体误差，用 $err()$ 表示。\n对应 $E_{in}(g), E_{out}(g)$可以用下面公式（3）和（4）表示。\n\n$$\nE_{in}(g) = \\frac{1}{N} \\cdot \\sum\\limits_{n=1}^N err(g(x_n), f(x_n))\n\\tag{$3$}\n$$\n\n$$\nE_{out}(g) = \\epsilon \\cdot err(g(x), f(x))\n\\tag{$4$}\n$$\n\n\n### 2) Two Important Pointwise Error Measures\n我们用0/1错误来衡量分类问题，用平方错误来衡量回归问题。其数学表达式，如公式（5）（6）所示\n$$\nerr( \\tilde{y}, y) = \\left[\\left[ \\tilde{y} \\neq y \\right]\\right]\n\\tag{$5$}\n$$\n\n$$\nerr( \\tilde{y}, y) = \\left( \\tilde{y} - y \\right)^2\n\\tag{$6$}\n$$\n\n### 3) Choice of Error Measures\n对于二元分类问题错误的类型也分为两类，如图二所示。\n\n![Error Types of Binary Classification](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/82fc93d617c6aec7addbbc0bcff9207f127e42f9/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-2%20Error.png)\n<center>图二 Error Types of Binary Classification<sup>[2]</sup></center>\n<br>\n\n在目标函数f为+1时，假设函数g却给出了-1，这种错误叫做错误的拒绝（false reject），另一种是在目标函数f为-1时，假设函数g却给出了+1，这种错误叫做错误的接受（false accept）。\n\n然后在不同的应用场合下，如果我们要给错误加上权重，那么这个权重会不一样。比如说：\n\n> 1. 超市指纹识别的例子\n\n如果在超市中通过指纹识别来进行打折活动，如果是vip用户，之前有指纹录入的话，就应该有优惠活动，否则没有。\n如果发生false reject的情况，那么顾客可能会不高兴，这样就会损失了一部分未来的生意；而如果发生false accept的话，超市只不过损失了一点小钱。\n所以对于超市的成本表如图三所示，false reject会牺牲成本比较大，而false accept牺牲的成本会较小。所以，我们应该尽量避免false reject的情形。\n\n![Finger Verification for Supermarket](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/055e02432ef3bf9402663249ae71198c78ecca15/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-3%20example-supermarket.png)\n<center>图三 Finger Verification for Supermarket<sup>[3]</sup></center>\n<br>\n\n\n> 2. CIA指纹识别的例子\n\n如果美国中情局，用指纹识别来判断该人是否有权限进入系统查看重要资料。\n那么，发生false accept的情况会导致很严重的后果，而false reject的话，就不会有太大的影响。\n所以对于CIA的成本表如图四所示，应该尽量避免false accept的情形。\n\n![Finger Verification for CIA](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/055e02432ef3bf9402663249ae71198c78ecca15/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-4%20example-cia.png)\n<center>图四 Finger Verification for CIA<sup>[3]</sup></center>\n<br>\n\n<br><br>\n----------------------------------\n# Summary\n1. 首先介绍了Noise，并且讨论了Noise对我们的VC Bound不会产生影响，即Noise不影响机器学习的可行性。\n2. 然后介绍了Error，然后讨论了0/1错误来衡量二元分类问题，以及用平方错误来衡量回归问题。\n3. 最后讨论了不同应用场景下的Error应该加上不同的权重，比如超市和CIA的指纹识别系统的侧重点不同。\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1]机器学习基石(台湾大学-林轩田)\\8\\8 - 1 - Noise and Probabilistic Target (17-01)\n\n[2]机器学习基石(台湾大学-林轩田)\\8\\8 - 2 - Error Measure (15-10)\n\n[3]机器学习基石(台湾大学-林轩田)\\8\\8 - 3 - Algorithmic Error Measure (13-46)\n\n<br>\n<br>\n---------------------------------------------","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-09-6.How can Machine Learn - Noice and Error","published":1,"updated":"2018-04-14T19:42:06.504Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2e9001frwtj5s2h83ss","content":"<h1 id=\"How-can-Machine-Learn-Noice-and-Error\"><a href=\"#How-can-Machine-Learn-Noice-and-Error\" class=\"headerlink\" title=\"How can Machine Learn? - Noice and Error\"></a>How can Machine Learn? - Noice and Error</h1><blockquote>\n<p>这一节主要讨论存在噪音的情况下，上一节中讨论的VC Bound 还能否适用，我们能不能相信机器能学到东西。</p>\n</blockquote>\n<h2 id=\"1-Noise\"><a href=\"#1-Noise\" class=\"headerlink\" title=\"1. Noise\"></a>1. Noise</h2><h3 id=\"1-Introduction-of-Noise\"><a href=\"#1-Introduction-of-Noise\" class=\"headerlink\" title=\"1) Introduction of Noise\"></a>1) Introduction of Noise</h3><p>噪音数据指的是：在实际情况中，与所希望的值不一样的数据。</p>\n<p>噪声有几种错误：</p>\n<ol>\n<li>noise in y: mislabeled data;</li>\n<li>noise in y: different labels for same x;</li>\n<li>noise in x: error x.</li>\n<li>…</li>\n</ol>\n<p>造成噪声的原因也有很多种，比如：</p>\n<ol>\n<li>数据在测量时产生误差</li>\n<li>数据录入错误</li>\n<li>数据在处理过程中处理不当</li>\n<li>…</li>\n</ol>\n<h3 id=\"2-The-Influence-of-Noise\"><a href=\"#2-The-Influence-of-Noise\" class=\"headerlink\" title=\"2) The Influence of Noise\"></a>2) The Influence of Noise</h3><p>继续以之前抽球的例子来说明，在一个罐子中有很多小球，要么是绿色，要么是橙色，假设存在一个会不断变色的小球，颜色不停的快速变化着（即噪音），我们称之为概率性小球（Probabilitic Ball），其他的小球颜色都固定，我们称为确定性小球（Deterministic Ball）。那么确定性的小球满足公式（1）的概率，然后我们再假设这个概率性小球为+1的概率为0.7，那么满足下面公式（2）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&\\rho \\left( +1|x \\right) = 1   &\\quad(当 y = f(x)) \\\\\n&\\rho \\left( -1|x \\right) = 0  &\\quad(当 y \\neq f(x))\n\\end{align}\n\\tag{$1$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\n&\\rho \\left( +1|x \\right) = 0.7   &\\quad(当 y = f(x)) \\\\\n&\\rho \\left( -1|x \\right) = 0.3  &\\quad(当 y \\neq f(x))\n\\end{align}\n\\tag{$2$}</script><p>两个分布函数 $\\rho(x)$ 和 $\\rho(y|x)$ ，其中 $\\rho(x)$ 越大意味着 $x$ 被选为中的概率越大，$\\rho(y|x)$越大意味着该样本属于某一类的概率越大，所以联合概率指的是样本点上分类尽量正确的可能性。这种含有噪音的输入样本以及标记分别服从 $\\rho(x)$ 和 $\\rho(y|x)$，即 $(x,y)$ 服从 $\\rho(x,y)$ 的联合概率分布。所以VC限制依然适用。</p>\n<p>所以我们的流程图更新为图一所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a4a0aeb4e07f1c860787df98a08d247319527acf/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-1%20Noice.png\" alt=\"Learning Flow\"></p>\n<p><center>图一 Learning Flow<sup>[1]</sup></center><br><br></p>\n<h2 id=\"2-Error\"><a href=\"#2-Error\" class=\"headerlink\" title=\"2. Error\"></a>2. Error</h2><p>这里主要介绍 Pointwise Error Measure的方法</p>\n<h3 id=\"1-Pointwise-Error-Measure\"><a href=\"#1-Pointwise-Error-Measure\" class=\"headerlink\" title=\"1) Pointwise Error Measure\"></a>1) Pointwise Error Measure</h3><p>我们用每个点的误差衡量来衡量整体误差，用 $err()$ 表示。<br>对应 $E_{in}(g), E_{out}(g)$可以用下面公式（3）和（4）表示。</p>\n<script type=\"math/tex; mode=display\">\nE_{in}(g) = \\frac{1}{N} \\cdot \\sum\\limits_{n=1}^N err(g(x_n), f(x_n))\n\\tag{$3$}</script><script type=\"math/tex; mode=display\">\nE_{out}(g) = \\epsilon \\cdot err(g(x), f(x))\n\\tag{$4$}</script><h3 id=\"2-Two-Important-Pointwise-Error-Measures\"><a href=\"#2-Two-Important-Pointwise-Error-Measures\" class=\"headerlink\" title=\"2) Two Important Pointwise Error Measures\"></a>2) Two Important Pointwise Error Measures</h3><p>我们用0/1错误来衡量分类问题，用平方错误来衡量回归问题。其数学表达式，如公式（5）（6）所示</p>\n<script type=\"math/tex; mode=display\">\nerr( \\tilde{y}, y) = \\left[\\left[ \\tilde{y} \\neq y \\right]\\right]\n\\tag{$5$}</script><script type=\"math/tex; mode=display\">\nerr( \\tilde{y}, y) = \\left( \\tilde{y} - y \\right)^2\n\\tag{$6$}</script><h3 id=\"3-Choice-of-Error-Measures\"><a href=\"#3-Choice-of-Error-Measures\" class=\"headerlink\" title=\"3) Choice of Error Measures\"></a>3) Choice of Error Measures</h3><p>对于二元分类问题错误的类型也分为两类，如图二所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/82fc93d617c6aec7addbbc0bcff9207f127e42f9/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-2%20Error.png\" alt=\"Error Types of Binary Classification\"></p>\n<p><center>图二 Error Types of Binary Classification<sup>[2]</sup></center><br><br></p>\n<p>在目标函数f为+1时，假设函数g却给出了-1，这种错误叫做错误的拒绝（false reject），另一种是在目标函数f为-1时，假设函数g却给出了+1，这种错误叫做错误的接受（false accept）。</p>\n<p>然后在不同的应用场合下，如果我们要给错误加上权重，那么这个权重会不一样。比如说：</p>\n<blockquote>\n<ol>\n<li>超市指纹识别的例子</li>\n</ol>\n</blockquote>\n<p>如果在超市中通过指纹识别来进行打折活动，如果是vip用户，之前有指纹录入的话，就应该有优惠活动，否则没有。<br>如果发生false reject的情况，那么顾客可能会不高兴，这样就会损失了一部分未来的生意；而如果发生false accept的话，超市只不过损失了一点小钱。<br>所以对于超市的成本表如图三所示，false reject会牺牲成本比较大，而false accept牺牲的成本会较小。所以，我们应该尽量避免false reject的情形。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/055e02432ef3bf9402663249ae71198c78ecca15/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-3%20example-supermarket.png\" alt=\"Finger Verification for Supermarket\"></p>\n<p><center>图三 Finger Verification for Supermarket<sup>[3]</sup></center><br><br></p>\n<blockquote>\n<ol>\n<li>CIA指纹识别的例子</li>\n</ol>\n</blockquote>\n<p>如果美国中情局，用指纹识别来判断该人是否有权限进入系统查看重要资料。<br>那么，发生false accept的情况会导致很严重的后果，而false reject的话，就不会有太大的影响。<br>所以对于CIA的成本表如图四所示，应该尽量避免false accept的情形。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/055e02432ef3bf9402663249ae71198c78ecca15/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-4%20example-cia.png\" alt=\"Finger Verification for CIA\"></p>\n<p><center>图四 Finger Verification for CIA<sup>[3]</sup></center><br><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了Noise，并且讨论了Noise对我们的VC Bound不会产生影响，即Noise不影响机器学习的可行性。</li>\n<li>然后介绍了Error，然后讨论了0/1错误来衡量二元分类问题，以及用平方错误来衡量回归问题。</li>\n<li>最后讨论了不同应用场景下的Error应该加上不同的权重，比如超市和CIA的指纹识别系统的侧重点不同。</li>\n</ol>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1]机器学习基石(台湾大学-林轩田)\\8\\8 - 1 - Noise and Probabilistic Target (17-01)</p>\n<p>[2]机器学习基石(台湾大学-林轩田)\\8\\8 - 2 - Error Measure (15-10)</p>\n<p>[3]机器学习基石(台湾大学-林轩田)\\8\\8 - 3 - Algorithmic Error Measure (13-46)</p>\n<p><br></p>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Noice-and-Error\"><a href=\"#How-can-Machine-Learn-Noice-and-Error\" class=\"headerlink\" title=\"How can Machine Learn? - Noice and Error\"></a>How can Machine Learn? - Noice and Error</h1><blockquote>\n<p>这一节主要讨论存在噪音的情况下，上一节中讨论的VC Bound 还能否适用，我们能不能相信机器能学到东西。</p>\n</blockquote>\n<h2 id=\"1-Noise\"><a href=\"#1-Noise\" class=\"headerlink\" title=\"1. Noise\"></a>1. Noise</h2><h3 id=\"1-Introduction-of-Noise\"><a href=\"#1-Introduction-of-Noise\" class=\"headerlink\" title=\"1) Introduction of Noise\"></a>1) Introduction of Noise</h3><p>噪音数据指的是：在实际情况中，与所希望的值不一样的数据。</p>\n<p>噪声有几种错误：</p>\n<ol>\n<li>noise in y: mislabeled data;</li>\n<li>noise in y: different labels for same x;</li>\n<li>noise in x: error x.</li>\n<li>…</li>\n</ol>\n<p>造成噪声的原因也有很多种，比如：</p>\n<ol>\n<li>数据在测量时产生误差</li>\n<li>数据录入错误</li>\n<li>数据在处理过程中处理不当</li>\n<li>…</li>\n</ol>\n<h3 id=\"2-The-Influence-of-Noise\"><a href=\"#2-The-Influence-of-Noise\" class=\"headerlink\" title=\"2) The Influence of Noise\"></a>2) The Influence of Noise</h3><p>继续以之前抽球的例子来说明，在一个罐子中有很多小球，要么是绿色，要么是橙色，假设存在一个会不断变色的小球，颜色不停的快速变化着（即噪音），我们称之为概率性小球（Probabilitic Ball），其他的小球颜色都固定，我们称为确定性小球（Deterministic Ball）。那么确定性的小球满足公式（1）的概率，然后我们再假设这个概率性小球为+1的概率为0.7，那么满足下面公式（2）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&\\rho \\left( +1|x \\right) = 1   &\\quad(当 y = f(x)) \\\\\n&\\rho \\left( -1|x \\right) = 0  &\\quad(当 y \\neq f(x))\n\\end{align}\n\\tag{$1$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\n&\\rho \\left( +1|x \\right) = 0.7   &\\quad(当 y = f(x)) \\\\\n&\\rho \\left( -1|x \\right) = 0.3  &\\quad(当 y \\neq f(x))\n\\end{align}\n\\tag{$2$}</script><p>两个分布函数 $\\rho(x)$ 和 $\\rho(y|x)$ ，其中 $\\rho(x)$ 越大意味着 $x$ 被选为中的概率越大，$\\rho(y|x)$越大意味着该样本属于某一类的概率越大，所以联合概率指的是样本点上分类尽量正确的可能性。这种含有噪音的输入样本以及标记分别服从 $\\rho(x)$ 和 $\\rho(y|x)$，即 $(x,y)$ 服从 $\\rho(x,y)$ 的联合概率分布。所以VC限制依然适用。</p>\n<p>所以我们的流程图更新为图一所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a4a0aeb4e07f1c860787df98a08d247319527acf/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-1%20Noice.png\" alt=\"Learning Flow\"></p>\n<p><center>图一 Learning Flow<sup>[1]</sup></center><br><br></p>\n<h2 id=\"2-Error\"><a href=\"#2-Error\" class=\"headerlink\" title=\"2. Error\"></a>2. Error</h2><p>这里主要介绍 Pointwise Error Measure的方法</p>\n<h3 id=\"1-Pointwise-Error-Measure\"><a href=\"#1-Pointwise-Error-Measure\" class=\"headerlink\" title=\"1) Pointwise Error Measure\"></a>1) Pointwise Error Measure</h3><p>我们用每个点的误差衡量来衡量整体误差，用 $err()$ 表示。<br>对应 $E_{in}(g), E_{out}(g)$可以用下面公式（3）和（4）表示。</p>\n<script type=\"math/tex; mode=display\">\nE_{in}(g) = \\frac{1}{N} \\cdot \\sum\\limits_{n=1}^N err(g(x_n), f(x_n))\n\\tag{$3$}</script><script type=\"math/tex; mode=display\">\nE_{out}(g) = \\epsilon \\cdot err(g(x), f(x))\n\\tag{$4$}</script><h3 id=\"2-Two-Important-Pointwise-Error-Measures\"><a href=\"#2-Two-Important-Pointwise-Error-Measures\" class=\"headerlink\" title=\"2) Two Important Pointwise Error Measures\"></a>2) Two Important Pointwise Error Measures</h3><p>我们用0/1错误来衡量分类问题，用平方错误来衡量回归问题。其数学表达式，如公式（5）（6）所示</p>\n<script type=\"math/tex; mode=display\">\nerr( \\tilde{y}, y) = \\left[\\left[ \\tilde{y} \\neq y \\right]\\right]\n\\tag{$5$}</script><script type=\"math/tex; mode=display\">\nerr( \\tilde{y}, y) = \\left( \\tilde{y} - y \\right)^2\n\\tag{$6$}</script><h3 id=\"3-Choice-of-Error-Measures\"><a href=\"#3-Choice-of-Error-Measures\" class=\"headerlink\" title=\"3) Choice of Error Measures\"></a>3) Choice of Error Measures</h3><p>对于二元分类问题错误的类型也分为两类，如图二所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/82fc93d617c6aec7addbbc0bcff9207f127e42f9/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-2%20Error.png\" alt=\"Error Types of Binary Classification\"></p>\n<p><center>图二 Error Types of Binary Classification<sup>[2]</sup></center><br><br></p>\n<p>在目标函数f为+1时，假设函数g却给出了-1，这种错误叫做错误的拒绝（false reject），另一种是在目标函数f为-1时，假设函数g却给出了+1，这种错误叫做错误的接受（false accept）。</p>\n<p>然后在不同的应用场合下，如果我们要给错误加上权重，那么这个权重会不一样。比如说：</p>\n<blockquote>\n<ol>\n<li>超市指纹识别的例子</li>\n</ol>\n</blockquote>\n<p>如果在超市中通过指纹识别来进行打折活动，如果是vip用户，之前有指纹录入的话，就应该有优惠活动，否则没有。<br>如果发生false reject的情况，那么顾客可能会不高兴，这样就会损失了一部分未来的生意；而如果发生false accept的话，超市只不过损失了一点小钱。<br>所以对于超市的成本表如图三所示，false reject会牺牲成本比较大，而false accept牺牲的成本会较小。所以，我们应该尽量避免false reject的情形。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/055e02432ef3bf9402663249ae71198c78ecca15/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-3%20example-supermarket.png\" alt=\"Finger Verification for Supermarket\"></p>\n<p><center>图三 Finger Verification for Supermarket<sup>[3]</sup></center><br><br></p>\n<blockquote>\n<ol>\n<li>CIA指纹识别的例子</li>\n</ol>\n</blockquote>\n<p>如果美国中情局，用指纹识别来判断该人是否有权限进入系统查看重要资料。<br>那么，发生false accept的情况会导致很严重的后果，而false reject的话，就不会有太大的影响。<br>所以对于CIA的成本表如图四所示，应该尽量避免false accept的情形。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/055e02432ef3bf9402663249ae71198c78ecca15/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter6-4%20example-cia.png\" alt=\"Finger Verification for CIA\"></p>\n<p><center>图四 Finger Verification for CIA<sup>[3]</sup></center><br><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了Noise，并且讨论了Noise对我们的VC Bound不会产生影响，即Noise不影响机器学习的可行性。</li>\n<li>然后介绍了Error，然后讨论了0/1错误来衡量二元分类问题，以及用平方错误来衡量回归问题。</li>\n<li>最后讨论了不同应用场景下的Error应该加上不同的权重，比如超市和CIA的指纹识别系统的侧重点不同。</li>\n</ol>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1]机器学习基石(台湾大学-林轩田)\\8\\8 - 1 - Noise and Probabilistic Target (17-01)</p>\n<p>[2]机器学习基石(台湾大学-林轩田)\\8\\8 - 2 - Error Measure (15-10)</p>\n<p>[3]机器学习基石(台湾大学-林轩田)\\8\\8 - 3 - Algorithmic Error Measure (13-46)</p>\n<p><br></p>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"9.How can Machine Learn? - Linear Model for Classification","date":"2017-10-14T04:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n\n# How can Machine Learn? - Linear Model for Classification\n\n>主要讨论 Linear Classification, Linear Reegression, Logistic Regression 在分类问题上的优劣对比，并拓展到多元分类\n\n## 1. Linear Models for Binary Classification\n\n### 1) Analyzing of Three Linear Models\n我们目前学习了Classification, Linear Reegression, Logistic Regression， 这三个模型有很多类似的地方。\n总结如图一，这里引入了s(score)作为得分，$s=w^Tx$\n\n![Linear Models Revisit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0e01f9a531b829ec48a4f3e9970fc737f62f9b56/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-1%20Linear%20Models.png)\n<center> 图一 Linear Models Revisit <sup>[1]</sup></center>\n\n从图中可以看出，Linear Classification的求解是NP-hard问题，其他两个方法都容易求解。并且可以看到他们的Hypothesis 都与 s 有关。所以可以利用这两种模型的算法近似求得二分类问题的最优权值w\n\n\n\n\n### 2) Error Functions Comparison\n\n1.首先对比3种方法的误差方程，如图二所示\n\n![Error Functions Revisit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f0bb56f85acd2e6c8500017ff01411c065136237/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-2%20Error%20Function%20Revisit.png)\n<center> 图二 Error Functions Revisit <sup>[1]</sup></center>\n\n\n\n2.根据图二的错误函数得到图三的关系图。\n\n![Visualizing Error Functions](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/89a29e7078c423404f64048c7ebbef7c33c6f534/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-3%20Visualizing%20Error%20Functions.png)\n<center> 图三 Visualizing Error Functions <sup>[1]</sup></center>\n\n从图三，我们可以得到以下几个结论：\n1. 对于 $err_{0/1}$， 在 $ys > 0$ 和 $ys < 0$ 的值相差很大\n2. $err_{CE}$ 随着 ys的增大而减小，而且无线接近于0，在 $ys > 0$ 的情况下，逐渐毕竟于 $err_{0/1}$\n3. $err_{SQR}$ 是一个凹函数，在 $ys = 1$ 的情况下，与 $err_{0/1}$ 相等\n4. $err_{0/1}$  始终小于 $err_{SQR}$， 绝大部分情况下小于 $err_{CE}$\n5. 虽然$err_{0/1}$  始终小于 $err_{SQR}$， 但是两者的差距较大\n6. 虽然$err_{0/1}$  不是全部情况下小于 $err_{CE}$，但是稍微做调整，可以实现 $err_{0/1}$  始终小于 $err_{CE}$ ，并且这两者的曲线接近，所以我们下面将想办法采用这种方式。\n\nerr为了让 $err_{0/1}$  也始终小于 $err_{CE}$ ，我们稍微做一些调整, 得到scaled CE $err_{SCE}$，如图四所示，这里只是给  $err_{CE}$ 的表达式把对数从 $log_e$ 换成 $log_2$。即从公式（1）换成公式（2）\n\n$$\nerr_{CE} = err(w,x,y) = err_{CE}(s,y) = ln(1+exp(-yw^Tx)) = ln(1+exp(-ys))\n\\tag{1}\n$$\n\n$$\nerr_{SCE} = \\frac{1}{ln2} err_{CE} = log_2(1+exp(-ys))\n\\tag{2}\n$$\n\n![Visualizing Error Functions with Scaled CE](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0277ba5bccfbf510f13974f587508dbebd21bea0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-4%20Visualizing%20Error%20Functions-scaled%20ce.png)\n<center> 图四 Visualizing Error Functions with Scaled CE <sup>[1]</sup></center>\n\n从图四中，我们可以发现，$err_{0/1}$  始终小于 $err_{SQR}$ 和 $err_{CE}$，如公式（3）所示，那么在数据量足够大的情况下，$E_{out}$ 也有类似的情况，如公式（4）所示。再接着，由于之前VC Bound的结论，我们可以得到公式（5）， 所以接下来我们就可以Logistic Regrssion的方法去解决Linear Classification的问题了\n\n$$\nE_{in}^{0/1}(w) \\leq E_{in}^{SCE}(w) = \\frac{1}{ln2} E_{in}^{CE}(w)\n\\tag{3}\n$$\n$$\nE_{out}^{0/1}(w) \\leq E_{out}^{SCE}(w) = \\frac{1}{ln2} E_{out}^{CE}(w)\n\\tag{4}\n$$\n$$\nE_{in}^{0/1}(w) \\leq E_{in}^{0/1}(w) + \\Omega ^{0/1}  \\leq E_{out}^{SCE}(w)  + \\Omega ^{0/1} = \\frac{1}{ln2} E_{out}^{CE}(w)  + \\Omega ^{0/1}\n\\tag{5}\n$$\n\n算法流程一般是在输出空间{-1, +1} 的情况下，通过线性回归和logistic回归相对应的求解方法求出最优的权值 $W_{REG}$；\n将求得的代入公式sign，得到最优假设函数。\n\n\n\n\n<br><br>\n----------------------------------\n## 2. Multiclass via Logistic Regression - OVA Algorithm\n\n实际生活中，也有很多的场合需要对多种情况进行分类：比如区分病人患了哪种类型的疾病；区分不同种类的蔬菜等。\n\n求解多类别问题可以采用二元分类的思想，将多类问题分解多多个二元分类问题，然后再分别求权值，最终分到最高权值的类别去。\n\n如图五的多类别问题，我们可以分解成图六的多个二元分类问题去求解，最终中间的公共区域，我们通过公式（6）求得最大的概率，哪一类的概率最大，就把这个点归于哪一类。\n>要注意使用软分类（因为直接分类的话中间的公共区域无法区分）\n\n$$\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\begin{equation}\ng(x) = argmax_{k ∈ y} (\\theta w^T_{[k]} x)\n\\end{equation}\n\\tag{6}\n$$\n\n![Multiclass Classification](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bcdd07e44255ec22600bb992b5928f177522630e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-5%20Multiclass%20Classification.png)\n<center> 图五 Multiclass Classification <sup>[2]</sup></center>\n\n![Multiclass Classification Soft Classifiers](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3d035b4130569658df8474312c2d45e632fdfc79/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-6%20Multiclass%20Classification%20Soft%20Classifiers.png)\n<center> 图六 Multiclass Classification Soft Classifiers <sup>[2]</sup></center>\n\n这种算法称作一对多（One-Versus-All），简称为OVA，表示一个类别对其他所有类别。\n\n算法的流程如图七所示\n\n![OVA Decomposition](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/378a48ebf7a50d47599da2296c24c90e68ebd10c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-7%20OVA%20Decomposition.png)\n<center> 图七 OVA Decomposition <sup>[2]</sup></center>\n\n该算法的优点是简单有效，易于类似于logistic函数的二元分类问题扩展成多类别分类；缺点是当类别特别多时，产生了不平衡的现象（如类别特别多，则+1的数据量就很少，大部分都是-1，数据量严重不平衡）。\n\n\n\n<br><br>\n----------------------------------\n## 3. Multiclass via Binary Classification - OVO Algorithm\n上一节的最后提到OVA的方式在类别非常多的情况下，出现了训练数据严重失衡的现象，于是有另外一种方法 一对一(One-Versus-One)算法，简称OVO。同样对图五的问题做多元分类，但是这次我们一次单独考虑2种类型的点，直到两两都进行过分类（分类次数就是组合数 $C$， 例子中就是 $C_4^2 = 6$）。如图八所示。\n\n![OVO](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/15e07a2ab76b1bdb9f632949833e71c801f1371e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-8%20OVO.png)\n<center> 图八 OVO <sup>[3]</sup></center>\n\n\n算法的流程如图九所示\n\n![OVO Decomposition](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0bd54f49a461dcdf1bf0b3f50c11ebdb6401472a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-9%20OVO%20Decomposition.png)\n<center> 图九 OVO <sup>[3]</sup></center>\n\n其优点是简单有效，在做两两对比时，每次使用的不是全部训练数据，而是仅属于当前两类的训练数据，能将所有类似于二元分类的算法扩展成多元分类问题； 缺点是对比次数是 O(N^2) 即：$C_k^2$。 也就是说要分的类别越多，就需要花费更多的存储空间和运算时间。\n\n# Summary\n1. 首先我们对比了目前学到的Linear Models，由于Linear Classification的NP Hard求解问题，我们分析，最终发现可以使用Logistic Regression 的方法进行求解\n2. 接着我们讨论了Multiclass Classification的 OVA 算法，但是该算法存在问题：类型很多的时候，会出现数据不平衡的问题\n3. 最后在OVA的基础上，我们继续讨论了OVO算法，该算法克服了OVA数据不平衡的问题。但是该算法的缺点是存储空间和运算时间都比较大\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\11\\11 - 1 - Linear Models for Binary Classification (21-35)\n\n[2] 机器学习基石(台湾大学-林轩田)\\11\\11 - 3 - Multiclass via Logistic Regression (14-18)\n\n[3] 机器学习基石(台湾大学-林轩田)\\11\\11 - 4 - Multiclass via Binary Classification (11-35)\n\n<br><br>\n----------------------------------","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-14-9.How can Machine Learn - Linear Model for Classification.md","raw":"---\ntitle: 9.How can Machine Learn? - Linear Model for Classification\ndate: 2017-10-14 12:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n\n# How can Machine Learn? - Linear Model for Classification\n\n>主要讨论 Linear Classification, Linear Reegression, Logistic Regression 在分类问题上的优劣对比，并拓展到多元分类\n\n## 1. Linear Models for Binary Classification\n\n### 1) Analyzing of Three Linear Models\n我们目前学习了Classification, Linear Reegression, Logistic Regression， 这三个模型有很多类似的地方。\n总结如图一，这里引入了s(score)作为得分，$s=w^Tx$\n\n![Linear Models Revisit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0e01f9a531b829ec48a4f3e9970fc737f62f9b56/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-1%20Linear%20Models.png)\n<center> 图一 Linear Models Revisit <sup>[1]</sup></center>\n\n从图中可以看出，Linear Classification的求解是NP-hard问题，其他两个方法都容易求解。并且可以看到他们的Hypothesis 都与 s 有关。所以可以利用这两种模型的算法近似求得二分类问题的最优权值w\n\n\n\n\n### 2) Error Functions Comparison\n\n1.首先对比3种方法的误差方程，如图二所示\n\n![Error Functions Revisit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f0bb56f85acd2e6c8500017ff01411c065136237/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-2%20Error%20Function%20Revisit.png)\n<center> 图二 Error Functions Revisit <sup>[1]</sup></center>\n\n\n\n2.根据图二的错误函数得到图三的关系图。\n\n![Visualizing Error Functions](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/89a29e7078c423404f64048c7ebbef7c33c6f534/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-3%20Visualizing%20Error%20Functions.png)\n<center> 图三 Visualizing Error Functions <sup>[1]</sup></center>\n\n从图三，我们可以得到以下几个结论：\n1. 对于 $err_{0/1}$， 在 $ys > 0$ 和 $ys < 0$ 的值相差很大\n2. $err_{CE}$ 随着 ys的增大而减小，而且无线接近于0，在 $ys > 0$ 的情况下，逐渐毕竟于 $err_{0/1}$\n3. $err_{SQR}$ 是一个凹函数，在 $ys = 1$ 的情况下，与 $err_{0/1}$ 相等\n4. $err_{0/1}$  始终小于 $err_{SQR}$， 绝大部分情况下小于 $err_{CE}$\n5. 虽然$err_{0/1}$  始终小于 $err_{SQR}$， 但是两者的差距较大\n6. 虽然$err_{0/1}$  不是全部情况下小于 $err_{CE}$，但是稍微做调整，可以实现 $err_{0/1}$  始终小于 $err_{CE}$ ，并且这两者的曲线接近，所以我们下面将想办法采用这种方式。\n\nerr为了让 $err_{0/1}$  也始终小于 $err_{CE}$ ，我们稍微做一些调整, 得到scaled CE $err_{SCE}$，如图四所示，这里只是给  $err_{CE}$ 的表达式把对数从 $log_e$ 换成 $log_2$。即从公式（1）换成公式（2）\n\n$$\nerr_{CE} = err(w,x,y) = err_{CE}(s,y) = ln(1+exp(-yw^Tx)) = ln(1+exp(-ys))\n\\tag{1}\n$$\n\n$$\nerr_{SCE} = \\frac{1}{ln2} err_{CE} = log_2(1+exp(-ys))\n\\tag{2}\n$$\n\n![Visualizing Error Functions with Scaled CE](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0277ba5bccfbf510f13974f587508dbebd21bea0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-4%20Visualizing%20Error%20Functions-scaled%20ce.png)\n<center> 图四 Visualizing Error Functions with Scaled CE <sup>[1]</sup></center>\n\n从图四中，我们可以发现，$err_{0/1}$  始终小于 $err_{SQR}$ 和 $err_{CE}$，如公式（3）所示，那么在数据量足够大的情况下，$E_{out}$ 也有类似的情况，如公式（4）所示。再接着，由于之前VC Bound的结论，我们可以得到公式（5）， 所以接下来我们就可以Logistic Regrssion的方法去解决Linear Classification的问题了\n\n$$\nE_{in}^{0/1}(w) \\leq E_{in}^{SCE}(w) = \\frac{1}{ln2} E_{in}^{CE}(w)\n\\tag{3}\n$$\n$$\nE_{out}^{0/1}(w) \\leq E_{out}^{SCE}(w) = \\frac{1}{ln2} E_{out}^{CE}(w)\n\\tag{4}\n$$\n$$\nE_{in}^{0/1}(w) \\leq E_{in}^{0/1}(w) + \\Omega ^{0/1}  \\leq E_{out}^{SCE}(w)  + \\Omega ^{0/1} = \\frac{1}{ln2} E_{out}^{CE}(w)  + \\Omega ^{0/1}\n\\tag{5}\n$$\n\n算法流程一般是在输出空间{-1, +1} 的情况下，通过线性回归和logistic回归相对应的求解方法求出最优的权值 $W_{REG}$；\n将求得的代入公式sign，得到最优假设函数。\n\n\n\n\n<br><br>\n----------------------------------\n## 2. Multiclass via Logistic Regression - OVA Algorithm\n\n实际生活中，也有很多的场合需要对多种情况进行分类：比如区分病人患了哪种类型的疾病；区分不同种类的蔬菜等。\n\n求解多类别问题可以采用二元分类的思想，将多类问题分解多多个二元分类问题，然后再分别求权值，最终分到最高权值的类别去。\n\n如图五的多类别问题，我们可以分解成图六的多个二元分类问题去求解，最终中间的公共区域，我们通过公式（6）求得最大的概率，哪一类的概率最大，就把这个点归于哪一类。\n>要注意使用软分类（因为直接分类的话中间的公共区域无法区分）\n\n$$\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\begin{equation}\ng(x) = argmax_{k ∈ y} (\\theta w^T_{[k]} x)\n\\end{equation}\n\\tag{6}\n$$\n\n![Multiclass Classification](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bcdd07e44255ec22600bb992b5928f177522630e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-5%20Multiclass%20Classification.png)\n<center> 图五 Multiclass Classification <sup>[2]</sup></center>\n\n![Multiclass Classification Soft Classifiers](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3d035b4130569658df8474312c2d45e632fdfc79/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-6%20Multiclass%20Classification%20Soft%20Classifiers.png)\n<center> 图六 Multiclass Classification Soft Classifiers <sup>[2]</sup></center>\n\n这种算法称作一对多（One-Versus-All），简称为OVA，表示一个类别对其他所有类别。\n\n算法的流程如图七所示\n\n![OVA Decomposition](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/378a48ebf7a50d47599da2296c24c90e68ebd10c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-7%20OVA%20Decomposition.png)\n<center> 图七 OVA Decomposition <sup>[2]</sup></center>\n\n该算法的优点是简单有效，易于类似于logistic函数的二元分类问题扩展成多类别分类；缺点是当类别特别多时，产生了不平衡的现象（如类别特别多，则+1的数据量就很少，大部分都是-1，数据量严重不平衡）。\n\n\n\n<br><br>\n----------------------------------\n## 3. Multiclass via Binary Classification - OVO Algorithm\n上一节的最后提到OVA的方式在类别非常多的情况下，出现了训练数据严重失衡的现象，于是有另外一种方法 一对一(One-Versus-One)算法，简称OVO。同样对图五的问题做多元分类，但是这次我们一次单独考虑2种类型的点，直到两两都进行过分类（分类次数就是组合数 $C$， 例子中就是 $C_4^2 = 6$）。如图八所示。\n\n![OVO](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/15e07a2ab76b1bdb9f632949833e71c801f1371e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-8%20OVO.png)\n<center> 图八 OVO <sup>[3]</sup></center>\n\n\n算法的流程如图九所示\n\n![OVO Decomposition](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0bd54f49a461dcdf1bf0b3f50c11ebdb6401472a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-9%20OVO%20Decomposition.png)\n<center> 图九 OVO <sup>[3]</sup></center>\n\n其优点是简单有效，在做两两对比时，每次使用的不是全部训练数据，而是仅属于当前两类的训练数据，能将所有类似于二元分类的算法扩展成多元分类问题； 缺点是对比次数是 O(N^2) 即：$C_k^2$。 也就是说要分的类别越多，就需要花费更多的存储空间和运算时间。\n\n# Summary\n1. 首先我们对比了目前学到的Linear Models，由于Linear Classification的NP Hard求解问题，我们分析，最终发现可以使用Logistic Regression 的方法进行求解\n2. 接着我们讨论了Multiclass Classification的 OVA 算法，但是该算法存在问题：类型很多的时候，会出现数据不平衡的问题\n3. 最后在OVA的基础上，我们继续讨论了OVO算法，该算法克服了OVA数据不平衡的问题。但是该算法的缺点是存储空间和运算时间都比较大\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\11\\11 - 1 - Linear Models for Binary Classification (21-35)\n\n[2] 机器学习基石(台湾大学-林轩田)\\11\\11 - 3 - Multiclass via Logistic Regression (14-18)\n\n[3] 机器学习基石(台湾大学-林轩田)\\11\\11 - 4 - Multiclass via Binary Classification (11-35)\n\n<br><br>\n----------------------------------","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-14-9.How can Machine Learn - Linear Model for Classification","published":1,"updated":"2018-04-14T19:42:06.506Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2eb001irwtjrrjl2cce","content":"<h1 id=\"How-can-Machine-Learn-Linear-Model-for-Classification\"><a href=\"#How-can-Machine-Learn-Linear-Model-for-Classification\" class=\"headerlink\" title=\"How can Machine Learn? - Linear Model for Classification\"></a>How can Machine Learn? - Linear Model for Classification</h1><blockquote>\n<p>主要讨论 Linear Classification, Linear Reegression, Logistic Regression 在分类问题上的优劣对比，并拓展到多元分类</p>\n</blockquote>\n<h2 id=\"1-Linear-Models-for-Binary-Classification\"><a href=\"#1-Linear-Models-for-Binary-Classification\" class=\"headerlink\" title=\"1. Linear Models for Binary Classification\"></a>1. Linear Models for Binary Classification</h2><h3 id=\"1-Analyzing-of-Three-Linear-Models\"><a href=\"#1-Analyzing-of-Three-Linear-Models\" class=\"headerlink\" title=\"1) Analyzing of Three Linear Models\"></a>1) Analyzing of Three Linear Models</h3><p>我们目前学习了Classification, Linear Reegression, Logistic Regression， 这三个模型有很多类似的地方。<br>总结如图一，这里引入了s(score)作为得分，$s=w^Tx$</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0e01f9a531b829ec48a4f3e9970fc737f62f9b56/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-1%20Linear%20Models.png\" alt=\"Linear Models Revisit\"></p>\n<center> 图一 Linear Models Revisit <sup>[1]</sup></center>\n\n<p>从图中可以看出，Linear Classification的求解是NP-hard问题，其他两个方法都容易求解。并且可以看到他们的Hypothesis 都与 s 有关。所以可以利用这两种模型的算法近似求得二分类问题的最优权值w</p>\n<h3 id=\"2-Error-Functions-Comparison\"><a href=\"#2-Error-Functions-Comparison\" class=\"headerlink\" title=\"2) Error Functions Comparison\"></a>2) Error Functions Comparison</h3><p>1.首先对比3种方法的误差方程，如图二所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f0bb56f85acd2e6c8500017ff01411c065136237/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-2%20Error%20Function%20Revisit.png\" alt=\"Error Functions Revisit\"></p>\n<center> 图二 Error Functions Revisit <sup>[1]</sup></center>\n\n\n\n<p>2.根据图二的错误函数得到图三的关系图。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/89a29e7078c423404f64048c7ebbef7c33c6f534/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-3%20Visualizing%20Error%20Functions.png\" alt=\"Visualizing Error Functions\"></p>\n<center> 图三 Visualizing Error Functions <sup>[1]</sup></center>\n\n<p>从图三，我们可以得到以下几个结论：</p>\n<ol>\n<li>对于 $err_{0/1}$， 在 $ys &gt; 0$ 和 $ys &lt; 0$ 的值相差很大</li>\n<li>$err_{CE}$ 随着 ys的增大而减小，而且无线接近于0，在 $ys &gt; 0$ 的情况下，逐渐毕竟于 $err_{0/1}$</li>\n<li>$err_{SQR}$ 是一个凹函数，在 $ys = 1$ 的情况下，与 $err_{0/1}$ 相等</li>\n<li>$err_{0/1}$  始终小于 $err_{SQR}$， 绝大部分情况下小于 $err_{CE}$</li>\n<li>虽然$err_{0/1}$  始终小于 $err_{SQR}$， 但是两者的差距较大</li>\n<li>虽然$err_{0/1}$  不是全部情况下小于 $err_{CE}$，但是稍微做调整，可以实现 $err_{0/1}$  始终小于 $err_{CE}$ ，并且这两者的曲线接近，所以我们下面将想办法采用这种方式。</li>\n</ol>\n<p>err为了让 $err_{0/1}$  也始终小于 $err_{CE}$ ，我们稍微做一些调整, 得到scaled CE $err_{SCE}$，如图四所示，这里只是给  $err_{CE}$ 的表达式把对数从 $log_e$ 换成 $log_2$。即从公式（1）换成公式（2）</p>\n<script type=\"math/tex; mode=display\">\nerr_{CE} = err(w,x,y) = err_{CE}(s,y) = ln(1+exp(-yw^Tx)) = ln(1+exp(-ys))\n\\tag{1}</script><script type=\"math/tex; mode=display\">\nerr_{SCE} = \\frac{1}{ln2} err_{CE} = log_2(1+exp(-ys))\n\\tag{2}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0277ba5bccfbf510f13974f587508dbebd21bea0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-4%20Visualizing%20Error%20Functions-scaled%20ce.png\" alt=\"Visualizing Error Functions with Scaled CE\"></p>\n<center> 图四 Visualizing Error Functions with Scaled CE <sup>[1]</sup></center>\n\n<p>从图四中，我们可以发现，$err_{0/1}$  始终小于 $err_{SQR}$ 和 $err_{CE}$，如公式（3）所示，那么在数据量足够大的情况下，$E_{out}$ 也有类似的情况，如公式（4）所示。再接着，由于之前VC Bound的结论，我们可以得到公式（5）， 所以接下来我们就可以Logistic Regrssion的方法去解决Linear Classification的问题了</p>\n<script type=\"math/tex; mode=display\">\nE_{in}^{0/1}(w) \\leq E_{in}^{SCE}(w) = \\frac{1}{ln2} E_{in}^{CE}(w)\n\\tag{3}</script><script type=\"math/tex; mode=display\">\nE_{out}^{0/1}(w) \\leq E_{out}^{SCE}(w) = \\frac{1}{ln2} E_{out}^{CE}(w)\n\\tag{4}</script><script type=\"math/tex; mode=display\">\nE_{in}^{0/1}(w) \\leq E_{in}^{0/1}(w) + \\Omega ^{0/1}  \\leq E_{out}^{SCE}(w)  + \\Omega ^{0/1} = \\frac{1}{ln2} E_{out}^{CE}(w)  + \\Omega ^{0/1}\n\\tag{5}</script><p>算法流程一般是在输出空间{-1, +1} 的情况下，通过线性回归和logistic回归相对应的求解方法求出最优的权值 $W_{REG}$；<br>将求得的代入公式sign，得到最优假设函数。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Multiclass-via-Logistic-Regression-OVA-Algorithm\"><a href=\"#2-Multiclass-via-Logistic-Regression-OVA-Algorithm\" class=\"headerlink\" title=\"2. Multiclass via Logistic Regression - OVA Algorithm\"></a>2. Multiclass via Logistic Regression - OVA Algorithm</h2><p>实际生活中，也有很多的场合需要对多种情况进行分类：比如区分病人患了哪种类型的疾病；区分不同种类的蔬菜等。</p>\n<p>求解多类别问题可以采用二元分类的思想，将多类问题分解多多个二元分类问题，然后再分别求权值，最终分到最高权值的类别去。</p>\n<p>如图五的多类别问题，我们可以分解成图六的多个二元分类问题去求解，最终中间的公共区域，我们通过公式（6）求得最大的概率，哪一类的概率最大，就把这个点归于哪一类。</p>\n<blockquote>\n<p>要注意使用软分类（因为直接分类的话中间的公共区域无法区分）</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\begin{equation}\ng(x) = argmax_{k ∈ y} (\\theta w^T_{[k]} x)\n\\end{equation}\n\\tag{6}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bcdd07e44255ec22600bb992b5928f177522630e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-5%20Multiclass%20Classification.png\" alt=\"Multiclass Classification\"></p>\n<center> 图五 Multiclass Classification <sup>[2]</sup></center>\n\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3d035b4130569658df8474312c2d45e632fdfc79/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-6%20Multiclass%20Classification%20Soft%20Classifiers.png\" alt=\"Multiclass Classification Soft Classifiers\"></p>\n<center> 图六 Multiclass Classification Soft Classifiers <sup>[2]</sup></center>\n\n<p>这种算法称作一对多（One-Versus-All），简称为OVA，表示一个类别对其他所有类别。</p>\n<p>算法的流程如图七所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/378a48ebf7a50d47599da2296c24c90e68ebd10c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-7%20OVA%20Decomposition.png\" alt=\"OVA Decomposition\"></p>\n<center> 图七 OVA Decomposition <sup>[2]</sup></center>\n\n<p>该算法的优点是简单有效，易于类似于logistic函数的二元分类问题扩展成多类别分类；缺点是当类别特别多时，产生了不平衡的现象（如类别特别多，则+1的数据量就很少，大部分都是-1，数据量严重不平衡）。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Multiclass-via-Binary-Classification-OVO-Algorithm\"><a href=\"#3-Multiclass-via-Binary-Classification-OVO-Algorithm\" class=\"headerlink\" title=\"3. Multiclass via Binary Classification - OVO Algorithm\"></a>3. Multiclass via Binary Classification - OVO Algorithm</h2><p>上一节的最后提到OVA的方式在类别非常多的情况下，出现了训练数据严重失衡的现象，于是有另外一种方法 一对一(One-Versus-One)算法，简称OVO。同样对图五的问题做多元分类，但是这次我们一次单独考虑2种类型的点，直到两两都进行过分类（分类次数就是组合数 $C$， 例子中就是 $C_4^2 = 6$）。如图八所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/15e07a2ab76b1bdb9f632949833e71c801f1371e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-8%20OVO.png\" alt=\"OVO\"></p>\n<center> 图八 OVO <sup>[3]</sup></center>\n\n\n<p>算法的流程如图九所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0bd54f49a461dcdf1bf0b3f50c11ebdb6401472a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-9%20OVO%20Decomposition.png\" alt=\"OVO Decomposition\"></p>\n<center> 图九 OVO <sup>[3]</sup></center>\n\n<p>其优点是简单有效，在做两两对比时，每次使用的不是全部训练数据，而是仅属于当前两类的训练数据，能将所有类似于二元分类的算法扩展成多元分类问题； 缺点是对比次数是 O(N^2) 即：$C_k^2$。 也就是说要分的类别越多，就需要花费更多的存储空间和运算时间。</p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先我们对比了目前学到的Linear Models，由于Linear Classification的NP Hard求解问题，我们分析，最终发现可以使用Logistic Regression 的方法进行求解</li>\n<li>接着我们讨论了Multiclass Classification的 OVA 算法，但是该算法存在问题：类型很多的时候，会出现数据不平衡的问题</li>\n<li>最后在OVA的基础上，我们继续讨论了OVO算法，该算法克服了OVA数据不平衡的问题。但是该算法的缺点是存储空间和运算时间都比较大</li>\n</ol>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\11\\11 - 1 - Linear Models for Binary Classification (21-35)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\11\\11 - 3 - Multiclass via Logistic Regression (14-18)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\11\\11 - 4 - Multiclass via Binary Classification (11-35)</p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Linear-Model-for-Classification\"><a href=\"#How-can-Machine-Learn-Linear-Model-for-Classification\" class=\"headerlink\" title=\"How can Machine Learn? - Linear Model for Classification\"></a>How can Machine Learn? - Linear Model for Classification</h1><blockquote>\n<p>主要讨论 Linear Classification, Linear Reegression, Logistic Regression 在分类问题上的优劣对比，并拓展到多元分类</p>\n</blockquote>\n<h2 id=\"1-Linear-Models-for-Binary-Classification\"><a href=\"#1-Linear-Models-for-Binary-Classification\" class=\"headerlink\" title=\"1. Linear Models for Binary Classification\"></a>1. Linear Models for Binary Classification</h2><h3 id=\"1-Analyzing-of-Three-Linear-Models\"><a href=\"#1-Analyzing-of-Three-Linear-Models\" class=\"headerlink\" title=\"1) Analyzing of Three Linear Models\"></a>1) Analyzing of Three Linear Models</h3><p>我们目前学习了Classification, Linear Reegression, Logistic Regression， 这三个模型有很多类似的地方。<br>总结如图一，这里引入了s(score)作为得分，$s=w^Tx$</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0e01f9a531b829ec48a4f3e9970fc737f62f9b56/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-1%20Linear%20Models.png\" alt=\"Linear Models Revisit\"></p>\n<center> 图一 Linear Models Revisit <sup>[1]</sup></center>\n\n<p>从图中可以看出，Linear Classification的求解是NP-hard问题，其他两个方法都容易求解。并且可以看到他们的Hypothesis 都与 s 有关。所以可以利用这两种模型的算法近似求得二分类问题的最优权值w</p>\n<h3 id=\"2-Error-Functions-Comparison\"><a href=\"#2-Error-Functions-Comparison\" class=\"headerlink\" title=\"2) Error Functions Comparison\"></a>2) Error Functions Comparison</h3><p>1.首先对比3种方法的误差方程，如图二所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f0bb56f85acd2e6c8500017ff01411c065136237/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-2%20Error%20Function%20Revisit.png\" alt=\"Error Functions Revisit\"></p>\n<center> 图二 Error Functions Revisit <sup>[1]</sup></center>\n\n\n\n<p>2.根据图二的错误函数得到图三的关系图。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/89a29e7078c423404f64048c7ebbef7c33c6f534/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-3%20Visualizing%20Error%20Functions.png\" alt=\"Visualizing Error Functions\"></p>\n<center> 图三 Visualizing Error Functions <sup>[1]</sup></center>\n\n<p>从图三，我们可以得到以下几个结论：</p>\n<ol>\n<li>对于 $err_{0/1}$， 在 $ys &gt; 0$ 和 $ys &lt; 0$ 的值相差很大</li>\n<li>$err_{CE}$ 随着 ys的增大而减小，而且无线接近于0，在 $ys &gt; 0$ 的情况下，逐渐毕竟于 $err_{0/1}$</li>\n<li>$err_{SQR}$ 是一个凹函数，在 $ys = 1$ 的情况下，与 $err_{0/1}$ 相等</li>\n<li>$err_{0/1}$  始终小于 $err_{SQR}$， 绝大部分情况下小于 $err_{CE}$</li>\n<li>虽然$err_{0/1}$  始终小于 $err_{SQR}$， 但是两者的差距较大</li>\n<li>虽然$err_{0/1}$  不是全部情况下小于 $err_{CE}$，但是稍微做调整，可以实现 $err_{0/1}$  始终小于 $err_{CE}$ ，并且这两者的曲线接近，所以我们下面将想办法采用这种方式。</li>\n</ol>\n<p>err为了让 $err_{0/1}$  也始终小于 $err_{CE}$ ，我们稍微做一些调整, 得到scaled CE $err_{SCE}$，如图四所示，这里只是给  $err_{CE}$ 的表达式把对数从 $log_e$ 换成 $log_2$。即从公式（1）换成公式（2）</p>\n<script type=\"math/tex; mode=display\">\nerr_{CE} = err(w,x,y) = err_{CE}(s,y) = ln(1+exp(-yw^Tx)) = ln(1+exp(-ys))\n\\tag{1}</script><script type=\"math/tex; mode=display\">\nerr_{SCE} = \\frac{1}{ln2} err_{CE} = log_2(1+exp(-ys))\n\\tag{2}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0277ba5bccfbf510f13974f587508dbebd21bea0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-4%20Visualizing%20Error%20Functions-scaled%20ce.png\" alt=\"Visualizing Error Functions with Scaled CE\"></p>\n<center> 图四 Visualizing Error Functions with Scaled CE <sup>[1]</sup></center>\n\n<p>从图四中，我们可以发现，$err_{0/1}$  始终小于 $err_{SQR}$ 和 $err_{CE}$，如公式（3）所示，那么在数据量足够大的情况下，$E_{out}$ 也有类似的情况，如公式（4）所示。再接着，由于之前VC Bound的结论，我们可以得到公式（5）， 所以接下来我们就可以Logistic Regrssion的方法去解决Linear Classification的问题了</p>\n<script type=\"math/tex; mode=display\">\nE_{in}^{0/1}(w) \\leq E_{in}^{SCE}(w) = \\frac{1}{ln2} E_{in}^{CE}(w)\n\\tag{3}</script><script type=\"math/tex; mode=display\">\nE_{out}^{0/1}(w) \\leq E_{out}^{SCE}(w) = \\frac{1}{ln2} E_{out}^{CE}(w)\n\\tag{4}</script><script type=\"math/tex; mode=display\">\nE_{in}^{0/1}(w) \\leq E_{in}^{0/1}(w) + \\Omega ^{0/1}  \\leq E_{out}^{SCE}(w)  + \\Omega ^{0/1} = \\frac{1}{ln2} E_{out}^{CE}(w)  + \\Omega ^{0/1}\n\\tag{5}</script><p>算法流程一般是在输出空间{-1, +1} 的情况下，通过线性回归和logistic回归相对应的求解方法求出最优的权值 $W_{REG}$；<br>将求得的代入公式sign，得到最优假设函数。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Multiclass-via-Logistic-Regression-OVA-Algorithm\"><a href=\"#2-Multiclass-via-Logistic-Regression-OVA-Algorithm\" class=\"headerlink\" title=\"2. Multiclass via Logistic Regression - OVA Algorithm\"></a>2. Multiclass via Logistic Regression - OVA Algorithm</h2><p>实际生活中，也有很多的场合需要对多种情况进行分类：比如区分病人患了哪种类型的疾病；区分不同种类的蔬菜等。</p>\n<p>求解多类别问题可以采用二元分类的思想，将多类问题分解多多个二元分类问题，然后再分别求权值，最终分到最高权值的类别去。</p>\n<p>如图五的多类别问题，我们可以分解成图六的多个二元分类问题去求解，最终中间的公共区域，我们通过公式（6）求得最大的概率，哪一类的概率最大，就把这个点归于哪一类。</p>\n<blockquote>\n<p>要注意使用软分类（因为直接分类的话中间的公共区域无法区分）</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\begin{equation}\ng(x) = argmax_{k ∈ y} (\\theta w^T_{[k]} x)\n\\end{equation}\n\\tag{6}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bcdd07e44255ec22600bb992b5928f177522630e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-5%20Multiclass%20Classification.png\" alt=\"Multiclass Classification\"></p>\n<center> 图五 Multiclass Classification <sup>[2]</sup></center>\n\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3d035b4130569658df8474312c2d45e632fdfc79/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-6%20Multiclass%20Classification%20Soft%20Classifiers.png\" alt=\"Multiclass Classification Soft Classifiers\"></p>\n<center> 图六 Multiclass Classification Soft Classifiers <sup>[2]</sup></center>\n\n<p>这种算法称作一对多（One-Versus-All），简称为OVA，表示一个类别对其他所有类别。</p>\n<p>算法的流程如图七所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/378a48ebf7a50d47599da2296c24c90e68ebd10c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-7%20OVA%20Decomposition.png\" alt=\"OVA Decomposition\"></p>\n<center> 图七 OVA Decomposition <sup>[2]</sup></center>\n\n<p>该算法的优点是简单有效，易于类似于logistic函数的二元分类问题扩展成多类别分类；缺点是当类别特别多时，产生了不平衡的现象（如类别特别多，则+1的数据量就很少，大部分都是-1，数据量严重不平衡）。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Multiclass-via-Binary-Classification-OVO-Algorithm\"><a href=\"#3-Multiclass-via-Binary-Classification-OVO-Algorithm\" class=\"headerlink\" title=\"3. Multiclass via Binary Classification - OVO Algorithm\"></a>3. Multiclass via Binary Classification - OVO Algorithm</h2><p>上一节的最后提到OVA的方式在类别非常多的情况下，出现了训练数据严重失衡的现象，于是有另外一种方法 一对一(One-Versus-One)算法，简称OVO。同样对图五的问题做多元分类，但是这次我们一次单独考虑2种类型的点，直到两两都进行过分类（分类次数就是组合数 $C$， 例子中就是 $C_4^2 = 6$）。如图八所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/15e07a2ab76b1bdb9f632949833e71c801f1371e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-8%20OVO.png\" alt=\"OVO\"></p>\n<center> 图八 OVO <sup>[3]</sup></center>\n\n\n<p>算法的流程如图九所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0bd54f49a461dcdf1bf0b3f50c11ebdb6401472a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-9%20OVO%20Decomposition.png\" alt=\"OVO Decomposition\"></p>\n<center> 图九 OVO <sup>[3]</sup></center>\n\n<p>其优点是简单有效，在做两两对比时，每次使用的不是全部训练数据，而是仅属于当前两类的训练数据，能将所有类似于二元分类的算法扩展成多元分类问题； 缺点是对比次数是 O(N^2) 即：$C_k^2$。 也就是说要分的类别越多，就需要花费更多的存储空间和运算时间。</p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先我们对比了目前学到的Linear Models，由于Linear Classification的NP Hard求解问题，我们分析，最终发现可以使用Logistic Regression 的方法进行求解</li>\n<li>接着我们讨论了Multiclass Classification的 OVA 算法，但是该算法存在问题：类型很多的时候，会出现数据不平衡的问题</li>\n<li>最后在OVA的基础上，我们继续讨论了OVO算法，该算法克服了OVA数据不平衡的问题。但是该算法的缺点是存储空间和运算时间都比较大</li>\n</ol>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\11\\11 - 1 - Linear Models for Binary Classification (21-35)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\11\\11 - 3 - Multiclass via Logistic Regression (14-18)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\11\\11 - 4 - Multiclass via Binary Classification (11-35)</p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"7.How can Machine Learn? - Linear Regression","date":"2017-10-11T04:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# How can Machine Learn? - Linear Regression\n\n\n## 1. What is Regression\n\n\n机器学习算法一般是这样一个步骤：\n\n1. 对于一个问题，用数学语言来描述它，然后建立模型，e.g., 回归模型或者分类模型；\n\n2. 建立代价函数: Cost Function, 用最大似然、最大后验概率或者最小化分类误差等等数，也就是一个最优化问题。找到最优化问题的解，也就是能拟合我们的数据的最好的模型参数；\n\n3. 求解这个代价函数，找到最优解。求解也就分很多种情况了：\n\n   1). 如果这个优化函数存在解析解。例如我们求最值一般是对代价函数求导，找到导数为0的点，也就是最大值或者最小值的地方了。如果代价函数能简单求导，并且求导后为0的式子存在解析解，那么我们就可以直接得到最优的参数了: Gradient Descent。\n\n   2). 如果式子很难求导，例如函数里面存在隐含的变量或者变量相互间存在耦合，也就互相依赖的情况。或者求导后式子得不到解释解，例如未知参数的个数大于已知方程组的个数等。这时候我们就需要借助迭代算法来一步一步找到最有解了。\n\n   3). 另外需要考虑的情况是，如果代价函数是凸函数，那么就存在全局最优解。\n\n\n回归(Regression)问题与分类(Classification)问题类似，关键的区别在于，Regression的输出是实数,是一个范围，所以不能提前确定所有输出值；而Classification的输出是确定的值。换个角度说，假如输出足够多的数据的时候：如果输出的值是连续的，这个问题叫做Regression，而如果输出的值是离散的，那么这个问题叫做Classification\n\n\n用数学符号表示如果公式（1）所示。\n\n$$\n\\begin{align}\nRegression : &f(x) = \\left[ value1, value2 \\right]  \\\\\nClassification: &f(x) = \\{ value_1, value_2, ..., value_n \\}\n\\end{align}\n\\tag{$1$}\n$$\n\n<br><br>\n----------------------------------\n\n## 2. Linear Regression\n### 1) Introduction of Linear Regression\n线性回归问题同样的表示各个属性（Attribution）对最终结果的贡献：也就是说每个属性乘以对应的权值，最后再加上一定的偏移，如公式（2）所示\n\n$$\ny = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n\n\\tag{$2$}\n$$\n\n如果给第一项的偏移 $w_0$ 加上属性值 $x_0$，并把单独的输出点$y$用向量进行表示，那么公式（2）就变成了公式（3）（这一步纯粹是为了表示方便）。\n\nHypothesis h(x):\n$$\n\\begin{align}\nh(x) &= w_0 \\cdot x_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n  \\quad(x_0 = 1) \\\\\n     &= w^T \\cdot x_i \\\\\n     &= W^T \\cdot X\n\\end{align}\n\\tag{$3$}\n$$\n从公式(3)的可以看出，与二元分类假设函数的表示只差了一个取正负号的函数 $sign()$。\n\n\n### 2) Error Measurement of Linear Regression\n线性回归问题的错误如图一所示。\n![Error Measurement](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/de4c8798497f08b40cbe86cf0cc665ceaa6d54ff/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-1%20Linear%20Regression.png)\n<center> 图一 Error Measurement <sup>[1]</sup></center>\n\n\n上一章中，提到了回归问题我们用平方误差来表示（其实还有RMSE， R2等方法，老师上课只讲了平方误差，为了说明方便，我们只写这个）\n根据图一的信息，很明显得到公式（4）（5）。\n\n$$\nE_{in}(w)) = \\frac{1}{N} \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2\n\\tag{$4$}\n$$\n\n$$\nE_{out}(w)) = \\epsilon \\cdot \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2\n\\tag{$5$}\n$$\nVC Bound可以约束各种情况的学习模型，当然回归类型的模型也被也受此约束，只需要寻找足够小 $E_{in}(w)$ 便可以满足 $E_{out}(w)$ 小的需求。\n\n\n\n### 3) Linear Regression Algorithm\n刚刚上面的Error Meansurement中，我们提到了VC Bound中，在训练样本足够的情况下，我们要用特定的算法，来找到最小的 $E_{in}(w)$，所以把上面公式（4）的pointwise error measurement 通过矩阵做调整得到公式（6）。\n\n$$\n\\begin{align}\nE_{in}(w)) &= \\frac{1}{N} \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2 \\\\\n           &= \\frac{1}{N} \\sum\\limits_{n=1}^{N}(x_n^Tw - y_n)^2   \\quad\\quad (向量内积，符合交换率) \\\\\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{array}{ccc}\n                x_1^Tw - y_1 & \\\\\n                x_2^Tw - y_2 & \\\\\n                ...            \\\\\n                x_N^Tw - y_N &\n           \\end{array}\n           \\right|\n           \\right|^2               \\quad\\quad(平方求和)\\\\\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{bmatrix}\n                -x_1^T-\\\\\n                -x_1^T-\\\\\n                ...    \\\\\n                -x_1^T-\\\\\n           \\end{bmatrix}\n           \\cdot w\n           \\begin{bmatrix}\n                y_1\\\\\n                y_2\\\\\n                ...\\\\\n                y_n\\\\\n           \\end{bmatrix}\n           \\right|\n           \\right|^2                \\quad\\quad(矩阵平方)\\\\\n\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{array}{ccc}\n                \\underbrace{X^T}_{N \\times (d+1)}\n                \\underbrace{w}_{(d+1)\\times 1}\n                -\n                \\underbrace{y}_{N \\times 1}\n           \\end{array}\n           \\right|\n           \\right|^2 \\\\\n\\end{align}\n\\tag{$6$}\n$$\n\n根据公式（6），那么 $E_{in}(w)$的最小值如公式（7）所示。\n\n$$\nmin_w E_{in}(w) = min_w \\frac{1}{N} \\left|\\left| X^Tw - y \\right|\\right|^2\n\\tag{$7$}\n$$\n\n$E_{in}(w)$的变化图如图二所示，可以看出这是一个连续（continuous）、可微（differentiable）的凸（convex）函数。最小值就是山谷的点（即转折点），并且对这个点求导的值为0。\n\n![Ein Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/edb6705ac1e3e9565cb69b9b170c1de25f86f987/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-2%20Linear%20Regression%20Ein.png)\n\n<center> 图二 Ein Curve <sup>[1]</sup></center>\n\n所以接下来，我们需要对$E_{in}(w)$求导并且找到一个值 $W_{LIN}$，使得 $\\nabla E_{in}(w)$ 的值为0。\n1. 首先我们需要考虑两种情况：①只有一个w； ②w为向量\n2. 首先我们把公式（6）平方去掉\n3. 只有一个w的时候，就按照正常的求导方式求导即可，得到公式（7）\n4. 当w为向量的时候，我们需要对其中一个进行转置，然后再求导，得到公式（8）\n5. 根据公式（7）（8）可以看出，两种情况的求导的结果非常相似，可以用公式（9）表示\n6. 最后我们令公式（9）的值为0，得到我们需要的 $W_{LIN}$ 如公式（10）所示,其中 $X^\\dagger$ 叫做[矩阵X的伪逆矩阵（pseudo-inverse）](http://blog.sina.com.cn/s/blog_890c6aa30101cn6t.html)\n\n$$\n\\begin{align}\none \\quad w : E_{in}(w) &= \\frac{1}{N} \\left( (X^T)X w^2 - 2 \\cdot X^T yw  + y^2 \\right) \\\\\n       \\nabla E_{in}(w) &= \\frac{1}{N} \\left( 2(X^T)X w - 2 \\cdot X^T y \\right)\n\\end{align}\n\\tag{$7$}\n$$\n\n$$\n\\begin{align}\nvector \\quad w :  E_{in}(w) &= \\frac{1}{N} \\left( (X^T)X w^T w - 2 \\cdot X^T y w^T  + y^2 \\right) \\\\\n          \\nabla E_{in}(w)  &= \\frac{1}{N} \\left( 2(X^T)X w - 2 \\cdot X^T y \\right)\n\\end{align}\n\\tag{$8$}\n$$\n\n$$\n\\nabla E_{in}(w)  = \\frac{2}{N} \\left( (X^T)X w - X^T y \\right)\n\\tag{$9$}\n$$\n\n$$\n\\begin{align}\nW_{LIN} &= \\underbrace{(X^T X) ^ {-1} X^T }_{X^\\dagger} y \\\\\n        &= \\underbrace{X ^\\dagger }_{(d+1)\\times N)}\n           \\underbrace{y}_{N \\times 1}\n\\end{align}\n\\tag{$10$}\n$$\n\n### 4) Generalization Issue\n上一节中，我们得到了最佳的 w 的解，但是，哪个是最小的 $E_{in}$， 实际上我们最需要的是使得 $E_{out}$最小，而我们这一节就是要讨论之前关于VC Bound保证了 $E_{in} \\approx E_{out}$ 是否依然适用于线性回归问题。（中间解释过程较复杂，后续补充。）\n\n结论就是得到了 $E_{in} E_{out}$关于 noise level的方程，如公式（11）（12），曲线图如图三所示。\n\n$$\nE_{in}(w)  = noise \\quad level  \\cdot (1- \\frac{d+1}{N})\n\\tag{$11$}\n$$\n\n$$\nE_{out}(w)  = noise \\quad level  \\cdot (1+ \\frac{d+1}{N})\n\\tag{$12$}\n$$\n\n![Ein Eout Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/01235bfa1a94084fee01577e205a27839be75228/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-3%20Ein%20Eout.png)\n<center> 图三 Ein Eout Curve<sup>[2]</sup></center>\n\n\n可以看出在N趋于无穷大时，与两者都会趋近于noise level的值，即 $\\sigma^2$ 泛化错误之间的差距：$\\frac{2(d+1)}{N}$ 。\n\n至此可以表明在线性回归中可以寻找到很好的 $E_{out}$，因此线性回归可以学习。\n\n\n\n\n\n### 5) Linear Regression for Binary Classification\n这一节，我们主要讨论能否通过求解线性回归的方式求二元分类问题\n\n首先对比Linear Classification 和 Linear Regression， 如图四\n\n![Linear Classification and Linear Regression](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d4a67d82122ef663fb09d41c3b1862d897536e2a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-4%20Linear%20Regression%20for%20Binary%20Classification.png)\n<center> 图四 Linear Classification and Linear Regression<sup>[3]</sup></center>\n<br>\n\n从求解问题的难度考虑，二元分类的求解是一个NP难问题，只能近似求解，而线性回归求解方便，程序编写也简单。\n\n直觉告诉我们:因为二元分类的输出空间{-1，+1}属于线性回归的输出空间，即 $\\{-1, +1\\}∈ R$。其中数据集的标记大于零的表示+1，小于零的表示-1，通过线性回归求得的解析解 $W_{LIN}$，直接得出最优假设 $g(x) = sign( W_{LIN} x)$ 。\n\n下面给出证明：\n> 入手点：误差大小\n\n首先对比两种方法的曲线图，如图五所示。\n![Relations of Two Errors](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/341baa79de829072a35424870d65421a0bc9f9ba/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-5%20Relation%20of%20Two%20Errors.png)\n<center> 图五 Relations of Two Errors<sup>[3]</sup></center>\n<br>\n\n可以发现，无论期望的y为-1还是+1， $err_{sqr}$ 始终大于 $err_{0/1}$，即公式（13），加上我们之前有的 $E_{out} \\approx E_{in}$，那么我们可以得到公式（14）。\n\n\n$$\nerr_{0/1} \\leq err_{sqr}\n\\tag{$13$}\n$$\n\n\n$$\n\\begin{align}\nClassification \\quad E_{out}(w)\n&\\leq Classification \\quad E_{in}(w)  + \\sqrt{\\frac{8}{N} \\ln\\left( \\frac{4 (2N)^{d_{vc}}}{\\delta} \\right)} \\\\\n&\\leq Regression \\quad\\quad E_{in}(w)  + \\sqrt{\\frac{8}{N} \\ln\\left( \\frac{4 (2N)^{d_{vc}}}{\\delta} \\right)}\n\\end{align}\n\\tag{$14$}\n$$\n\n\n所以我们可以通过使用 Linear Regression 的方法来求解Linear Classification的问题，虽然通过这种方法，得到的误差可能更大，但却是一种更有效率的求解方式。在实际运用中，一般都将通过线性回归求得的解析解作为PLA或者pocket的初始值，达到快速求解的目的 。\n<br><br>\n----------------------------------\n\n\n# Summary\n1. 首先介绍了Regression。\n2. 然后通过例子引入Linear Regression，分别介绍Linear Regression的误差方程，算法流程。\n3. 接着我们引入VC Bound 解释了Linear Regression为什么可以实现学习。\n4. 最后我们应用Linear Regression的算法到Classification的问题中：虽然会损失一定的准确度，但是效率提升很大。\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\9\\9 - 1 - Linear Regression Problem (10-08)\n\n[2] 机器学习基石(台湾大学-林轩田)\\9\\9 - 3 - Generalization Issue (20-34)\n\n[3] 机器学习基石(台湾大学-林轩田)\\9\\9 - 4 - Linear Regression for Binary Classification (11-23)\n\n\n<br>\n<br>\n---------------------------------------------","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-11-7.How can Machine Learn - Linear Regression.md","raw":"---\ntitle: 7.How can Machine Learn? - Linear Regression\ndate: 2017-10-11 12:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# How can Machine Learn? - Linear Regression\n\n\n## 1. What is Regression\n\n\n机器学习算法一般是这样一个步骤：\n\n1. 对于一个问题，用数学语言来描述它，然后建立模型，e.g., 回归模型或者分类模型；\n\n2. 建立代价函数: Cost Function, 用最大似然、最大后验概率或者最小化分类误差等等数，也就是一个最优化问题。找到最优化问题的解，也就是能拟合我们的数据的最好的模型参数；\n\n3. 求解这个代价函数，找到最优解。求解也就分很多种情况了：\n\n   1). 如果这个优化函数存在解析解。例如我们求最值一般是对代价函数求导，找到导数为0的点，也就是最大值或者最小值的地方了。如果代价函数能简单求导，并且求导后为0的式子存在解析解，那么我们就可以直接得到最优的参数了: Gradient Descent。\n\n   2). 如果式子很难求导，例如函数里面存在隐含的变量或者变量相互间存在耦合，也就互相依赖的情况。或者求导后式子得不到解释解，例如未知参数的个数大于已知方程组的个数等。这时候我们就需要借助迭代算法来一步一步找到最有解了。\n\n   3). 另外需要考虑的情况是，如果代价函数是凸函数，那么就存在全局最优解。\n\n\n回归(Regression)问题与分类(Classification)问题类似，关键的区别在于，Regression的输出是实数,是一个范围，所以不能提前确定所有输出值；而Classification的输出是确定的值。换个角度说，假如输出足够多的数据的时候：如果输出的值是连续的，这个问题叫做Regression，而如果输出的值是离散的，那么这个问题叫做Classification\n\n\n用数学符号表示如果公式（1）所示。\n\n$$\n\\begin{align}\nRegression : &f(x) = \\left[ value1, value2 \\right]  \\\\\nClassification: &f(x) = \\{ value_1, value_2, ..., value_n \\}\n\\end{align}\n\\tag{$1$}\n$$\n\n<br><br>\n----------------------------------\n\n## 2. Linear Regression\n### 1) Introduction of Linear Regression\n线性回归问题同样的表示各个属性（Attribution）对最终结果的贡献：也就是说每个属性乘以对应的权值，最后再加上一定的偏移，如公式（2）所示\n\n$$\ny = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n\n\\tag{$2$}\n$$\n\n如果给第一项的偏移 $w_0$ 加上属性值 $x_0$，并把单独的输出点$y$用向量进行表示，那么公式（2）就变成了公式（3）（这一步纯粹是为了表示方便）。\n\nHypothesis h(x):\n$$\n\\begin{align}\nh(x) &= w_0 \\cdot x_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n  \\quad(x_0 = 1) \\\\\n     &= w^T \\cdot x_i \\\\\n     &= W^T \\cdot X\n\\end{align}\n\\tag{$3$}\n$$\n从公式(3)的可以看出，与二元分类假设函数的表示只差了一个取正负号的函数 $sign()$。\n\n\n### 2) Error Measurement of Linear Regression\n线性回归问题的错误如图一所示。\n![Error Measurement](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/de4c8798497f08b40cbe86cf0cc665ceaa6d54ff/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-1%20Linear%20Regression.png)\n<center> 图一 Error Measurement <sup>[1]</sup></center>\n\n\n上一章中，提到了回归问题我们用平方误差来表示（其实还有RMSE， R2等方法，老师上课只讲了平方误差，为了说明方便，我们只写这个）\n根据图一的信息，很明显得到公式（4）（5）。\n\n$$\nE_{in}(w)) = \\frac{1}{N} \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2\n\\tag{$4$}\n$$\n\n$$\nE_{out}(w)) = \\epsilon \\cdot \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2\n\\tag{$5$}\n$$\nVC Bound可以约束各种情况的学习模型，当然回归类型的模型也被也受此约束，只需要寻找足够小 $E_{in}(w)$ 便可以满足 $E_{out}(w)$ 小的需求。\n\n\n\n### 3) Linear Regression Algorithm\n刚刚上面的Error Meansurement中，我们提到了VC Bound中，在训练样本足够的情况下，我们要用特定的算法，来找到最小的 $E_{in}(w)$，所以把上面公式（4）的pointwise error measurement 通过矩阵做调整得到公式（6）。\n\n$$\n\\begin{align}\nE_{in}(w)) &= \\frac{1}{N} \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2 \\\\\n           &= \\frac{1}{N} \\sum\\limits_{n=1}^{N}(x_n^Tw - y_n)^2   \\quad\\quad (向量内积，符合交换率) \\\\\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{array}{ccc}\n                x_1^Tw - y_1 & \\\\\n                x_2^Tw - y_2 & \\\\\n                ...            \\\\\n                x_N^Tw - y_N &\n           \\end{array}\n           \\right|\n           \\right|^2               \\quad\\quad(平方求和)\\\\\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{bmatrix}\n                -x_1^T-\\\\\n                -x_1^T-\\\\\n                ...    \\\\\n                -x_1^T-\\\\\n           \\end{bmatrix}\n           \\cdot w\n           \\begin{bmatrix}\n                y_1\\\\\n                y_2\\\\\n                ...\\\\\n                y_n\\\\\n           \\end{bmatrix}\n           \\right|\n           \\right|^2                \\quad\\quad(矩阵平方)\\\\\n\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{array}{ccc}\n                \\underbrace{X^T}_{N \\times (d+1)}\n                \\underbrace{w}_{(d+1)\\times 1}\n                -\n                \\underbrace{y}_{N \\times 1}\n           \\end{array}\n           \\right|\n           \\right|^2 \\\\\n\\end{align}\n\\tag{$6$}\n$$\n\n根据公式（6），那么 $E_{in}(w)$的最小值如公式（7）所示。\n\n$$\nmin_w E_{in}(w) = min_w \\frac{1}{N} \\left|\\left| X^Tw - y \\right|\\right|^2\n\\tag{$7$}\n$$\n\n$E_{in}(w)$的变化图如图二所示，可以看出这是一个连续（continuous）、可微（differentiable）的凸（convex）函数。最小值就是山谷的点（即转折点），并且对这个点求导的值为0。\n\n![Ein Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/edb6705ac1e3e9565cb69b9b170c1de25f86f987/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-2%20Linear%20Regression%20Ein.png)\n\n<center> 图二 Ein Curve <sup>[1]</sup></center>\n\n所以接下来，我们需要对$E_{in}(w)$求导并且找到一个值 $W_{LIN}$，使得 $\\nabla E_{in}(w)$ 的值为0。\n1. 首先我们需要考虑两种情况：①只有一个w； ②w为向量\n2. 首先我们把公式（6）平方去掉\n3. 只有一个w的时候，就按照正常的求导方式求导即可，得到公式（7）\n4. 当w为向量的时候，我们需要对其中一个进行转置，然后再求导，得到公式（8）\n5. 根据公式（7）（8）可以看出，两种情况的求导的结果非常相似，可以用公式（9）表示\n6. 最后我们令公式（9）的值为0，得到我们需要的 $W_{LIN}$ 如公式（10）所示,其中 $X^\\dagger$ 叫做[矩阵X的伪逆矩阵（pseudo-inverse）](http://blog.sina.com.cn/s/blog_890c6aa30101cn6t.html)\n\n$$\n\\begin{align}\none \\quad w : E_{in}(w) &= \\frac{1}{N} \\left( (X^T)X w^2 - 2 \\cdot X^T yw  + y^2 \\right) \\\\\n       \\nabla E_{in}(w) &= \\frac{1}{N} \\left( 2(X^T)X w - 2 \\cdot X^T y \\right)\n\\end{align}\n\\tag{$7$}\n$$\n\n$$\n\\begin{align}\nvector \\quad w :  E_{in}(w) &= \\frac{1}{N} \\left( (X^T)X w^T w - 2 \\cdot X^T y w^T  + y^2 \\right) \\\\\n          \\nabla E_{in}(w)  &= \\frac{1}{N} \\left( 2(X^T)X w - 2 \\cdot X^T y \\right)\n\\end{align}\n\\tag{$8$}\n$$\n\n$$\n\\nabla E_{in}(w)  = \\frac{2}{N} \\left( (X^T)X w - X^T y \\right)\n\\tag{$9$}\n$$\n\n$$\n\\begin{align}\nW_{LIN} &= \\underbrace{(X^T X) ^ {-1} X^T }_{X^\\dagger} y \\\\\n        &= \\underbrace{X ^\\dagger }_{(d+1)\\times N)}\n           \\underbrace{y}_{N \\times 1}\n\\end{align}\n\\tag{$10$}\n$$\n\n### 4) Generalization Issue\n上一节中，我们得到了最佳的 w 的解，但是，哪个是最小的 $E_{in}$， 实际上我们最需要的是使得 $E_{out}$最小，而我们这一节就是要讨论之前关于VC Bound保证了 $E_{in} \\approx E_{out}$ 是否依然适用于线性回归问题。（中间解释过程较复杂，后续补充。）\n\n结论就是得到了 $E_{in} E_{out}$关于 noise level的方程，如公式（11）（12），曲线图如图三所示。\n\n$$\nE_{in}(w)  = noise \\quad level  \\cdot (1- \\frac{d+1}{N})\n\\tag{$11$}\n$$\n\n$$\nE_{out}(w)  = noise \\quad level  \\cdot (1+ \\frac{d+1}{N})\n\\tag{$12$}\n$$\n\n![Ein Eout Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/01235bfa1a94084fee01577e205a27839be75228/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-3%20Ein%20Eout.png)\n<center> 图三 Ein Eout Curve<sup>[2]</sup></center>\n\n\n可以看出在N趋于无穷大时，与两者都会趋近于noise level的值，即 $\\sigma^2$ 泛化错误之间的差距：$\\frac{2(d+1)}{N}$ 。\n\n至此可以表明在线性回归中可以寻找到很好的 $E_{out}$，因此线性回归可以学习。\n\n\n\n\n\n### 5) Linear Regression for Binary Classification\n这一节，我们主要讨论能否通过求解线性回归的方式求二元分类问题\n\n首先对比Linear Classification 和 Linear Regression， 如图四\n\n![Linear Classification and Linear Regression](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d4a67d82122ef663fb09d41c3b1862d897536e2a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-4%20Linear%20Regression%20for%20Binary%20Classification.png)\n<center> 图四 Linear Classification and Linear Regression<sup>[3]</sup></center>\n<br>\n\n从求解问题的难度考虑，二元分类的求解是一个NP难问题，只能近似求解，而线性回归求解方便，程序编写也简单。\n\n直觉告诉我们:因为二元分类的输出空间{-1，+1}属于线性回归的输出空间，即 $\\{-1, +1\\}∈ R$。其中数据集的标记大于零的表示+1，小于零的表示-1，通过线性回归求得的解析解 $W_{LIN}$，直接得出最优假设 $g(x) = sign( W_{LIN} x)$ 。\n\n下面给出证明：\n> 入手点：误差大小\n\n首先对比两种方法的曲线图，如图五所示。\n![Relations of Two Errors](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/341baa79de829072a35424870d65421a0bc9f9ba/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-5%20Relation%20of%20Two%20Errors.png)\n<center> 图五 Relations of Two Errors<sup>[3]</sup></center>\n<br>\n\n可以发现，无论期望的y为-1还是+1， $err_{sqr}$ 始终大于 $err_{0/1}$，即公式（13），加上我们之前有的 $E_{out} \\approx E_{in}$，那么我们可以得到公式（14）。\n\n\n$$\nerr_{0/1} \\leq err_{sqr}\n\\tag{$13$}\n$$\n\n\n$$\n\\begin{align}\nClassification \\quad E_{out}(w)\n&\\leq Classification \\quad E_{in}(w)  + \\sqrt{\\frac{8}{N} \\ln\\left( \\frac{4 (2N)^{d_{vc}}}{\\delta} \\right)} \\\\\n&\\leq Regression \\quad\\quad E_{in}(w)  + \\sqrt{\\frac{8}{N} \\ln\\left( \\frac{4 (2N)^{d_{vc}}}{\\delta} \\right)}\n\\end{align}\n\\tag{$14$}\n$$\n\n\n所以我们可以通过使用 Linear Regression 的方法来求解Linear Classification的问题，虽然通过这种方法，得到的误差可能更大，但却是一种更有效率的求解方式。在实际运用中，一般都将通过线性回归求得的解析解作为PLA或者pocket的初始值，达到快速求解的目的 。\n<br><br>\n----------------------------------\n\n\n# Summary\n1. 首先介绍了Regression。\n2. 然后通过例子引入Linear Regression，分别介绍Linear Regression的误差方程，算法流程。\n3. 接着我们引入VC Bound 解释了Linear Regression为什么可以实现学习。\n4. 最后我们应用Linear Regression的算法到Classification的问题中：虽然会损失一定的准确度，但是效率提升很大。\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\9\\9 - 1 - Linear Regression Problem (10-08)\n\n[2] 机器学习基石(台湾大学-林轩田)\\9\\9 - 3 - Generalization Issue (20-34)\n\n[3] 机器学习基石(台湾大学-林轩田)\\9\\9 - 4 - Linear Regression for Binary Classification (11-23)\n\n\n<br>\n<br>\n---------------------------------------------","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-11-7.How can Machine Learn - Linear Regression","published":1,"updated":"2018-04-14T19:42:06.505Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2ed001mrwtj4w9464e5","content":"<h1 id=\"How-can-Machine-Learn-Linear-Regression\"><a href=\"#How-can-Machine-Learn-Linear-Regression\" class=\"headerlink\" title=\"How can Machine Learn? - Linear Regression\"></a>How can Machine Learn? - Linear Regression</h1><h2 id=\"1-What-is-Regression\"><a href=\"#1-What-is-Regression\" class=\"headerlink\" title=\"1. What is Regression\"></a>1. What is Regression</h2><p>机器学习算法一般是这样一个步骤：</p>\n<ol>\n<li><p>对于一个问题，用数学语言来描述它，然后建立模型，e.g., 回归模型或者分类模型；</p>\n</li>\n<li><p>建立代价函数: Cost Function, 用最大似然、最大后验概率或者最小化分类误差等等数，也就是一个最优化问题。找到最优化问题的解，也就是能拟合我们的数据的最好的模型参数；</p>\n</li>\n<li><p>求解这个代价函数，找到最优解。求解也就分很多种情况了：</p>\n<p>1). 如果这个优化函数存在解析解。例如我们求最值一般是对代价函数求导，找到导数为0的点，也就是最大值或者最小值的地方了。如果代价函数能简单求导，并且求导后为0的式子存在解析解，那么我们就可以直接得到最优的参数了: Gradient Descent。</p>\n<p>2). 如果式子很难求导，例如函数里面存在隐含的变量或者变量相互间存在耦合，也就互相依赖的情况。或者求导后式子得不到解释解，例如未知参数的个数大于已知方程组的个数等。这时候我们就需要借助迭代算法来一步一步找到最有解了。</p>\n<p>3). 另外需要考虑的情况是，如果代价函数是凸函数，那么就存在全局最优解。</p>\n</li>\n</ol>\n<p>回归(Regression)问题与分类(Classification)问题类似，关键的区别在于，Regression的输出是实数,是一个范围，所以不能提前确定所有输出值；而Classification的输出是确定的值。换个角度说，假如输出足够多的数据的时候：如果输出的值是连续的，这个问题叫做Regression，而如果输出的值是离散的，那么这个问题叫做Classification</p>\n<p>用数学符号表示如果公式（1）所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nRegression : &f(x) = \\left[ value1, value2 \\right]  \\\\\nClassification: &f(x) = \\{ value_1, value_2, ..., value_n \\}\n\\end{align}\n\\tag{$1$}</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Linear-Regression\"><a href=\"#2-Linear-Regression\" class=\"headerlink\" title=\"2. Linear Regression\"></a>2. Linear Regression</h2><h3 id=\"1-Introduction-of-Linear-Regression\"><a href=\"#1-Introduction-of-Linear-Regression\" class=\"headerlink\" title=\"1) Introduction of Linear Regression\"></a>1) Introduction of Linear Regression</h3><p>线性回归问题同样的表示各个属性（Attribution）对最终结果的贡献：也就是说每个属性乘以对应的权值，最后再加上一定的偏移，如公式（2）所示</p>\n<script type=\"math/tex; mode=display\">\ny = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n\n\\tag{$2$}</script><p>如果给第一项的偏移 $w_0$ 加上属性值 $x_0$，并把单独的输出点$y$用向量进行表示，那么公式（2）就变成了公式（3）（这一步纯粹是为了表示方便）。</p>\n<p>Hypothesis h(x):</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh(x) &= w_0 \\cdot x_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n  \\quad(x_0 = 1) \\\\\n     &= w^T \\cdot x_i \\\\\n     &= W^T \\cdot X\n\\end{align}\n\\tag{$3$}</script><p>从公式(3)的可以看出，与二元分类假设函数的表示只差了一个取正负号的函数 $sign()$。</p>\n<h3 id=\"2-Error-Measurement-of-Linear-Regression\"><a href=\"#2-Error-Measurement-of-Linear-Regression\" class=\"headerlink\" title=\"2) Error Measurement of Linear Regression\"></a>2) Error Measurement of Linear Regression</h3><p>线性回归问题的错误如图一所示。<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/de4c8798497f08b40cbe86cf0cc665ceaa6d54ff/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-1%20Linear%20Regression.png\" alt=\"Error Measurement\"></p>\n<center> 图一 Error Measurement <sup>[1]</sup></center>\n\n\n<p>上一章中，提到了回归问题我们用平方误差来表示（其实还有RMSE， R2等方法，老师上课只讲了平方误差，为了说明方便，我们只写这个）<br>根据图一的信息，很明显得到公式（4）（5）。</p>\n<script type=\"math/tex; mode=display\">\nE_{in}(w)) = \\frac{1}{N} \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2\n\\tag{$4$}</script><script type=\"math/tex; mode=display\">\nE_{out}(w)) = \\epsilon \\cdot \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2\n\\tag{$5$}</script><p>VC Bound可以约束各种情况的学习模型，当然回归类型的模型也被也受此约束，只需要寻找足够小 $E_{in}(w)$ 便可以满足 $E_{out}(w)$ 小的需求。</p>\n<h3 id=\"3-Linear-Regression-Algorithm\"><a href=\"#3-Linear-Regression-Algorithm\" class=\"headerlink\" title=\"3) Linear Regression Algorithm\"></a>3) Linear Regression Algorithm</h3><p>刚刚上面的Error Meansurement中，我们提到了VC Bound中，在训练样本足够的情况下，我们要用特定的算法，来找到最小的 $E_{in}(w)$，所以把上面公式（4）的pointwise error measurement 通过矩阵做调整得到公式（6）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nE_{in}(w)) &= \\frac{1}{N} \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2 \\\\\n           &= \\frac{1}{N} \\sum\\limits_{n=1}^{N}(x_n^Tw - y_n)^2   \\quad\\quad (向量内积，符合交换率) \\\\\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{array}{ccc}\n                x_1^Tw - y_1 & \\\\\n                x_2^Tw - y_2 & \\\\\n                ...            \\\\\n                x_N^Tw - y_N &\n           \\end{array}\n           \\right|\n           \\right|^2               \\quad\\quad(平方求和)\\\\\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{bmatrix}\n                -x_1^T-\\\\\n                -x_1^T-\\\\\n                ...    \\\\\n                -x_1^T-\\\\\n           \\end{bmatrix}\n           \\cdot w\n           \\begin{bmatrix}\n                y_1\\\\\n                y_2\\\\\n                ...\\\\\n                y_n\\\\\n           \\end{bmatrix}\n           \\right|\n           \\right|^2                \\quad\\quad(矩阵平方)\\\\\n\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{array}{ccc}\n                \\underbrace{X^T}_{N \\times (d+1)}\n                \\underbrace{w}_{(d+1)\\times 1}\n                -\n                \\underbrace{y}_{N \\times 1}\n           \\end{array}\n           \\right|\n           \\right|^2 \\\\\n\\end{align}\n\\tag{$6$}</script><p>根据公式（6），那么 $E_{in}(w)$的最小值如公式（7）所示。</p>\n<script type=\"math/tex; mode=display\">\nmin_w E_{in}(w) = min_w \\frac{1}{N} \\left|\\left| X^Tw - y \\right|\\right|^2\n\\tag{$7$}</script><p>$E_{in}(w)$的变化图如图二所示，可以看出这是一个连续（continuous）、可微（differentiable）的凸（convex）函数。最小值就是山谷的点（即转折点），并且对这个点求导的值为0。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/edb6705ac1e3e9565cb69b9b170c1de25f86f987/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-2%20Linear%20Regression%20Ein.png\" alt=\"Ein Curve\"></p>\n<center> 图二 Ein Curve <sup>[1]</sup></center>\n\n<p>所以接下来，我们需要对$E_{in}(w)$求导并且找到一个值 $W_{LIN}$，使得 $\\nabla E_{in}(w)$ 的值为0。</p>\n<ol>\n<li>首先我们需要考虑两种情况：①只有一个w； ②w为向量</li>\n<li>首先我们把公式（6）平方去掉</li>\n<li>只有一个w的时候，就按照正常的求导方式求导即可，得到公式（7）</li>\n<li>当w为向量的时候，我们需要对其中一个进行转置，然后再求导，得到公式（8）</li>\n<li>根据公式（7）（8）可以看出，两种情况的求导的结果非常相似，可以用公式（9）表示</li>\n<li>最后我们令公式（9）的值为0，得到我们需要的 $W_{LIN}$ 如公式（10）所示,其中 $X^\\dagger$ 叫做<a href=\"http://blog.sina.com.cn/s/blog_890c6aa30101cn6t.html\" target=\"_blank\" rel=\"external\">矩阵X的伪逆矩阵（pseudo-inverse）</a></li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\none \\quad w : E_{in}(w) &= \\frac{1}{N} \\left( (X^T)X w^2 - 2 \\cdot X^T yw  + y^2 \\right) \\\\\n       \\nabla E_{in}(w) &= \\frac{1}{N} \\left( 2(X^T)X w - 2 \\cdot X^T y \\right)\n\\end{align}\n\\tag{$7$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\nvector \\quad w :  E_{in}(w) &= \\frac{1}{N} \\left( (X^T)X w^T w - 2 \\cdot X^T y w^T  + y^2 \\right) \\\\\n          \\nabla E_{in}(w)  &= \\frac{1}{N} \\left( 2(X^T)X w - 2 \\cdot X^T y \\right)\n\\end{align}\n\\tag{$8$}</script><script type=\"math/tex; mode=display\">\n\\nabla E_{in}(w)  = \\frac{2}{N} \\left( (X^T)X w - X^T y \\right)\n\\tag{$9$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\nW_{LIN} &= \\underbrace{(X^T X) ^ {-1} X^T }_{X^\\dagger} y \\\\\n        &= \\underbrace{X ^\\dagger }_{(d+1)\\times N)}\n           \\underbrace{y}_{N \\times 1}\n\\end{align}\n\\tag{$10$}</script><h3 id=\"4-Generalization-Issue\"><a href=\"#4-Generalization-Issue\" class=\"headerlink\" title=\"4) Generalization Issue\"></a>4) Generalization Issue</h3><p>上一节中，我们得到了最佳的 w 的解，但是，哪个是最小的 $E_{in}$， 实际上我们最需要的是使得 $E_{out}$最小，而我们这一节就是要讨论之前关于VC Bound保证了 $E_{in} \\approx E_{out}$ 是否依然适用于线性回归问题。（中间解释过程较复杂，后续补充。）</p>\n<p>结论就是得到了 $E_{in} E_{out}$关于 noise level的方程，如公式（11）（12），曲线图如图三所示。</p>\n<script type=\"math/tex; mode=display\">\nE_{in}(w)  = noise \\quad level  \\cdot (1- \\frac{d+1}{N})\n\\tag{$11$}</script><script type=\"math/tex; mode=display\">\nE_{out}(w)  = noise \\quad level  \\cdot (1+ \\frac{d+1}{N})\n\\tag{$12$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/01235bfa1a94084fee01577e205a27839be75228/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-3%20Ein%20Eout.png\" alt=\"Ein Eout Curve\"></p>\n<center> 图三 Ein Eout Curve<sup>[2]</sup></center>\n\n\n<p>可以看出在N趋于无穷大时，与两者都会趋近于noise level的值，即 $\\sigma^2$ 泛化错误之间的差距：$\\frac{2(d+1)}{N}$ 。</p>\n<p>至此可以表明在线性回归中可以寻找到很好的 $E_{out}$，因此线性回归可以学习。</p>\n<h3 id=\"5-Linear-Regression-for-Binary-Classification\"><a href=\"#5-Linear-Regression-for-Binary-Classification\" class=\"headerlink\" title=\"5) Linear Regression for Binary Classification\"></a>5) Linear Regression for Binary Classification</h3><p>这一节，我们主要讨论能否通过求解线性回归的方式求二元分类问题</p>\n<p>首先对比Linear Classification 和 Linear Regression， 如图四</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d4a67d82122ef663fb09d41c3b1862d897536e2a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-4%20Linear%20Regression%20for%20Binary%20Classification.png\" alt=\"Linear Classification and Linear Regression\"></p>\n<p><center> 图四 Linear Classification and Linear Regression<sup>[3]</sup></center><br><br></p>\n<p>从求解问题的难度考虑，二元分类的求解是一个NP难问题，只能近似求解，而线性回归求解方便，程序编写也简单。</p>\n<p>直觉告诉我们:因为二元分类的输出空间{-1，+1}属于线性回归的输出空间，即 $\\{-1, +1\\}∈ R$。其中数据集的标记大于零的表示+1，小于零的表示-1，通过线性回归求得的解析解 $W_{LIN}$，直接得出最优假设 $g(x) = sign( W_{LIN} x)$ 。</p>\n<p>下面给出证明：</p>\n<blockquote>\n<p>入手点：误差大小</p>\n</blockquote>\n<p>首先对比两种方法的曲线图，如图五所示。<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/341baa79de829072a35424870d65421a0bc9f9ba/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-5%20Relation%20of%20Two%20Errors.png\" alt=\"Relations of Two Errors\"></p>\n<p><center> 图五 Relations of Two Errors<sup>[3]</sup></center><br><br></p>\n<p>可以发现，无论期望的y为-1还是+1， $err_{sqr}$ 始终大于 $err_{0/1}$，即公式（13），加上我们之前有的 $E_{out} \\approx E_{in}$，那么我们可以得到公式（14）。</p>\n<script type=\"math/tex; mode=display\">\nerr_{0/1} \\leq err_{sqr}\n\\tag{$13$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\nClassification \\quad E_{out}(w)\n&\\leq Classification \\quad E_{in}(w)  + \\sqrt{\\frac{8}{N} \\ln\\left( \\frac{4 (2N)^{d_{vc}}}{\\delta} \\right)} \\\\\n&\\leq Regression \\quad\\quad E_{in}(w)  + \\sqrt{\\frac{8}{N} \\ln\\left( \\frac{4 (2N)^{d_{vc}}}{\\delta} \\right)}\n\\end{align}\n\\tag{$14$}</script><p>所以我们可以通过使用 Linear Regression 的方法来求解Linear Classification的问题，虽然通过这种方法，得到的误差可能更大，但却是一种更有效率的求解方式。在实际运用中，一般都将通过线性回归求得的解析解作为PLA或者pocket的初始值，达到快速求解的目的 。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了Regression。</li>\n<li>然后通过例子引入Linear Regression，分别介绍Linear Regression的误差方程，算法流程。</li>\n<li>接着我们引入VC Bound 解释了Linear Regression为什么可以实现学习。</li>\n<li>最后我们应用Linear Regression的算法到Classification的问题中：虽然会损失一定的准确度，但是效率提升很大。</li>\n</ol>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\9\\9 - 1 - Linear Regression Problem (10-08)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\9\\9 - 3 - Generalization Issue (20-34)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\9\\9 - 4 - Linear Regression for Binary Classification (11-23)</p>\n<p><br></p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Linear-Regression\"><a href=\"#How-can-Machine-Learn-Linear-Regression\" class=\"headerlink\" title=\"How can Machine Learn? - Linear Regression\"></a>How can Machine Learn? - Linear Regression</h1><h2 id=\"1-What-is-Regression\"><a href=\"#1-What-is-Regression\" class=\"headerlink\" title=\"1. What is Regression\"></a>1. What is Regression</h2><p>机器学习算法一般是这样一个步骤：</p>\n<ol>\n<li><p>对于一个问题，用数学语言来描述它，然后建立模型，e.g., 回归模型或者分类模型；</p>\n</li>\n<li><p>建立代价函数: Cost Function, 用最大似然、最大后验概率或者最小化分类误差等等数，也就是一个最优化问题。找到最优化问题的解，也就是能拟合我们的数据的最好的模型参数；</p>\n</li>\n<li><p>求解这个代价函数，找到最优解。求解也就分很多种情况了：</p>\n<p>1). 如果这个优化函数存在解析解。例如我们求最值一般是对代价函数求导，找到导数为0的点，也就是最大值或者最小值的地方了。如果代价函数能简单求导，并且求导后为0的式子存在解析解，那么我们就可以直接得到最优的参数了: Gradient Descent。</p>\n<p>2). 如果式子很难求导，例如函数里面存在隐含的变量或者变量相互间存在耦合，也就互相依赖的情况。或者求导后式子得不到解释解，例如未知参数的个数大于已知方程组的个数等。这时候我们就需要借助迭代算法来一步一步找到最有解了。</p>\n<p>3). 另外需要考虑的情况是，如果代价函数是凸函数，那么就存在全局最优解。</p>\n</li>\n</ol>\n<p>回归(Regression)问题与分类(Classification)问题类似，关键的区别在于，Regression的输出是实数,是一个范围，所以不能提前确定所有输出值；而Classification的输出是确定的值。换个角度说，假如输出足够多的数据的时候：如果输出的值是连续的，这个问题叫做Regression，而如果输出的值是离散的，那么这个问题叫做Classification</p>\n<p>用数学符号表示如果公式（1）所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nRegression : &f(x) = \\left[ value1, value2 \\right]  \\\\\nClassification: &f(x) = \\{ value_1, value_2, ..., value_n \\}\n\\end{align}\n\\tag{$1$}</script><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Linear-Regression\"><a href=\"#2-Linear-Regression\" class=\"headerlink\" title=\"2. Linear Regression\"></a>2. Linear Regression</h2><h3 id=\"1-Introduction-of-Linear-Regression\"><a href=\"#1-Introduction-of-Linear-Regression\" class=\"headerlink\" title=\"1) Introduction of Linear Regression\"></a>1) Introduction of Linear Regression</h3><p>线性回归问题同样的表示各个属性（Attribution）对最终结果的贡献：也就是说每个属性乘以对应的权值，最后再加上一定的偏移，如公式（2）所示</p>\n<script type=\"math/tex; mode=display\">\ny = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n\n\\tag{$2$}</script><p>如果给第一项的偏移 $w_0$ 加上属性值 $x_0$，并把单独的输出点$y$用向量进行表示，那么公式（2）就变成了公式（3）（这一步纯粹是为了表示方便）。</p>\n<p>Hypothesis h(x):</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh(x) &= w_0 \\cdot x_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n  \\quad(x_0 = 1) \\\\\n     &= w^T \\cdot x_i \\\\\n     &= W^T \\cdot X\n\\end{align}\n\\tag{$3$}</script><p>从公式(3)的可以看出，与二元分类假设函数的表示只差了一个取正负号的函数 $sign()$。</p>\n<h3 id=\"2-Error-Measurement-of-Linear-Regression\"><a href=\"#2-Error-Measurement-of-Linear-Regression\" class=\"headerlink\" title=\"2) Error Measurement of Linear Regression\"></a>2) Error Measurement of Linear Regression</h3><p>线性回归问题的错误如图一所示。<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/de4c8798497f08b40cbe86cf0cc665ceaa6d54ff/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-1%20Linear%20Regression.png\" alt=\"Error Measurement\"></p>\n<center> 图一 Error Measurement <sup>[1]</sup></center>\n\n\n<p>上一章中，提到了回归问题我们用平方误差来表示（其实还有RMSE， R2等方法，老师上课只讲了平方误差，为了说明方便，我们只写这个）<br>根据图一的信息，很明显得到公式（4）（5）。</p>\n<script type=\"math/tex; mode=display\">\nE_{in}(w)) = \\frac{1}{N} \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2\n\\tag{$4$}</script><script type=\"math/tex; mode=display\">\nE_{out}(w)) = \\epsilon \\cdot \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2\n\\tag{$5$}</script><p>VC Bound可以约束各种情况的学习模型，当然回归类型的模型也被也受此约束，只需要寻找足够小 $E_{in}(w)$ 便可以满足 $E_{out}(w)$ 小的需求。</p>\n<h3 id=\"3-Linear-Regression-Algorithm\"><a href=\"#3-Linear-Regression-Algorithm\" class=\"headerlink\" title=\"3) Linear Regression Algorithm\"></a>3) Linear Regression Algorithm</h3><p>刚刚上面的Error Meansurement中，我们提到了VC Bound中，在训练样本足够的情况下，我们要用特定的算法，来找到最小的 $E_{in}(w)$，所以把上面公式（4）的pointwise error measurement 通过矩阵做调整得到公式（6）。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nE_{in}(w)) &= \\frac{1}{N} \\sum\\limits_{n=1}^{N}(w^Tx_n - y_n)^2 \\\\\n           &= \\frac{1}{N} \\sum\\limits_{n=1}^{N}(x_n^Tw - y_n)^2   \\quad\\quad (向量内积，符合交换率) \\\\\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{array}{ccc}\n                x_1^Tw - y_1 & \\\\\n                x_2^Tw - y_2 & \\\\\n                ...            \\\\\n                x_N^Tw - y_N &\n           \\end{array}\n           \\right|\n           \\right|^2               \\quad\\quad(平方求和)\\\\\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{bmatrix}\n                -x_1^T-\\\\\n                -x_1^T-\\\\\n                ...    \\\\\n                -x_1^T-\\\\\n           \\end{bmatrix}\n           \\cdot w\n           \\begin{bmatrix}\n                y_1\\\\\n                y_2\\\\\n                ...\\\\\n                y_n\\\\\n           \\end{bmatrix}\n           \\right|\n           \\right|^2                \\quad\\quad(矩阵平方)\\\\\n\n           &= \\frac{1}{N}\n           \\left|\n           \\left|\n           \\begin{array}{ccc}\n                \\underbrace{X^T}_{N \\times (d+1)}\n                \\underbrace{w}_{(d+1)\\times 1}\n                -\n                \\underbrace{y}_{N \\times 1}\n           \\end{array}\n           \\right|\n           \\right|^2 \\\\\n\\end{align}\n\\tag{$6$}</script><p>根据公式（6），那么 $E_{in}(w)$的最小值如公式（7）所示。</p>\n<script type=\"math/tex; mode=display\">\nmin_w E_{in}(w) = min_w \\frac{1}{N} \\left|\\left| X^Tw - y \\right|\\right|^2\n\\tag{$7$}</script><p>$E_{in}(w)$的变化图如图二所示，可以看出这是一个连续（continuous）、可微（differentiable）的凸（convex）函数。最小值就是山谷的点（即转折点），并且对这个点求导的值为0。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/edb6705ac1e3e9565cb69b9b170c1de25f86f987/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-2%20Linear%20Regression%20Ein.png\" alt=\"Ein Curve\"></p>\n<center> 图二 Ein Curve <sup>[1]</sup></center>\n\n<p>所以接下来，我们需要对$E_{in}(w)$求导并且找到一个值 $W_{LIN}$，使得 $\\nabla E_{in}(w)$ 的值为0。</p>\n<ol>\n<li>首先我们需要考虑两种情况：①只有一个w； ②w为向量</li>\n<li>首先我们把公式（6）平方去掉</li>\n<li>只有一个w的时候，就按照正常的求导方式求导即可，得到公式（7）</li>\n<li>当w为向量的时候，我们需要对其中一个进行转置，然后再求导，得到公式（8）</li>\n<li>根据公式（7）（8）可以看出，两种情况的求导的结果非常相似，可以用公式（9）表示</li>\n<li>最后我们令公式（9）的值为0，得到我们需要的 $W_{LIN}$ 如公式（10）所示,其中 $X^\\dagger$ 叫做<a href=\"http://blog.sina.com.cn/s/blog_890c6aa30101cn6t.html\" target=\"_blank\" rel=\"external\">矩阵X的伪逆矩阵（pseudo-inverse）</a></li>\n</ol>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\none \\quad w : E_{in}(w) &= \\frac{1}{N} \\left( (X^T)X w^2 - 2 \\cdot X^T yw  + y^2 \\right) \\\\\n       \\nabla E_{in}(w) &= \\frac{1}{N} \\left( 2(X^T)X w - 2 \\cdot X^T y \\right)\n\\end{align}\n\\tag{$7$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\nvector \\quad w :  E_{in}(w) &= \\frac{1}{N} \\left( (X^T)X w^T w - 2 \\cdot X^T y w^T  + y^2 \\right) \\\\\n          \\nabla E_{in}(w)  &= \\frac{1}{N} \\left( 2(X^T)X w - 2 \\cdot X^T y \\right)\n\\end{align}\n\\tag{$8$}</script><script type=\"math/tex; mode=display\">\n\\nabla E_{in}(w)  = \\frac{2}{N} \\left( (X^T)X w - X^T y \\right)\n\\tag{$9$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\nW_{LIN} &= \\underbrace{(X^T X) ^ {-1} X^T }_{X^\\dagger} y \\\\\n        &= \\underbrace{X ^\\dagger }_{(d+1)\\times N)}\n           \\underbrace{y}_{N \\times 1}\n\\end{align}\n\\tag{$10$}</script><h3 id=\"4-Generalization-Issue\"><a href=\"#4-Generalization-Issue\" class=\"headerlink\" title=\"4) Generalization Issue\"></a>4) Generalization Issue</h3><p>上一节中，我们得到了最佳的 w 的解，但是，哪个是最小的 $E_{in}$， 实际上我们最需要的是使得 $E_{out}$最小，而我们这一节就是要讨论之前关于VC Bound保证了 $E_{in} \\approx E_{out}$ 是否依然适用于线性回归问题。（中间解释过程较复杂，后续补充。）</p>\n<p>结论就是得到了 $E_{in} E_{out}$关于 noise level的方程，如公式（11）（12），曲线图如图三所示。</p>\n<script type=\"math/tex; mode=display\">\nE_{in}(w)  = noise \\quad level  \\cdot (1- \\frac{d+1}{N})\n\\tag{$11$}</script><script type=\"math/tex; mode=display\">\nE_{out}(w)  = noise \\quad level  \\cdot (1+ \\frac{d+1}{N})\n\\tag{$12$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/01235bfa1a94084fee01577e205a27839be75228/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-3%20Ein%20Eout.png\" alt=\"Ein Eout Curve\"></p>\n<center> 图三 Ein Eout Curve<sup>[2]</sup></center>\n\n\n<p>可以看出在N趋于无穷大时，与两者都会趋近于noise level的值，即 $\\sigma^2$ 泛化错误之间的差距：$\\frac{2(d+1)}{N}$ 。</p>\n<p>至此可以表明在线性回归中可以寻找到很好的 $E_{out}$，因此线性回归可以学习。</p>\n<h3 id=\"5-Linear-Regression-for-Binary-Classification\"><a href=\"#5-Linear-Regression-for-Binary-Classification\" class=\"headerlink\" title=\"5) Linear Regression for Binary Classification\"></a>5) Linear Regression for Binary Classification</h3><p>这一节，我们主要讨论能否通过求解线性回归的方式求二元分类问题</p>\n<p>首先对比Linear Classification 和 Linear Regression， 如图四</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d4a67d82122ef663fb09d41c3b1862d897536e2a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-4%20Linear%20Regression%20for%20Binary%20Classification.png\" alt=\"Linear Classification and Linear Regression\"></p>\n<p><center> 图四 Linear Classification and Linear Regression<sup>[3]</sup></center><br><br></p>\n<p>从求解问题的难度考虑，二元分类的求解是一个NP难问题，只能近似求解，而线性回归求解方便，程序编写也简单。</p>\n<p>直觉告诉我们:因为二元分类的输出空间{-1，+1}属于线性回归的输出空间，即 $\\{-1, +1\\}∈ R$。其中数据集的标记大于零的表示+1，小于零的表示-1，通过线性回归求得的解析解 $W_{LIN}$，直接得出最优假设 $g(x) = sign( W_{LIN} x)$ 。</p>\n<p>下面给出证明：</p>\n<blockquote>\n<p>入手点：误差大小</p>\n</blockquote>\n<p>首先对比两种方法的曲线图，如图五所示。<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/341baa79de829072a35424870d65421a0bc9f9ba/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter7-5%20Relation%20of%20Two%20Errors.png\" alt=\"Relations of Two Errors\"></p>\n<p><center> 图五 Relations of Two Errors<sup>[3]</sup></center><br><br></p>\n<p>可以发现，无论期望的y为-1还是+1， $err_{sqr}$ 始终大于 $err_{0/1}$，即公式（13），加上我们之前有的 $E_{out} \\approx E_{in}$，那么我们可以得到公式（14）。</p>\n<script type=\"math/tex; mode=display\">\nerr_{0/1} \\leq err_{sqr}\n\\tag{$13$}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\nClassification \\quad E_{out}(w)\n&\\leq Classification \\quad E_{in}(w)  + \\sqrt{\\frac{8}{N} \\ln\\left( \\frac{4 (2N)^{d_{vc}}}{\\delta} \\right)} \\\\\n&\\leq Regression \\quad\\quad E_{in}(w)  + \\sqrt{\\frac{8}{N} \\ln\\left( \\frac{4 (2N)^{d_{vc}}}{\\delta} \\right)}\n\\end{align}\n\\tag{$14$}</script><p>所以我们可以通过使用 Linear Regression 的方法来求解Linear Classification的问题，虽然通过这种方法，得到的误差可能更大，但却是一种更有效率的求解方式。在实际运用中，一般都将通过线性回归求得的解析解作为PLA或者pocket的初始值，达到快速求解的目的 。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了Regression。</li>\n<li>然后通过例子引入Linear Regression，分别介绍Linear Regression的误差方程，算法流程。</li>\n<li>接着我们引入VC Bound 解释了Linear Regression为什么可以实现学习。</li>\n<li>最后我们应用Linear Regression的算法到Classification的问题中：虽然会损失一定的准确度，但是效率提升很大。</li>\n</ol>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\9\\9 - 1 - Linear Regression Problem (10-08)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\9\\9 - 3 - Generalization Issue (20-34)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\9\\9 - 4 - Linear Regression for Binary Classification (11-23)</p>\n<p><br></p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"8.How can Machine Learn? - Logistic Regression","date":"2017-10-13T04:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# How can Machine Learn? - Logistic Regression\n\n\n>这一节讨论与 Linear Regression 非常类似的Logistic Regression\n\n## 1. Introduction of Logistic Regression\n使用二元分类分析心脏病复发问题，其输出空间只含有两项{+1，-1}，分别表示复发和不发复发。在含有噪音的情况下，目标函数f可以使用目标分布P来表示，如公式（1）所示\n\n$$\nf(x) = sign \\left( P(+1|x) - \\frac{1}{2} \\right) ∈ \\{ +1,-1 \\}\n\\tag{1}\n$$\n\n但是实际情况，医生往往不会直接告诉病人说是否会心脏病复发，而是用概率，例如说有80%的可能性会复发，此种情况被称为软二元分类（soft binary classification），目标函数f的表达如公式（2）所示，其输出以概率的形式，在0~1之间。\n\n$$\nf(x) =  P(+1|x)  ∈ [ +1,-1 ]\n\\tag{2}\n$$\n\n但是病人的病历里面不可能记录以前有多少多少的几率复发/不复发，而是真实的记录病人是否复发。所以概率的情况来说，复发/不复发的情况就像是噪音了(因为偏离中间的概率值大)，所以我们把实际的训练数据看成是含有噪音的理想训练数据。这种问题如何求解呢？我们可以通过输入各属性 $x=(x_0,x_1, x_2, ..., x_n)$ 的加权总分数（weighted scores），如公式（3）所示\n\n$$\ns = \\sum\\limits_{i=0}^n w_ix_i = w^Tx\n\\tag{3}\n$$\n\n这里的s的值不在 0~1之间，所以我们还需要将他进行归一化处理，那就可以使用Logistic Regression。函数用表示 $\\theta(s)$，叫做Logistic Function 或者 Sigmoid Function，如公式（4）所示。分数s越大风险越高，分数s越小风险越低。假设函数h如公式(5)所示，函数曲线的示意图如图一所示。\n\n\n$$\n\\theta( s ) = \\frac{ e } {e + e^z} = \\frac{1}{1+e^{-z}}\n\\tag{4}\n$$\n$$\nh(x) = \\theta( w^T x ) = \\frac{ 1 } {1+e^{-w^Tx}}\n\\tag{5}\n$$\n\n![Logistic Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/57938b7a7b126af2ff2d5abdbea1a2fa0f6a4e1f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-1%20Logistic%20Curve.png)\n<center> 图一 Logistic Curve <sup>[1]</sup></center>\n\n\n观察函数的图形，该函数是一个平滑（处处可微分），单调（monotonic）的S形（sigmoid）函数，因此又被称为sigmoid函数。\n\nLogistic Regression 是当前业界比较常用的机器学习方法，用于估计某种事物的可能性，应用场合如：广告预测，购物推荐，患病可能性判断等。 Logistic Regression既可以做回归，也可以做分类（二分类为主）。\n\n\n\n<br><br>\n----------------------------------\n## 2. Comparison of Linear Regression, Logistic Classification and Logistic Regression\n将logisitic回归与之前学习的二元分类和线性回归做一对比，如图二所示。\n\n![Comparison of Three Linear Models](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a2cbd54aa0efe3a8f2f2f539a2fd595c4075971e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-2%20Comparison%20of%20Three%20Linear%20Models.png)\n<center> 图二 Comparison of Three Linear Models <sup>[2]</sup></center>\n\n其中分数s是在每个假设函数中都会出现的，前两个学习模型的错误衡量分别对应着0/1错误和平方错误，而logistic回归所使用的err函数应如何表示则是本节要介绍的内容。\n\n\n\n<br><br>\n----------------------------------\n## 3. Error Measurement of Logistic Regression - cross-entropy error\n这一节的推导需要对最大似然法和条件概率求解有一定的了解。\n\n>TODO:最大似然法 和 条件概率\n\n1.首先从 Logistic Function 可以推导出下面的公式（6），花括号上半部分不难理解，是将目标函数等式左右对调的结果，而下半部分的推导也很简单，因为+1与-1的总概率为1。\n$$\nf(x) = P(+1|x) \\Leftrightarrow P(y|x) = \\left\\{\n\\begin{aligned}\n& f(x)  \\quad   &for \\quad y = +1\\\\\n& 1-f(x)        &for \\quad y = -1\n\\end{aligned}\n\\right.\n\\tag{6}\n$$\n\n2.假设存在一个数据集 $D={(x_1, \\circ), (x_2, \\times), \\cdots, (x_n, \\times)}$,则通过目标函数产生此种数据集样本的概率可以用公式（7）表示。\n$$\nP(D) = P\\{ x_1 \\} P\\{ \\circ|x_1 \\} \\times P\\{ x_2 \\}P\\{ \\times|x_2 \\} \\times \\dots \\times P\\{ x_n \\} P\\{ \\times|x_n \\}\n\\tag{7}\n$$\n\n3.把公式（6）的公式带入公式（7)中，可以得到公式（8）。\n\n$$\nP(D) = P\\{ x_1 \\} f(x_1) \\times P\\{ x_2 \\} (1-f(x_2)) \\times \\dots \\times P\\{ x_n \\} (1-f(x_n))\n\\tag{7}\n$$\n\n4.f(x)是理想的函数，而我们实际训练得到的是hypothesis h(x)，所以我们还得想办法用h(x)代替f(x)，但是这样的前提是我们假设函数h(x)对数据集与f(x)产生的可能性很大，即likelihood(似然)，即我们在之前在VC Bound的推论中，知道在数据量足够大的情况下g(x)是会接近于f(x)的，如公式（8）所示\n\n$$\nP(D) = P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (1-h(x_2)) \\times \\dots \\times P\\{ x_n \\} (1-h(x_n))\n\\tag{8}\n$$\n\n\n5.那么最大似然我们表示为 likelihood(h)，在代入simoid函数的特性 $1-h(x) = h(-x)$，可以得到公式（9）\n\n$$\nlikelihood(h) = P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (h(-x_2)) \\times \\dots \\times P\\{ x_n \\} (h(-x_n))\n\\tag{9}\n$$\n\n6.那么最大的likelihood(h)如公式（10）所示，在计算最大的likelihood(h)时，所有$P(x_i)$的对大小没有影响，因为所有的假设函数都会乘以同样的$P(x_i)$，所以在表示的时候可以只考虑h(x)。\n\n$$\n\\begin{align}\n\\max \\limits_h \\quad likelihood(logistic \\quad h)\n&\\propto P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (h(-x_2)) \\times \\dots \\times P\\{ x_n \\} (h(-x_n)) \\\\\n&\\propto  \\prod \\limits_{i=1}^n h(y_ix_i) \\\\\n\\max \\limits_w \\quad likelihood(w) &\\propto  \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n\\max \\limits_w \\quad likelihood(w) &\\propto\\max \\quad ln \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n\\end{align}\n\\tag{10}\n$$\n\n\n7.连乘公式不容易求解最大问题，因此求其对数，此处以自然对数e为底，并代入sigmoid 方程，令$err(w,y_i,x_i) = ln(1+exp(-yw^Tx))$，如公式（11）所示，误差方程 $E_{in}$ 如公式（12)所示。\n\n$$\n\\begin{align}\n\\max \\limits_w \\quad likelihood(w)\n&\\propto \\max \\quad ln \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n&\\propto \\min\\limits_w \\quad \\frac{1}{N} \\sum\\limits_{i=1}^{n} -ln \\theta (y_iw^Tx_i) \\\\\n&\\propto \\min\\limits_w \\quad \\underbrace{ \\frac{1}{N} \\sum\\limits_{i=1}^{n} err(w,y_i,x_i) }_{E_{in}(w)}  \\quad \\quad\n\\end{align}\n\\tag{11}\n$$\n\n$$\nE_{in}(w) = \\frac{1}{N} \\sum\\limits_{i=1}^{n} ln(1+exp(-yw^Tx))\n\\tag{12}\n$$\n\n\n\n<br><br>\n----------------------------------\n## 4. Gradient of Logistic Regression Error\n上一节中，推导出logistic回归的 ，下一步的工作是寻找使得最小的权值向量w。\n对公式（12）进行求导，可以得到公式（13），推导过程参考老师用的方法：用圈圈代替exp里面的数，用正方形代替ln里面的表达式，这样可以使得推导过程看起来更加明白\n$$\n\\begin{align}\n\nE_{in}(w) &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} ln \\underbrace{(1+exp \\underbrace{(-yw^Tx)}_\\bigcirc)}_\\Box \\\\\n\n\n\\frac{\\partial E_{in}(w)}{\\partial w_i} &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left(\\frac{\\partial ln(\\Box)}{\\partial \\Box}\\right) \\left( \\frac{\\partial (1+exp(\\bigcirc))}{\\partial \\bigcirc} \\right) \\left( \\frac{\\partial - y_iw^Tx_i}{\\partial w_i} \\right) \\\\\n                                        &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\frac{1}{\\Box} \\right) \\left( exp(\\bigcirc) \\right) \\left( -y_i x_i \\right) \\\\\n\n                  \t\t\t\t\t\t&= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( exp(\\bigcirc) \\right) \\left(-y_i x_i \\right) \\quad \\because(\\frac{1}{\\Box}) \\approx 1\\\\\n\t\t\t\t\t\t\t\t\t\t&= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\theta(\\bigcirc) \\right) \\left( -y_i x_i \\right)  \\\\\n\t\t\t\t\t\t\t\t\t\t&= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\theta(-y_iw^Tx_n) \\right) \\left( -y_i x_i \\right)  \\\\\n\\end{align}\n\\tag{13}\n$$\n\n 从公式（13）可以看出，该函数是一个 $\\theta$ 函数作为权值，关于 $(-y_nx_n)$ 的加权求和函数。如果函数的所有权值为零，，可以看出 $y_i$与所有的对应的 $w^Tx_n$ 的同号，即线性可分。\n\n但是，求该问题的解不能使用类似求解线性回归时使用的闭式解的求解方式，此最小值又该如何计算？我们可以借鉴之前PLA的方法进行迭代求解，如公式（14）\n\n$$\nw_{t+1} = w_t + \\left[\\left[ sign(w_t^Tx_i) \\neq y_i \\right]\\right] y_ix_i\n\\tag{14}\n$$\n\n从公式（14）可以看出，当 $sign(w_t^Tx_i)=y_n$ 的时候，向量不改变，当 $sign(w_t^Tx_i) \\neq y_n$ 的时候 要加上 $y_ix_i$，然后我们把公式（14)做一定的调整得到公式（15），其中多乘以一个1作为更新的步长，用 $\\eta$表示，PLA中更新的部分用 $\\nu$ 来代表，表示更新的方向。该算法被称为迭代优化方法（iterative optimization approach）\n\n$$\n\\begin{align}\nw_{t+1} &= w_t + \\underbrace{1}_\\eta \\cdot\n                 \\underbrace{\\left[\\left[ sign(w_t^Tx_i) \\neq y_i \\right]\\right] y_ix_i}_\\nu \\\\\n\t    &= w_t + \\eta \\cdot \\nu\n\\end{align}\n\\tag{15}\n$$\n\n\n<br><br>\n----------------------------------\n## 5. Use Gradient Descent to Minimize the Error of Logistic Regression\n上面我们根据PLA的方法求得针对Logistic回归问题的误差方程，现在我们就需要找到最佳的参数 $\\eta$ 和 $\\nu$。首先误差方程的曲线图如图三所示。$E-{in}$ 是关于权值向量 $w$ 的示意图为一个平滑且可微的凸函数，其中图像谷底的点对应着最佳 $w$。\n\n![Iterative Optimization](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/efb59327c4bf9ddccb57959dcc04f7801267f667/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-3%20Iterative%20Optimization.png)\n<center> 图三 Iterative Optimization <sup>[3]</sup></center>\n\n为了分工明确，设 $\\nu$ 作为单位向量仅代表方向， $\\eta$ 代表步长表示每次更新改变的大小。在 $\\eta$ 固定的情况下，$\\nu$ 按照最陡峭的方向更改。即在 $\\eta$ 固定 $|\\nu| = 1$ 的情况下，有最快的速度找出使得 $E_{in}$ 最小的 $w$，得到公式（16）\n\n$$\n\\min\\limits_{|\\nu|=1} E_{in} \\underbrace{(w_i + \\eta \\nu)}_{w_{t+1}}\n\\tag{16}\n$$\n\n但是公式（16）依然很难求得最小的 $w$，当 $\\eta$ 很小时，我们通过泰勒展开公式（17）可以得到公式（18），其中 $w_t$ 对应 $x_0$\n\nTylor Expansion\n$$\nf(x) = f(x_0) + \\frac{f'(x_0)}{1!}(x-x_0) + \\frac{f^{(2)}(x_0)}{2!}(x-x_0)^2 + \\dots + \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + R_n(x)\n\\tag{17}\n$$\n\n$$\n\\begin{align}\n\\min\\limits_{|\\nu|=1} E_{in} (w_i + \\eta \\nu ^ T) &\\approx   E_{in}(w_t) + \\left(\\left( w_t + \\eta\\nu^T  \\right) - w_t \\right) \\frac{\\nabla E_{in}(w_t)}{1!} \\\\\n\n                                                  &= E_in(w_t) + \\eta\\nu^T \\nabla E_{in}(w_t)\n\n\\end{align}\n\\tag{18}\n$$\n\n\n接着我们继续分析公式（18），其中 $E_in{w_t}$ 我们是知道的， $\\eta$ 是给定的步长， $\\nabla E_{in}(w_t)$ 也是知道的，所以求解公式（18）的最小值问题，可以转换成求解 $\\nu^T nabla E_{in}(w_t)$ 的最小值，即公式（19）\n\n$$\n\\min\\limits_{|\\nu|=1} \\nu^T \\nabla E_{in}(w_t)\n\\tag{19}\n$$\n\n两个向量最小的情况为其方向相反，即内积为负，得到公式（20）,这种情况下 $\\nu$ 是一个单位向量\n\n$$\n\\begin{align}\n\\min\\limits_{|\\nu|=1} \\nu^T \\nabla E_{in}(w_t) &= -1 \\\\\n                                           \\nu &= - \\frac{\\nabla E_{in}(w_t)}{||\\nabla E_{in}(w_t)||}\n\\end{align}\n\\tag{20}\n$$\n\n所以把公式（20）带入公式（15），可以得到公式（21）\n$$\n\\begin{align}\nw_{t+1} &= w_t - \\eta \\cdot \\frac{\\nabla E_{in}(w_t)}{||\\nabla E_{in}(w_t)||}\n\\end{align}\n\\tag{21}\n$$\n\n从公式（21）可以看出，每次更新权值，w都是减少一点（具体多少要看我们谁的哪个的步长，已经误差大小），按照此种方式更新可以找到使得最小的w。此种方式称作梯度下降（gradient descent）。\n\n\n\n<br><br>\n----------------------------------\n## 6. Choose Step Length for Gradient Descent\n由上面的公式（21）可以看出，w受 步长大小 $\\eta$ 和 误差大小的影响。在一定的 $\\eta$ 下，越接近谷底，纠正的也越来越小；但是如果选择的 $\\eta$ 太大，一个新就更新到对面的山峰上面去了（可能会导致纠正后误差更大），或者  $\\eta$ 太小，更新好久还没有更新到需要的准确度。所以选择适当的 $\\eta$ 很重要。如图四所示。\n![Choice of Step Length](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/09288541bf7cddb805bd091f25cf104e438fc179/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-4%20Choice%20of%20eta.png)\n<center> 图四 Choice of Step Length <sup>[3]</sup></center>\n\n因为 $\\eta$ 与 梯度大小 ${||\\nabla E_{in}(w_t)||}$ 正比，所以我们可以得到公式（22）\n$$\n\\eta_{new} = \\frac{ \\eta_{old}}{||\\nabla E_{in}(w_t)||}\n\\tag{22}\n$$\n\n结合公式（21）（22），我们调整的公式（21）为（23）\n\n$$\n\\begin{align}\nw_{t+1} &= w_t - \\eta_{new} \\cdot {\\nabla E_{in}(w_t)}\n\\end{align}\n\\tag{21}\n$$\n\n此时的 $\\eta$ 被称作固定的学习速率（fixed learning rate）。最终得到Logstic Regression 的步骤如下：\n1. 设置权值w为 $w_0$，迭代次数他，并计算梯度 $\\nabla E_{in}(w_t) = \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\theta \\left( -y_iw_t^Tx_i \\right)\\left( -y_ix_i \\right)$\n2. 不断迭代，并更新权值向量w，$w_{t+1} = w_t - \\eta_{new} \\cdot {\\nabla E_{in}(w_t)}$，直到误差函数的导数近似于0，或者迭代一定的次数。\n\n\nGradient Descent 劣势分析：\n1. 不稳定:如果选择的步长太大太小，都会对算法有影响\n2. 局部最优：如果函数不是凸函数的话，可能存在多个局部最优点，那样的话，Gradient Descent只能找到最近的局部最优。\n3. 计算复杂度大，为O(N)，因为导数需要对所有的点进行一次遍历\n\n\n<br><br>\n----------------------------------\n## 7. Stochastic Gradient Descent - Another Approach of Gradient Descent\n上面讨论了 Gradient Descent,以及计算复杂度大的问题，这一节我们讨论另一种方法，可以将计算复杂度降成O(1)级别。这种方法就是Stochastic Gradient Descent（随机梯度），用符号 $\\nabla_w err(w, w_i, y_i)$ 表示， 核心思想是：通过N个样本中随机抽取一个样本点求出的梯度取代原来的期望梯度。 随机梯度值可以看做真实的梯度值加上一个噪音，使用随机梯度取代真实梯度做梯度下降的算法称作随机梯度下降 stochastic gradient descent(SGD)。在迭代次数足够多的情况下，平均的随机梯度和平均的真实梯度相差不大。真实梯度与随机梯度的关系如公式（22）所示\n\n$$\n\\nabla_w E_{in}(w_t) = \\varepsilon_{i} \\cdot \\nabla_w err( w, x_i, y_i)\n\\tag{22}\n$$\n\nLogistic Regression 的SGD的迭代如公式(23)所示。\n\n$$\n\\begin{align}\nw_{t+1} &= w_t - \\eta \\cdot( \\nabla_w err( w, x_i, y_i) )(y_ix_i) \\\\\n        &= w_t + \\eta  \\underbrace{\\theta (-y_i w_t^T x_i)(y_i x_i)}_{- \\nabla_w err( w, x_i, y_i)}(y_ix_i)\n\\end{align}\n\\tag{23}\n$$\n\n对比之前的PLA算法的公式（如公式（24）），容易发现两个公式很类似，因此logistic Regession 的SGD算法又叫\"软\"PLA，因为权值并没有那么绝对不是1就是0，而是一个在0~1之间的值。如果 $\\eta = 1$ 且 $w_t^T x_i \\approx \\infty$ 始终是一个很大的值，则logistic Regession 的SGD相当于是PLA算法。\n\n$$\nw_{t+1} = w_t + \\underbrace{1}_\\eta  \\underbrace{ \\left[\\left[ sign(w_tTx_i) \\neq y_i\\right]\\right]}_{\\nu}(y_ix_i)\n\\tag{24}\n$$\n\n\nSGD算法关键是找出两个最佳的参数: 迭代次数 $t$ 和学习步长 $\\eta$。\n1. 对于迭代次数 $t$ 只能假设足够步数后是已经做到足够好，即通常设置一个大的数值即可；\n2. 学习步长 $\\eta$通常也很难选定(老师推荐：0.1126)。\n\n\nSGD算法的优缺点：\n1. 优点：计算简单快速，适用于大数据或者流式数据；\n2. 缺点：不稳定，需要一定的调试时间。\n\n<br><br>\n----------------------------------\n\n# Summary\n1. 首先引入了 Logistic Regression\n2. 然后对比了 Logistic Regression 和 Linear Regression, Linear Classification。\n3. 接着分析Logistic Regression 的误差方程，梯度方程，并用Gradient Descent 来最小化误差，并分析如何选择步长。\n\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\10\\10 - 1 - Logistic Regression Problem (14-33)\n\n[2] 机器学习基石(台湾大学-林轩田)\\10\\10 - 2 - Logistic Regression Error (15-58)\n\n[3] 机器学习基石(台湾大学-林轩田)\\10\\10 - 4 - Gradient Descent (19-18)\n\n\n<br>\n<br>\n---------------------------------------------","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-13-8.How can Machine Learn - Logistic Regression.md","raw":"---\ntitle: 8.How can Machine Learn? - Logistic Regression\ndate: 2017-10-13 12:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# How can Machine Learn? - Logistic Regression\n\n\n>这一节讨论与 Linear Regression 非常类似的Logistic Regression\n\n## 1. Introduction of Logistic Regression\n使用二元分类分析心脏病复发问题，其输出空间只含有两项{+1，-1}，分别表示复发和不发复发。在含有噪音的情况下，目标函数f可以使用目标分布P来表示，如公式（1）所示\n\n$$\nf(x) = sign \\left( P(+1|x) - \\frac{1}{2} \\right) ∈ \\{ +1,-1 \\}\n\\tag{1}\n$$\n\n但是实际情况，医生往往不会直接告诉病人说是否会心脏病复发，而是用概率，例如说有80%的可能性会复发，此种情况被称为软二元分类（soft binary classification），目标函数f的表达如公式（2）所示，其输出以概率的形式，在0~1之间。\n\n$$\nf(x) =  P(+1|x)  ∈ [ +1,-1 ]\n\\tag{2}\n$$\n\n但是病人的病历里面不可能记录以前有多少多少的几率复发/不复发，而是真实的记录病人是否复发。所以概率的情况来说，复发/不复发的情况就像是噪音了(因为偏离中间的概率值大)，所以我们把实际的训练数据看成是含有噪音的理想训练数据。这种问题如何求解呢？我们可以通过输入各属性 $x=(x_0,x_1, x_2, ..., x_n)$ 的加权总分数（weighted scores），如公式（3）所示\n\n$$\ns = \\sum\\limits_{i=0}^n w_ix_i = w^Tx\n\\tag{3}\n$$\n\n这里的s的值不在 0~1之间，所以我们还需要将他进行归一化处理，那就可以使用Logistic Regression。函数用表示 $\\theta(s)$，叫做Logistic Function 或者 Sigmoid Function，如公式（4）所示。分数s越大风险越高，分数s越小风险越低。假设函数h如公式(5)所示，函数曲线的示意图如图一所示。\n\n\n$$\n\\theta( s ) = \\frac{ e } {e + e^z} = \\frac{1}{1+e^{-z}}\n\\tag{4}\n$$\n$$\nh(x) = \\theta( w^T x ) = \\frac{ 1 } {1+e^{-w^Tx}}\n\\tag{5}\n$$\n\n![Logistic Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/57938b7a7b126af2ff2d5abdbea1a2fa0f6a4e1f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-1%20Logistic%20Curve.png)\n<center> 图一 Logistic Curve <sup>[1]</sup></center>\n\n\n观察函数的图形，该函数是一个平滑（处处可微分），单调（monotonic）的S形（sigmoid）函数，因此又被称为sigmoid函数。\n\nLogistic Regression 是当前业界比较常用的机器学习方法，用于估计某种事物的可能性，应用场合如：广告预测，购物推荐，患病可能性判断等。 Logistic Regression既可以做回归，也可以做分类（二分类为主）。\n\n\n\n<br><br>\n----------------------------------\n## 2. Comparison of Linear Regression, Logistic Classification and Logistic Regression\n将logisitic回归与之前学习的二元分类和线性回归做一对比，如图二所示。\n\n![Comparison of Three Linear Models](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a2cbd54aa0efe3a8f2f2f539a2fd595c4075971e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-2%20Comparison%20of%20Three%20Linear%20Models.png)\n<center> 图二 Comparison of Three Linear Models <sup>[2]</sup></center>\n\n其中分数s是在每个假设函数中都会出现的，前两个学习模型的错误衡量分别对应着0/1错误和平方错误，而logistic回归所使用的err函数应如何表示则是本节要介绍的内容。\n\n\n\n<br><br>\n----------------------------------\n## 3. Error Measurement of Logistic Regression - cross-entropy error\n这一节的推导需要对最大似然法和条件概率求解有一定的了解。\n\n>TODO:最大似然法 和 条件概率\n\n1.首先从 Logistic Function 可以推导出下面的公式（6），花括号上半部分不难理解，是将目标函数等式左右对调的结果，而下半部分的推导也很简单，因为+1与-1的总概率为1。\n$$\nf(x) = P(+1|x) \\Leftrightarrow P(y|x) = \\left\\{\n\\begin{aligned}\n& f(x)  \\quad   &for \\quad y = +1\\\\\n& 1-f(x)        &for \\quad y = -1\n\\end{aligned}\n\\right.\n\\tag{6}\n$$\n\n2.假设存在一个数据集 $D={(x_1, \\circ), (x_2, \\times), \\cdots, (x_n, \\times)}$,则通过目标函数产生此种数据集样本的概率可以用公式（7）表示。\n$$\nP(D) = P\\{ x_1 \\} P\\{ \\circ|x_1 \\} \\times P\\{ x_2 \\}P\\{ \\times|x_2 \\} \\times \\dots \\times P\\{ x_n \\} P\\{ \\times|x_n \\}\n\\tag{7}\n$$\n\n3.把公式（6）的公式带入公式（7)中，可以得到公式（8）。\n\n$$\nP(D) = P\\{ x_1 \\} f(x_1) \\times P\\{ x_2 \\} (1-f(x_2)) \\times \\dots \\times P\\{ x_n \\} (1-f(x_n))\n\\tag{7}\n$$\n\n4.f(x)是理想的函数，而我们实际训练得到的是hypothesis h(x)，所以我们还得想办法用h(x)代替f(x)，但是这样的前提是我们假设函数h(x)对数据集与f(x)产生的可能性很大，即likelihood(似然)，即我们在之前在VC Bound的推论中，知道在数据量足够大的情况下g(x)是会接近于f(x)的，如公式（8）所示\n\n$$\nP(D) = P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (1-h(x_2)) \\times \\dots \\times P\\{ x_n \\} (1-h(x_n))\n\\tag{8}\n$$\n\n\n5.那么最大似然我们表示为 likelihood(h)，在代入simoid函数的特性 $1-h(x) = h(-x)$，可以得到公式（9）\n\n$$\nlikelihood(h) = P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (h(-x_2)) \\times \\dots \\times P\\{ x_n \\} (h(-x_n))\n\\tag{9}\n$$\n\n6.那么最大的likelihood(h)如公式（10）所示，在计算最大的likelihood(h)时，所有$P(x_i)$的对大小没有影响，因为所有的假设函数都会乘以同样的$P(x_i)$，所以在表示的时候可以只考虑h(x)。\n\n$$\n\\begin{align}\n\\max \\limits_h \\quad likelihood(logistic \\quad h)\n&\\propto P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (h(-x_2)) \\times \\dots \\times P\\{ x_n \\} (h(-x_n)) \\\\\n&\\propto  \\prod \\limits_{i=1}^n h(y_ix_i) \\\\\n\\max \\limits_w \\quad likelihood(w) &\\propto  \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n\\max \\limits_w \\quad likelihood(w) &\\propto\\max \\quad ln \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n\\end{align}\n\\tag{10}\n$$\n\n\n7.连乘公式不容易求解最大问题，因此求其对数，此处以自然对数e为底，并代入sigmoid 方程，令$err(w,y_i,x_i) = ln(1+exp(-yw^Tx))$，如公式（11）所示，误差方程 $E_{in}$ 如公式（12)所示。\n\n$$\n\\begin{align}\n\\max \\limits_w \\quad likelihood(w)\n&\\propto \\max \\quad ln \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n&\\propto \\min\\limits_w \\quad \\frac{1}{N} \\sum\\limits_{i=1}^{n} -ln \\theta (y_iw^Tx_i) \\\\\n&\\propto \\min\\limits_w \\quad \\underbrace{ \\frac{1}{N} \\sum\\limits_{i=1}^{n} err(w,y_i,x_i) }_{E_{in}(w)}  \\quad \\quad\n\\end{align}\n\\tag{11}\n$$\n\n$$\nE_{in}(w) = \\frac{1}{N} \\sum\\limits_{i=1}^{n} ln(1+exp(-yw^Tx))\n\\tag{12}\n$$\n\n\n\n<br><br>\n----------------------------------\n## 4. Gradient of Logistic Regression Error\n上一节中，推导出logistic回归的 ，下一步的工作是寻找使得最小的权值向量w。\n对公式（12）进行求导，可以得到公式（13），推导过程参考老师用的方法：用圈圈代替exp里面的数，用正方形代替ln里面的表达式，这样可以使得推导过程看起来更加明白\n$$\n\\begin{align}\n\nE_{in}(w) &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} ln \\underbrace{(1+exp \\underbrace{(-yw^Tx)}_\\bigcirc)}_\\Box \\\\\n\n\n\\frac{\\partial E_{in}(w)}{\\partial w_i} &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left(\\frac{\\partial ln(\\Box)}{\\partial \\Box}\\right) \\left( \\frac{\\partial (1+exp(\\bigcirc))}{\\partial \\bigcirc} \\right) \\left( \\frac{\\partial - y_iw^Tx_i}{\\partial w_i} \\right) \\\\\n                                        &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\frac{1}{\\Box} \\right) \\left( exp(\\bigcirc) \\right) \\left( -y_i x_i \\right) \\\\\n\n                  \t\t\t\t\t\t&= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( exp(\\bigcirc) \\right) \\left(-y_i x_i \\right) \\quad \\because(\\frac{1}{\\Box}) \\approx 1\\\\\n\t\t\t\t\t\t\t\t\t\t&= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\theta(\\bigcirc) \\right) \\left( -y_i x_i \\right)  \\\\\n\t\t\t\t\t\t\t\t\t\t&= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\theta(-y_iw^Tx_n) \\right) \\left( -y_i x_i \\right)  \\\\\n\\end{align}\n\\tag{13}\n$$\n\n 从公式（13）可以看出，该函数是一个 $\\theta$ 函数作为权值，关于 $(-y_nx_n)$ 的加权求和函数。如果函数的所有权值为零，，可以看出 $y_i$与所有的对应的 $w^Tx_n$ 的同号，即线性可分。\n\n但是，求该问题的解不能使用类似求解线性回归时使用的闭式解的求解方式，此最小值又该如何计算？我们可以借鉴之前PLA的方法进行迭代求解，如公式（14）\n\n$$\nw_{t+1} = w_t + \\left[\\left[ sign(w_t^Tx_i) \\neq y_i \\right]\\right] y_ix_i\n\\tag{14}\n$$\n\n从公式（14）可以看出，当 $sign(w_t^Tx_i)=y_n$ 的时候，向量不改变，当 $sign(w_t^Tx_i) \\neq y_n$ 的时候 要加上 $y_ix_i$，然后我们把公式（14)做一定的调整得到公式（15），其中多乘以一个1作为更新的步长，用 $\\eta$表示，PLA中更新的部分用 $\\nu$ 来代表，表示更新的方向。该算法被称为迭代优化方法（iterative optimization approach）\n\n$$\n\\begin{align}\nw_{t+1} &= w_t + \\underbrace{1}_\\eta \\cdot\n                 \\underbrace{\\left[\\left[ sign(w_t^Tx_i) \\neq y_i \\right]\\right] y_ix_i}_\\nu \\\\\n\t    &= w_t + \\eta \\cdot \\nu\n\\end{align}\n\\tag{15}\n$$\n\n\n<br><br>\n----------------------------------\n## 5. Use Gradient Descent to Minimize the Error of Logistic Regression\n上面我们根据PLA的方法求得针对Logistic回归问题的误差方程，现在我们就需要找到最佳的参数 $\\eta$ 和 $\\nu$。首先误差方程的曲线图如图三所示。$E-{in}$ 是关于权值向量 $w$ 的示意图为一个平滑且可微的凸函数，其中图像谷底的点对应着最佳 $w$。\n\n![Iterative Optimization](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/efb59327c4bf9ddccb57959dcc04f7801267f667/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-3%20Iterative%20Optimization.png)\n<center> 图三 Iterative Optimization <sup>[3]</sup></center>\n\n为了分工明确，设 $\\nu$ 作为单位向量仅代表方向， $\\eta$ 代表步长表示每次更新改变的大小。在 $\\eta$ 固定的情况下，$\\nu$ 按照最陡峭的方向更改。即在 $\\eta$ 固定 $|\\nu| = 1$ 的情况下，有最快的速度找出使得 $E_{in}$ 最小的 $w$，得到公式（16）\n\n$$\n\\min\\limits_{|\\nu|=1} E_{in} \\underbrace{(w_i + \\eta \\nu)}_{w_{t+1}}\n\\tag{16}\n$$\n\n但是公式（16）依然很难求得最小的 $w$，当 $\\eta$ 很小时，我们通过泰勒展开公式（17）可以得到公式（18），其中 $w_t$ 对应 $x_0$\n\nTylor Expansion\n$$\nf(x) = f(x_0) + \\frac{f'(x_0)}{1!}(x-x_0) + \\frac{f^{(2)}(x_0)}{2!}(x-x_0)^2 + \\dots + \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + R_n(x)\n\\tag{17}\n$$\n\n$$\n\\begin{align}\n\\min\\limits_{|\\nu|=1} E_{in} (w_i + \\eta \\nu ^ T) &\\approx   E_{in}(w_t) + \\left(\\left( w_t + \\eta\\nu^T  \\right) - w_t \\right) \\frac{\\nabla E_{in}(w_t)}{1!} \\\\\n\n                                                  &= E_in(w_t) + \\eta\\nu^T \\nabla E_{in}(w_t)\n\n\\end{align}\n\\tag{18}\n$$\n\n\n接着我们继续分析公式（18），其中 $E_in{w_t}$ 我们是知道的， $\\eta$ 是给定的步长， $\\nabla E_{in}(w_t)$ 也是知道的，所以求解公式（18）的最小值问题，可以转换成求解 $\\nu^T nabla E_{in}(w_t)$ 的最小值，即公式（19）\n\n$$\n\\min\\limits_{|\\nu|=1} \\nu^T \\nabla E_{in}(w_t)\n\\tag{19}\n$$\n\n两个向量最小的情况为其方向相反，即内积为负，得到公式（20）,这种情况下 $\\nu$ 是一个单位向量\n\n$$\n\\begin{align}\n\\min\\limits_{|\\nu|=1} \\nu^T \\nabla E_{in}(w_t) &= -1 \\\\\n                                           \\nu &= - \\frac{\\nabla E_{in}(w_t)}{||\\nabla E_{in}(w_t)||}\n\\end{align}\n\\tag{20}\n$$\n\n所以把公式（20）带入公式（15），可以得到公式（21）\n$$\n\\begin{align}\nw_{t+1} &= w_t - \\eta \\cdot \\frac{\\nabla E_{in}(w_t)}{||\\nabla E_{in}(w_t)||}\n\\end{align}\n\\tag{21}\n$$\n\n从公式（21）可以看出，每次更新权值，w都是减少一点（具体多少要看我们谁的哪个的步长，已经误差大小），按照此种方式更新可以找到使得最小的w。此种方式称作梯度下降（gradient descent）。\n\n\n\n<br><br>\n----------------------------------\n## 6. Choose Step Length for Gradient Descent\n由上面的公式（21）可以看出，w受 步长大小 $\\eta$ 和 误差大小的影响。在一定的 $\\eta$ 下，越接近谷底，纠正的也越来越小；但是如果选择的 $\\eta$ 太大，一个新就更新到对面的山峰上面去了（可能会导致纠正后误差更大），或者  $\\eta$ 太小，更新好久还没有更新到需要的准确度。所以选择适当的 $\\eta$ 很重要。如图四所示。\n![Choice of Step Length](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/09288541bf7cddb805bd091f25cf104e438fc179/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-4%20Choice%20of%20eta.png)\n<center> 图四 Choice of Step Length <sup>[3]</sup></center>\n\n因为 $\\eta$ 与 梯度大小 ${||\\nabla E_{in}(w_t)||}$ 正比，所以我们可以得到公式（22）\n$$\n\\eta_{new} = \\frac{ \\eta_{old}}{||\\nabla E_{in}(w_t)||}\n\\tag{22}\n$$\n\n结合公式（21）（22），我们调整的公式（21）为（23）\n\n$$\n\\begin{align}\nw_{t+1} &= w_t - \\eta_{new} \\cdot {\\nabla E_{in}(w_t)}\n\\end{align}\n\\tag{21}\n$$\n\n此时的 $\\eta$ 被称作固定的学习速率（fixed learning rate）。最终得到Logstic Regression 的步骤如下：\n1. 设置权值w为 $w_0$，迭代次数他，并计算梯度 $\\nabla E_{in}(w_t) = \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\theta \\left( -y_iw_t^Tx_i \\right)\\left( -y_ix_i \\right)$\n2. 不断迭代，并更新权值向量w，$w_{t+1} = w_t - \\eta_{new} \\cdot {\\nabla E_{in}(w_t)}$，直到误差函数的导数近似于0，或者迭代一定的次数。\n\n\nGradient Descent 劣势分析：\n1. 不稳定:如果选择的步长太大太小，都会对算法有影响\n2. 局部最优：如果函数不是凸函数的话，可能存在多个局部最优点，那样的话，Gradient Descent只能找到最近的局部最优。\n3. 计算复杂度大，为O(N)，因为导数需要对所有的点进行一次遍历\n\n\n<br><br>\n----------------------------------\n## 7. Stochastic Gradient Descent - Another Approach of Gradient Descent\n上面讨论了 Gradient Descent,以及计算复杂度大的问题，这一节我们讨论另一种方法，可以将计算复杂度降成O(1)级别。这种方法就是Stochastic Gradient Descent（随机梯度），用符号 $\\nabla_w err(w, w_i, y_i)$ 表示， 核心思想是：通过N个样本中随机抽取一个样本点求出的梯度取代原来的期望梯度。 随机梯度值可以看做真实的梯度值加上一个噪音，使用随机梯度取代真实梯度做梯度下降的算法称作随机梯度下降 stochastic gradient descent(SGD)。在迭代次数足够多的情况下，平均的随机梯度和平均的真实梯度相差不大。真实梯度与随机梯度的关系如公式（22）所示\n\n$$\n\\nabla_w E_{in}(w_t) = \\varepsilon_{i} \\cdot \\nabla_w err( w, x_i, y_i)\n\\tag{22}\n$$\n\nLogistic Regression 的SGD的迭代如公式(23)所示。\n\n$$\n\\begin{align}\nw_{t+1} &= w_t - \\eta \\cdot( \\nabla_w err( w, x_i, y_i) )(y_ix_i) \\\\\n        &= w_t + \\eta  \\underbrace{\\theta (-y_i w_t^T x_i)(y_i x_i)}_{- \\nabla_w err( w, x_i, y_i)}(y_ix_i)\n\\end{align}\n\\tag{23}\n$$\n\n对比之前的PLA算法的公式（如公式（24）），容易发现两个公式很类似，因此logistic Regession 的SGD算法又叫\"软\"PLA，因为权值并没有那么绝对不是1就是0，而是一个在0~1之间的值。如果 $\\eta = 1$ 且 $w_t^T x_i \\approx \\infty$ 始终是一个很大的值，则logistic Regession 的SGD相当于是PLA算法。\n\n$$\nw_{t+1} = w_t + \\underbrace{1}_\\eta  \\underbrace{ \\left[\\left[ sign(w_tTx_i) \\neq y_i\\right]\\right]}_{\\nu}(y_ix_i)\n\\tag{24}\n$$\n\n\nSGD算法关键是找出两个最佳的参数: 迭代次数 $t$ 和学习步长 $\\eta$。\n1. 对于迭代次数 $t$ 只能假设足够步数后是已经做到足够好，即通常设置一个大的数值即可；\n2. 学习步长 $\\eta$通常也很难选定(老师推荐：0.1126)。\n\n\nSGD算法的优缺点：\n1. 优点：计算简单快速，适用于大数据或者流式数据；\n2. 缺点：不稳定，需要一定的调试时间。\n\n<br><br>\n----------------------------------\n\n# Summary\n1. 首先引入了 Logistic Regression\n2. 然后对比了 Logistic Regression 和 Linear Regression, Linear Classification。\n3. 接着分析Logistic Regression 的误差方程，梯度方程，并用Gradient Descent 来最小化误差，并分析如何选择步长。\n\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\10\\10 - 1 - Logistic Regression Problem (14-33)\n\n[2] 机器学习基石(台湾大学-林轩田)\\10\\10 - 2 - Logistic Regression Error (15-58)\n\n[3] 机器学习基石(台湾大学-林轩田)\\10\\10 - 4 - Gradient Descent (19-18)\n\n\n<br>\n<br>\n---------------------------------------------","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-13-8.How can Machine Learn - Logistic Regression","published":1,"updated":"2018-04-14T19:42:06.505Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2eh001prwtjxsaycyxb","content":"<h1 id=\"How-can-Machine-Learn-Logistic-Regression\"><a href=\"#How-can-Machine-Learn-Logistic-Regression\" class=\"headerlink\" title=\"How can Machine Learn? - Logistic Regression\"></a>How can Machine Learn? - Logistic Regression</h1><blockquote>\n<p>这一节讨论与 Linear Regression 非常类似的Logistic Regression</p>\n</blockquote>\n<h2 id=\"1-Introduction-of-Logistic-Regression\"><a href=\"#1-Introduction-of-Logistic-Regression\" class=\"headerlink\" title=\"1. Introduction of Logistic Regression\"></a>1. Introduction of Logistic Regression</h2><p>使用二元分类分析心脏病复发问题，其输出空间只含有两项{+1，-1}，分别表示复发和不发复发。在含有噪音的情况下，目标函数f可以使用目标分布P来表示，如公式（1）所示</p>\n<script type=\"math/tex; mode=display\">\nf(x) = sign \\left( P(+1|x) - \\frac{1}{2} \\right) ∈ \\{ +1,-1 \\}\n\\tag{1}</script><p>但是实际情况，医生往往不会直接告诉病人说是否会心脏病复发，而是用概率，例如说有80%的可能性会复发，此种情况被称为软二元分类（soft binary classification），目标函数f的表达如公式（2）所示，其输出以概率的形式，在0~1之间。</p>\n<script type=\"math/tex; mode=display\">\nf(x) =  P(+1|x)  ∈ [ +1,-1 ]\n\\tag{2}</script><p>但是病人的病历里面不可能记录以前有多少多少的几率复发/不复发，而是真实的记录病人是否复发。所以概率的情况来说，复发/不复发的情况就像是噪音了(因为偏离中间的概率值大)，所以我们把实际的训练数据看成是含有噪音的理想训练数据。这种问题如何求解呢？我们可以通过输入各属性 $x=(x_0,x_1, x_2, …, x_n)$ 的加权总分数（weighted scores），如公式（3）所示</p>\n<script type=\"math/tex; mode=display\">\ns = \\sum\\limits_{i=0}^n w_ix_i = w^Tx\n\\tag{3}</script><p>这里的s的值不在 0~1之间，所以我们还需要将他进行归一化处理，那就可以使用Logistic Regression。函数用表示 $\\theta(s)$，叫做Logistic Function 或者 Sigmoid Function，如公式（4）所示。分数s越大风险越高，分数s越小风险越低。假设函数h如公式(5)所示，函数曲线的示意图如图一所示。</p>\n<script type=\"math/tex; mode=display\">\n\\theta( s ) = \\frac{ e } {e + e^z} = \\frac{1}{1+e^{-z}}\n\\tag{4}</script><script type=\"math/tex; mode=display\">\nh(x) = \\theta( w^T x ) = \\frac{ 1 } {1+e^{-w^Tx}}\n\\tag{5}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/57938b7a7b126af2ff2d5abdbea1a2fa0f6a4e1f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-1%20Logistic%20Curve.png\" alt=\"Logistic Curve\"></p>\n<center> 图一 Logistic Curve <sup>[1]</sup></center>\n\n\n<p>观察函数的图形，该函数是一个平滑（处处可微分），单调（monotonic）的S形（sigmoid）函数，因此又被称为sigmoid函数。</p>\n<p>Logistic Regression 是当前业界比较常用的机器学习方法，用于估计某种事物的可能性，应用场合如：广告预测，购物推荐，患病可能性判断等。 Logistic Regression既可以做回归，也可以做分类（二分类为主）。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Comparison-of-Linear-Regression-Logistic-Classification-and-Logistic-Regression\"><a href=\"#2-Comparison-of-Linear-Regression-Logistic-Classification-and-Logistic-Regression\" class=\"headerlink\" title=\"2. Comparison of Linear Regression, Logistic Classification and Logistic Regression\"></a>2. Comparison of Linear Regression, Logistic Classification and Logistic Regression</h2><p>将logisitic回归与之前学习的二元分类和线性回归做一对比，如图二所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a2cbd54aa0efe3a8f2f2f539a2fd595c4075971e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-2%20Comparison%20of%20Three%20Linear%20Models.png\" alt=\"Comparison of Three Linear Models\"></p>\n<center> 图二 Comparison of Three Linear Models <sup>[2]</sup></center>\n\n<p>其中分数s是在每个假设函数中都会出现的，前两个学习模型的错误衡量分别对应着0/1错误和平方错误，而logistic回归所使用的err函数应如何表示则是本节要介绍的内容。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Error-Measurement-of-Logistic-Regression-cross-entropy-error\"><a href=\"#3-Error-Measurement-of-Logistic-Regression-cross-entropy-error\" class=\"headerlink\" title=\"3. Error Measurement of Logistic Regression - cross-entropy error\"></a>3. Error Measurement of Logistic Regression - cross-entropy error</h2><p>这一节的推导需要对最大似然法和条件概率求解有一定的了解。</p>\n<blockquote>\n<p>TODO:最大似然法 和 条件概率</p>\n</blockquote>\n<p>1.首先从 Logistic Function 可以推导出下面的公式（6），花括号上半部分不难理解，是将目标函数等式左右对调的结果，而下半部分的推导也很简单，因为+1与-1的总概率为1。</p>\n<script type=\"math/tex; mode=display\">\nf(x) = P(+1|x) \\Leftrightarrow P(y|x) = \\left\\{\n\\begin{aligned}\n& f(x)  \\quad   &for \\quad y = +1\\\\\n& 1-f(x)        &for \\quad y = -1\n\\end{aligned}\n\\right.\n\\tag{6}</script><p>2.假设存在一个数据集 $D={(x_1, \\circ), (x_2, \\times), \\cdots, (x_n, \\times)}$,则通过目标函数产生此种数据集样本的概率可以用公式（7）表示。</p>\n<script type=\"math/tex; mode=display\">\nP(D) = P\\{ x_1 \\} P\\{ \\circ|x_1 \\} \\times P\\{ x_2 \\}P\\{ \\times|x_2 \\} \\times \\dots \\times P\\{ x_n \\} P\\{ \\times|x_n \\}\n\\tag{7}</script><p>3.把公式（6）的公式带入公式（7)中，可以得到公式（8）。</p>\n<script type=\"math/tex; mode=display\">\nP(D) = P\\{ x_1 \\} f(x_1) \\times P\\{ x_2 \\} (1-f(x_2)) \\times \\dots \\times P\\{ x_n \\} (1-f(x_n))\n\\tag{7}</script><p>4.f(x)是理想的函数，而我们实际训练得到的是hypothesis h(x)，所以我们还得想办法用h(x)代替f(x)，但是这样的前提是我们假设函数h(x)对数据集与f(x)产生的可能性很大，即likelihood(似然)，即我们在之前在VC Bound的推论中，知道在数据量足够大的情况下g(x)是会接近于f(x)的，如公式（8）所示</p>\n<script type=\"math/tex; mode=display\">\nP(D) = P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (1-h(x_2)) \\times \\dots \\times P\\{ x_n \\} (1-h(x_n))\n\\tag{8}</script><p>5.那么最大似然我们表示为 likelihood(h)，在代入simoid函数的特性 $1-h(x) = h(-x)$，可以得到公式（9）</p>\n<script type=\"math/tex; mode=display\">\nlikelihood(h) = P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (h(-x_2)) \\times \\dots \\times P\\{ x_n \\} (h(-x_n))\n\\tag{9}</script><p>6.那么最大的likelihood(h)如公式（10）所示，在计算最大的likelihood(h)时，所有$P(x_i)$的对大小没有影响，因为所有的假设函数都会乘以同样的$P(x_i)$，所以在表示的时候可以只考虑h(x)。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\max \\limits_h \\quad likelihood(logistic \\quad h)\n&\\propto P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (h(-x_2)) \\times \\dots \\times P\\{ x_n \\} (h(-x_n)) \\\\\n&\\propto  \\prod \\limits_{i=1}^n h(y_ix_i) \\\\\n\\max \\limits_w \\quad likelihood(w) &\\propto  \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n\\max \\limits_w \\quad likelihood(w) &\\propto\\max \\quad ln \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n\\end{align}\n\\tag{10}</script><p>7.连乘公式不容易求解最大问题，因此求其对数，此处以自然对数e为底，并代入sigmoid 方程，令$err(w,y_i,x_i) = ln(1+exp(-yw^Tx))$，如公式（11）所示，误差方程 $E_{in}$ 如公式（12)所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\max \\limits_w \\quad likelihood(w)\n&\\propto \\max \\quad ln \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n&\\propto \\min\\limits_w \\quad \\frac{1}{N} \\sum\\limits_{i=1}^{n} -ln \\theta (y_iw^Tx_i) \\\\\n&\\propto \\min\\limits_w \\quad \\underbrace{ \\frac{1}{N} \\sum\\limits_{i=1}^{n} err(w,y_i,x_i) }_{E_{in}(w)}  \\quad \\quad\n\\end{align}\n\\tag{11}</script><script type=\"math/tex; mode=display\">\nE_{in}(w) = \\frac{1}{N} \\sum\\limits_{i=1}^{n} ln(1+exp(-yw^Tx))\n\\tag{12}</script><h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"4-Gradient-of-Logistic-Regression-Error\"><a href=\"#4-Gradient-of-Logistic-Regression-Error\" class=\"headerlink\" title=\"4. Gradient of Logistic Regression Error\"></a>4. Gradient of Logistic Regression Error</h2><p>上一节中，推导出logistic回归的 ，下一步的工作是寻找使得最小的权值向量w。<br>对公式（12）进行求导，可以得到公式（13），推导过程参考老师用的方法：用圈圈代替exp里面的数，用正方形代替ln里面的表达式，这样可以使得推导过程看起来更加明白</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\nE_{in}(w) &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} ln \\underbrace{(1+exp \\underbrace{(-yw^Tx)}_\\bigcirc)}_\\Box \\\\\n\n\n\\frac{\\partial E_{in}(w)}{\\partial w_i} &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left(\\frac{\\partial ln(\\Box)}{\\partial \\Box}\\right) \\left( \\frac{\\partial (1+exp(\\bigcirc))}{\\partial \\bigcirc} \\right) \\left( \\frac{\\partial - y_iw^Tx_i}{\\partial w_i} \\right) \\\\\n                                        &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\frac{1}{\\Box} \\right) \\left( exp(\\bigcirc) \\right) \\left( -y_i x_i \\right) \\\\\n\n                                          &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( exp(\\bigcirc) \\right) \\left(-y_i x_i \\right) \\quad \\because(\\frac{1}{\\Box}) \\approx 1\\\\\n                                        &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\theta(\\bigcirc) \\right) \\left( -y_i x_i \\right)  \\\\\n                                        &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\theta(-y_iw^Tx_n) \\right) \\left( -y_i x_i \\right)  \\\\\n\\end{align}\n\\tag{13}</script><p> 从公式（13）可以看出，该函数是一个 $\\theta$ 函数作为权值，关于 $(-y_nx_n)$ 的加权求和函数。如果函数的所有权值为零，，可以看出 $y_i$与所有的对应的 $w^Tx_n$ 的同号，即线性可分。</p>\n<p>但是，求该问题的解不能使用类似求解线性回归时使用的闭式解的求解方式，此最小值又该如何计算？我们可以借鉴之前PLA的方法进行迭代求解，如公式（14）</p>\n<script type=\"math/tex; mode=display\">\nw_{t+1} = w_t + \\left[\\left[ sign(w_t^Tx_i) \\neq y_i \\right]\\right] y_ix_i\n\\tag{14}</script><p>从公式（14）可以看出，当 $sign(w_t^Tx_i)=y_n$ 的时候，向量不改变，当 $sign(w_t^Tx_i) \\neq y_n$ 的时候 要加上 $y_ix_i$，然后我们把公式（14)做一定的调整得到公式（15），其中多乘以一个1作为更新的步长，用 $\\eta$表示，PLA中更新的部分用 $\\nu$ 来代表，表示更新的方向。该算法被称为迭代优化方法（iterative optimization approach）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nw_{t+1} &= w_t + \\underbrace{1}_\\eta \\cdot\n                 \\underbrace{\\left[\\left[ sign(w_t^Tx_i) \\neq y_i \\right]\\right] y_ix_i}_\\nu \\\\\n        &= w_t + \\eta \\cdot \\nu\n\\end{align}\n\\tag{15}</script><h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"5-Use-Gradient-Descent-to-Minimize-the-Error-of-Logistic-Regression\"><a href=\"#5-Use-Gradient-Descent-to-Minimize-the-Error-of-Logistic-Regression\" class=\"headerlink\" title=\"5. Use Gradient Descent to Minimize the Error of Logistic Regression\"></a>5. Use Gradient Descent to Minimize the Error of Logistic Regression</h2><p>上面我们根据PLA的方法求得针对Logistic回归问题的误差方程，现在我们就需要找到最佳的参数 $\\eta$ 和 $\\nu$。首先误差方程的曲线图如图三所示。$E-{in}$ 是关于权值向量 $w$ 的示意图为一个平滑且可微的凸函数，其中图像谷底的点对应着最佳 $w$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/efb59327c4bf9ddccb57959dcc04f7801267f667/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-3%20Iterative%20Optimization.png\" alt=\"Iterative Optimization\"></p>\n<center> 图三 Iterative Optimization <sup>[3]</sup></center>\n\n<p>为了分工明确，设 $\\nu$ 作为单位向量仅代表方向， $\\eta$ 代表步长表示每次更新改变的大小。在 $\\eta$ 固定的情况下，$\\nu$ 按照最陡峭的方向更改。即在 $\\eta$ 固定 $|\\nu| = 1$ 的情况下，有最快的速度找出使得 $E_{in}$ 最小的 $w$，得到公式（16）</p>\n<script type=\"math/tex; mode=display\">\n\\min\\limits_{|\\nu|=1} E_{in} \\underbrace{(w_i + \\eta \\nu)}_{w_{t+1}}\n\\tag{16}</script><p>但是公式（16）依然很难求得最小的 $w$，当 $\\eta$ 很小时，我们通过泰勒展开公式（17）可以得到公式（18），其中 $w_t$ 对应 $x_0$</p>\n<p>Tylor Expansion</p>\n<script type=\"math/tex; mode=display\">\nf(x) = f(x_0) + \\frac{f'(x_0)}{1!}(x-x_0) + \\frac{f^{(2)}(x_0)}{2!}(x-x_0)^2 + \\dots + \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + R_n(x)\n\\tag{17}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\n\\min\\limits_{|\\nu|=1} E_{in} (w_i + \\eta \\nu ^ T) &\\approx   E_{in}(w_t) + \\left(\\left( w_t + \\eta\\nu^T  \\right) - w_t \\right) \\frac{\\nabla E_{in}(w_t)}{1!} \\\\\n\n                                                  &= E_in(w_t) + \\eta\\nu^T \\nabla E_{in}(w_t)\n\n\\end{align}\n\\tag{18}</script><p>接着我们继续分析公式（18），其中 $E_in{w_t}$ 我们是知道的， $\\eta$ 是给定的步长， $\\nabla E_{in}(w_t)$ 也是知道的，所以求解公式（18）的最小值问题，可以转换成求解 $\\nu^T nabla E_{in}(w_t)$ 的最小值，即公式（19）</p>\n<script type=\"math/tex; mode=display\">\n\\min\\limits_{|\\nu|=1} \\nu^T \\nabla E_{in}(w_t)\n\\tag{19}</script><p>两个向量最小的情况为其方向相反，即内积为负，得到公式（20）,这种情况下 $\\nu$ 是一个单位向量</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\min\\limits_{|\\nu|=1} \\nu^T \\nabla E_{in}(w_t) &= -1 \\\\\n                                           \\nu &= - \\frac{\\nabla E_{in}(w_t)}{||\\nabla E_{in}(w_t)||}\n\\end{align}\n\\tag{20}</script><p>所以把公式（20）带入公式（15），可以得到公式（21）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nw_{t+1} &= w_t - \\eta \\cdot \\frac{\\nabla E_{in}(w_t)}{||\\nabla E_{in}(w_t)||}\n\\end{align}\n\\tag{21}</script><p>从公式（21）可以看出，每次更新权值，w都是减少一点（具体多少要看我们谁的哪个的步长，已经误差大小），按照此种方式更新可以找到使得最小的w。此种方式称作梯度下降（gradient descent）。</p>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"6-Choose-Step-Length-for-Gradient-Descent\"><a href=\"#6-Choose-Step-Length-for-Gradient-Descent\" class=\"headerlink\" title=\"6. Choose Step Length for Gradient Descent\"></a>6. Choose Step Length for Gradient Descent</h2><p>由上面的公式（21）可以看出，w受 步长大小 $\\eta$ 和 误差大小的影响。在一定的 $\\eta$ 下，越接近谷底，纠正的也越来越小；但是如果选择的 $\\eta$ 太大，一个新就更新到对面的山峰上面去了（可能会导致纠正后误差更大），或者  $\\eta$ 太小，更新好久还没有更新到需要的准确度。所以选择适当的 $\\eta$ 很重要。如图四所示。<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/09288541bf7cddb805bd091f25cf104e438fc179/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-4%20Choice%20of%20eta.png\" alt=\"Choice of Step Length\"></p>\n<center> 图四 Choice of Step Length <sup>[3]</sup></center>\n\n<p>因为 $\\eta$ 与 梯度大小 ${||\\nabla E_{in}(w_t)||}$ 正比，所以我们可以得到公式（22）</p>\n<script type=\"math/tex; mode=display\">\n\\eta_{new} = \\frac{ \\eta_{old}}{||\\nabla E_{in}(w_t)||}\n\\tag{22}</script><p>结合公式（21）（22），我们调整的公式（21）为（23）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nw_{t+1} &= w_t - \\eta_{new} \\cdot {\\nabla E_{in}(w_t)}\n\\end{align}\n\\tag{21}</script><p>此时的 $\\eta$ 被称作固定的学习速率（fixed learning rate）。最终得到Logstic Regression 的步骤如下：</p>\n<ol>\n<li>设置权值w为 $w_0$，迭代次数他，并计算梯度 $\\nabla E_{in}(w_t) = \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\theta \\left( -y_iw_t^Tx_i \\right)\\left( -y_ix_i \\right)$</li>\n<li>不断迭代，并更新权值向量w，$w_{t+1} = w_t - \\eta_{new} \\cdot {\\nabla E_{in}(w_t)}$，直到误差函数的导数近似于0，或者迭代一定的次数。</li>\n</ol>\n<p>Gradient Descent 劣势分析：</p>\n<ol>\n<li>不稳定:如果选择的步长太大太小，都会对算法有影响</li>\n<li>局部最优：如果函数不是凸函数的话，可能存在多个局部最优点，那样的话，Gradient Descent只能找到最近的局部最优。</li>\n<li>计算复杂度大，为O(N)，因为导数需要对所有的点进行一次遍历</li>\n</ol>\n<h2 id=\"-5\"><a href=\"#-5\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"7-Stochastic-Gradient-Descent-Another-Approach-of-Gradient-Descent\"><a href=\"#7-Stochastic-Gradient-Descent-Another-Approach-of-Gradient-Descent\" class=\"headerlink\" title=\"7. Stochastic Gradient Descent - Another Approach of Gradient Descent\"></a>7. Stochastic Gradient Descent - Another Approach of Gradient Descent</h2><p>上面讨论了 Gradient Descent,以及计算复杂度大的问题，这一节我们讨论另一种方法，可以将计算复杂度降成O(1)级别。这种方法就是Stochastic Gradient Descent（随机梯度），用符号 $\\nabla_w err(w, w_i, y_i)$ 表示， 核心思想是：通过N个样本中随机抽取一个样本点求出的梯度取代原来的期望梯度。 随机梯度值可以看做真实的梯度值加上一个噪音，使用随机梯度取代真实梯度做梯度下降的算法称作随机梯度下降 stochastic gradient descent(SGD)。在迭代次数足够多的情况下，平均的随机梯度和平均的真实梯度相差不大。真实梯度与随机梯度的关系如公式（22）所示</p>\n<script type=\"math/tex; mode=display\">\n\\nabla_w E_{in}(w_t) = \\varepsilon_{i} \\cdot \\nabla_w err( w, x_i, y_i)\n\\tag{22}</script><p>Logistic Regression 的SGD的迭代如公式(23)所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nw_{t+1} &= w_t - \\eta \\cdot( \\nabla_w err( w, x_i, y_i) )(y_ix_i) \\\\\n        &= w_t + \\eta  \\underbrace{\\theta (-y_i w_t^T x_i)(y_i x_i)}_{- \\nabla_w err( w, x_i, y_i)}(y_ix_i)\n\\end{align}\n\\tag{23}</script><p>对比之前的PLA算法的公式（如公式（24）），容易发现两个公式很类似，因此logistic Regession 的SGD算法又叫”软”PLA，因为权值并没有那么绝对不是1就是0，而是一个在0~1之间的值。如果 $\\eta = 1$ 且 $w_t^T x_i \\approx \\infty$ 始终是一个很大的值，则logistic Regession 的SGD相当于是PLA算法。</p>\n<script type=\"math/tex; mode=display\">\nw_{t+1} = w_t + \\underbrace{1}_\\eta  \\underbrace{ \\left[\\left[ sign(w_tTx_i) \\neq y_i\\right]\\right]}_{\\nu}(y_ix_i)\n\\tag{24}</script><p>SGD算法关键是找出两个最佳的参数: 迭代次数 $t$ 和学习步长 $\\eta$。</p>\n<ol>\n<li>对于迭代次数 $t$ 只能假设足够步数后是已经做到足够好，即通常设置一个大的数值即可；</li>\n<li>学习步长 $\\eta$通常也很难选定(老师推荐：0.1126)。</li>\n</ol>\n<p>SGD算法的优缺点：</p>\n<ol>\n<li>优点：计算简单快速，适用于大数据或者流式数据；</li>\n<li>缺点：不稳定，需要一定的调试时间。</li>\n</ol>\n<h2 id=\"-6\"><a href=\"#-6\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先引入了 Logistic Regression</li>\n<li>然后对比了 Logistic Regression 和 Linear Regression, Linear Classification。</li>\n<li>接着分析Logistic Regression 的误差方程，梯度方程，并用Gradient Descent 来最小化误差，并分析如何选择步长。</li>\n</ol>\n<h2 id=\"-7\"><a href=\"#-7\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\10\\10 - 1 - Logistic Regression Problem (14-33)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\10\\10 - 2 - Logistic Regression Error (15-58)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\10\\10 - 4 - Gradient Descent (19-18)</p>\n<p><br></p>\n<h2 id=\"-8\"><a href=\"#-8\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Logistic-Regression\"><a href=\"#How-can-Machine-Learn-Logistic-Regression\" class=\"headerlink\" title=\"How can Machine Learn? - Logistic Regression\"></a>How can Machine Learn? - Logistic Regression</h1><blockquote>\n<p>这一节讨论与 Linear Regression 非常类似的Logistic Regression</p>\n</blockquote>\n<h2 id=\"1-Introduction-of-Logistic-Regression\"><a href=\"#1-Introduction-of-Logistic-Regression\" class=\"headerlink\" title=\"1. Introduction of Logistic Regression\"></a>1. Introduction of Logistic Regression</h2><p>使用二元分类分析心脏病复发问题，其输出空间只含有两项{+1，-1}，分别表示复发和不发复发。在含有噪音的情况下，目标函数f可以使用目标分布P来表示，如公式（1）所示</p>\n<script type=\"math/tex; mode=display\">\nf(x) = sign \\left( P(+1|x) - \\frac{1}{2} \\right) ∈ \\{ +1,-1 \\}\n\\tag{1}</script><p>但是实际情况，医生往往不会直接告诉病人说是否会心脏病复发，而是用概率，例如说有80%的可能性会复发，此种情况被称为软二元分类（soft binary classification），目标函数f的表达如公式（2）所示，其输出以概率的形式，在0~1之间。</p>\n<script type=\"math/tex; mode=display\">\nf(x) =  P(+1|x)  ∈ [ +1,-1 ]\n\\tag{2}</script><p>但是病人的病历里面不可能记录以前有多少多少的几率复发/不复发，而是真实的记录病人是否复发。所以概率的情况来说，复发/不复发的情况就像是噪音了(因为偏离中间的概率值大)，所以我们把实际的训练数据看成是含有噪音的理想训练数据。这种问题如何求解呢？我们可以通过输入各属性 $x=(x_0,x_1, x_2, …, x_n)$ 的加权总分数（weighted scores），如公式（3）所示</p>\n<script type=\"math/tex; mode=display\">\ns = \\sum\\limits_{i=0}^n w_ix_i = w^Tx\n\\tag{3}</script><p>这里的s的值不在 0~1之间，所以我们还需要将他进行归一化处理，那就可以使用Logistic Regression。函数用表示 $\\theta(s)$，叫做Logistic Function 或者 Sigmoid Function，如公式（4）所示。分数s越大风险越高，分数s越小风险越低。假设函数h如公式(5)所示，函数曲线的示意图如图一所示。</p>\n<script type=\"math/tex; mode=display\">\n\\theta( s ) = \\frac{ e } {e + e^z} = \\frac{1}{1+e^{-z}}\n\\tag{4}</script><script type=\"math/tex; mode=display\">\nh(x) = \\theta( w^T x ) = \\frac{ 1 } {1+e^{-w^Tx}}\n\\tag{5}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/57938b7a7b126af2ff2d5abdbea1a2fa0f6a4e1f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-1%20Logistic%20Curve.png\" alt=\"Logistic Curve\"></p>\n<center> 图一 Logistic Curve <sup>[1]</sup></center>\n\n\n<p>观察函数的图形，该函数是一个平滑（处处可微分），单调（monotonic）的S形（sigmoid）函数，因此又被称为sigmoid函数。</p>\n<p>Logistic Regression 是当前业界比较常用的机器学习方法，用于估计某种事物的可能性，应用场合如：广告预测，购物推荐，患病可能性判断等。 Logistic Regression既可以做回归，也可以做分类（二分类为主）。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Comparison-of-Linear-Regression-Logistic-Classification-and-Logistic-Regression\"><a href=\"#2-Comparison-of-Linear-Regression-Logistic-Classification-and-Logistic-Regression\" class=\"headerlink\" title=\"2. Comparison of Linear Regression, Logistic Classification and Logistic Regression\"></a>2. Comparison of Linear Regression, Logistic Classification and Logistic Regression</h2><p>将logisitic回归与之前学习的二元分类和线性回归做一对比，如图二所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a2cbd54aa0efe3a8f2f2f539a2fd595c4075971e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-2%20Comparison%20of%20Three%20Linear%20Models.png\" alt=\"Comparison of Three Linear Models\"></p>\n<center> 图二 Comparison of Three Linear Models <sup>[2]</sup></center>\n\n<p>其中分数s是在每个假设函数中都会出现的，前两个学习模型的错误衡量分别对应着0/1错误和平方错误，而logistic回归所使用的err函数应如何表示则是本节要介绍的内容。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Error-Measurement-of-Logistic-Regression-cross-entropy-error\"><a href=\"#3-Error-Measurement-of-Logistic-Regression-cross-entropy-error\" class=\"headerlink\" title=\"3. Error Measurement of Logistic Regression - cross-entropy error\"></a>3. Error Measurement of Logistic Regression - cross-entropy error</h2><p>这一节的推导需要对最大似然法和条件概率求解有一定的了解。</p>\n<blockquote>\n<p>TODO:最大似然法 和 条件概率</p>\n</blockquote>\n<p>1.首先从 Logistic Function 可以推导出下面的公式（6），花括号上半部分不难理解，是将目标函数等式左右对调的结果，而下半部分的推导也很简单，因为+1与-1的总概率为1。</p>\n<script type=\"math/tex; mode=display\">\nf(x) = P(+1|x) \\Leftrightarrow P(y|x) = \\left\\{\n\\begin{aligned}\n& f(x)  \\quad   &for \\quad y = +1\\\\\n& 1-f(x)        &for \\quad y = -1\n\\end{aligned}\n\\right.\n\\tag{6}</script><p>2.假设存在一个数据集 $D={(x_1, \\circ), (x_2, \\times), \\cdots, (x_n, \\times)}$,则通过目标函数产生此种数据集样本的概率可以用公式（7）表示。</p>\n<script type=\"math/tex; mode=display\">\nP(D) = P\\{ x_1 \\} P\\{ \\circ|x_1 \\} \\times P\\{ x_2 \\}P\\{ \\times|x_2 \\} \\times \\dots \\times P\\{ x_n \\} P\\{ \\times|x_n \\}\n\\tag{7}</script><p>3.把公式（6）的公式带入公式（7)中，可以得到公式（8）。</p>\n<script type=\"math/tex; mode=display\">\nP(D) = P\\{ x_1 \\} f(x_1) \\times P\\{ x_2 \\} (1-f(x_2)) \\times \\dots \\times P\\{ x_n \\} (1-f(x_n))\n\\tag{7}</script><p>4.f(x)是理想的函数，而我们实际训练得到的是hypothesis h(x)，所以我们还得想办法用h(x)代替f(x)，但是这样的前提是我们假设函数h(x)对数据集与f(x)产生的可能性很大，即likelihood(似然)，即我们在之前在VC Bound的推论中，知道在数据量足够大的情况下g(x)是会接近于f(x)的，如公式（8）所示</p>\n<script type=\"math/tex; mode=display\">\nP(D) = P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (1-h(x_2)) \\times \\dots \\times P\\{ x_n \\} (1-h(x_n))\n\\tag{8}</script><p>5.那么最大似然我们表示为 likelihood(h)，在代入simoid函数的特性 $1-h(x) = h(-x)$，可以得到公式（9）</p>\n<script type=\"math/tex; mode=display\">\nlikelihood(h) = P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (h(-x_2)) \\times \\dots \\times P\\{ x_n \\} (h(-x_n))\n\\tag{9}</script><p>6.那么最大的likelihood(h)如公式（10）所示，在计算最大的likelihood(h)时，所有$P(x_i)$的对大小没有影响，因为所有的假设函数都会乘以同样的$P(x_i)$，所以在表示的时候可以只考虑h(x)。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\max \\limits_h \\quad likelihood(logistic \\quad h)\n&\\propto P\\{ x_1 \\} h(x_1) \\times P\\{ x_2 \\} (h(-x_2)) \\times \\dots \\times P\\{ x_n \\} (h(-x_n)) \\\\\n&\\propto  \\prod \\limits_{i=1}^n h(y_ix_i) \\\\\n\\max \\limits_w \\quad likelihood(w) &\\propto  \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n\\max \\limits_w \\quad likelihood(w) &\\propto\\max \\quad ln \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n\\end{align}\n\\tag{10}</script><p>7.连乘公式不容易求解最大问题，因此求其对数，此处以自然对数e为底，并代入sigmoid 方程，令$err(w,y_i,x_i) = ln(1+exp(-yw^Tx))$，如公式（11）所示，误差方程 $E_{in}$ 如公式（12)所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\max \\limits_w \\quad likelihood(w)\n&\\propto \\max \\quad ln \\prod \\limits_{i=1}^n h(y_iw^Tx_i) \\\\\n&\\propto \\min\\limits_w \\quad \\frac{1}{N} \\sum\\limits_{i=1}^{n} -ln \\theta (y_iw^Tx_i) \\\\\n&\\propto \\min\\limits_w \\quad \\underbrace{ \\frac{1}{N} \\sum\\limits_{i=1}^{n} err(w,y_i,x_i) }_{E_{in}(w)}  \\quad \\quad\n\\end{align}\n\\tag{11}</script><script type=\"math/tex; mode=display\">\nE_{in}(w) = \\frac{1}{N} \\sum\\limits_{i=1}^{n} ln(1+exp(-yw^Tx))\n\\tag{12}</script><h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"4-Gradient-of-Logistic-Regression-Error\"><a href=\"#4-Gradient-of-Logistic-Regression-Error\" class=\"headerlink\" title=\"4. Gradient of Logistic Regression Error\"></a>4. Gradient of Logistic Regression Error</h2><p>上一节中，推导出logistic回归的 ，下一步的工作是寻找使得最小的权值向量w。<br>对公式（12）进行求导，可以得到公式（13），推导过程参考老师用的方法：用圈圈代替exp里面的数，用正方形代替ln里面的表达式，这样可以使得推导过程看起来更加明白</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\nE_{in}(w) &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} ln \\underbrace{(1+exp \\underbrace{(-yw^Tx)}_\\bigcirc)}_\\Box \\\\\n\n\n\\frac{\\partial E_{in}(w)}{\\partial w_i} &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left(\\frac{\\partial ln(\\Box)}{\\partial \\Box}\\right) \\left( \\frac{\\partial (1+exp(\\bigcirc))}{\\partial \\bigcirc} \\right) \\left( \\frac{\\partial - y_iw^Tx_i}{\\partial w_i} \\right) \\\\\n                                        &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\frac{1}{\\Box} \\right) \\left( exp(\\bigcirc) \\right) \\left( -y_i x_i \\right) \\\\\n\n                                          &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( exp(\\bigcirc) \\right) \\left(-y_i x_i \\right) \\quad \\because(\\frac{1}{\\Box}) \\approx 1\\\\\n                                        &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\theta(\\bigcirc) \\right) \\left( -y_i x_i \\right)  \\\\\n                                        &= \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\left( \\theta(-y_iw^Tx_n) \\right) \\left( -y_i x_i \\right)  \\\\\n\\end{align}\n\\tag{13}</script><p> 从公式（13）可以看出，该函数是一个 $\\theta$ 函数作为权值，关于 $(-y_nx_n)$ 的加权求和函数。如果函数的所有权值为零，，可以看出 $y_i$与所有的对应的 $w^Tx_n$ 的同号，即线性可分。</p>\n<p>但是，求该问题的解不能使用类似求解线性回归时使用的闭式解的求解方式，此最小值又该如何计算？我们可以借鉴之前PLA的方法进行迭代求解，如公式（14）</p>\n<script type=\"math/tex; mode=display\">\nw_{t+1} = w_t + \\left[\\left[ sign(w_t^Tx_i) \\neq y_i \\right]\\right] y_ix_i\n\\tag{14}</script><p>从公式（14）可以看出，当 $sign(w_t^Tx_i)=y_n$ 的时候，向量不改变，当 $sign(w_t^Tx_i) \\neq y_n$ 的时候 要加上 $y_ix_i$，然后我们把公式（14)做一定的调整得到公式（15），其中多乘以一个1作为更新的步长，用 $\\eta$表示，PLA中更新的部分用 $\\nu$ 来代表，表示更新的方向。该算法被称为迭代优化方法（iterative optimization approach）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nw_{t+1} &= w_t + \\underbrace{1}_\\eta \\cdot\n                 \\underbrace{\\left[\\left[ sign(w_t^Tx_i) \\neq y_i \\right]\\right] y_ix_i}_\\nu \\\\\n        &= w_t + \\eta \\cdot \\nu\n\\end{align}\n\\tag{15}</script><h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"5-Use-Gradient-Descent-to-Minimize-the-Error-of-Logistic-Regression\"><a href=\"#5-Use-Gradient-Descent-to-Minimize-the-Error-of-Logistic-Regression\" class=\"headerlink\" title=\"5. Use Gradient Descent to Minimize the Error of Logistic Regression\"></a>5. Use Gradient Descent to Minimize the Error of Logistic Regression</h2><p>上面我们根据PLA的方法求得针对Logistic回归问题的误差方程，现在我们就需要找到最佳的参数 $\\eta$ 和 $\\nu$。首先误差方程的曲线图如图三所示。$E-{in}$ 是关于权值向量 $w$ 的示意图为一个平滑且可微的凸函数，其中图像谷底的点对应着最佳 $w$。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/efb59327c4bf9ddccb57959dcc04f7801267f667/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-3%20Iterative%20Optimization.png\" alt=\"Iterative Optimization\"></p>\n<center> 图三 Iterative Optimization <sup>[3]</sup></center>\n\n<p>为了分工明确，设 $\\nu$ 作为单位向量仅代表方向， $\\eta$ 代表步长表示每次更新改变的大小。在 $\\eta$ 固定的情况下，$\\nu$ 按照最陡峭的方向更改。即在 $\\eta$ 固定 $|\\nu| = 1$ 的情况下，有最快的速度找出使得 $E_{in}$ 最小的 $w$，得到公式（16）</p>\n<script type=\"math/tex; mode=display\">\n\\min\\limits_{|\\nu|=1} E_{in} \\underbrace{(w_i + \\eta \\nu)}_{w_{t+1}}\n\\tag{16}</script><p>但是公式（16）依然很难求得最小的 $w$，当 $\\eta$ 很小时，我们通过泰勒展开公式（17）可以得到公式（18），其中 $w_t$ 对应 $x_0$</p>\n<p>Tylor Expansion</p>\n<script type=\"math/tex; mode=display\">\nf(x) = f(x_0) + \\frac{f'(x_0)}{1!}(x-x_0) + \\frac{f^{(2)}(x_0)}{2!}(x-x_0)^2 + \\dots + \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + R_n(x)\n\\tag{17}</script><script type=\"math/tex; mode=display\">\n\\begin{align}\n\\min\\limits_{|\\nu|=1} E_{in} (w_i + \\eta \\nu ^ T) &\\approx   E_{in}(w_t) + \\left(\\left( w_t + \\eta\\nu^T  \\right) - w_t \\right) \\frac{\\nabla E_{in}(w_t)}{1!} \\\\\n\n                                                  &= E_in(w_t) + \\eta\\nu^T \\nabla E_{in}(w_t)\n\n\\end{align}\n\\tag{18}</script><p>接着我们继续分析公式（18），其中 $E_in{w_t}$ 我们是知道的， $\\eta$ 是给定的步长， $\\nabla E_{in}(w_t)$ 也是知道的，所以求解公式（18）的最小值问题，可以转换成求解 $\\nu^T nabla E_{in}(w_t)$ 的最小值，即公式（19）</p>\n<script type=\"math/tex; mode=display\">\n\\min\\limits_{|\\nu|=1} \\nu^T \\nabla E_{in}(w_t)\n\\tag{19}</script><p>两个向量最小的情况为其方向相反，即内积为负，得到公式（20）,这种情况下 $\\nu$ 是一个单位向量</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\min\\limits_{|\\nu|=1} \\nu^T \\nabla E_{in}(w_t) &= -1 \\\\\n                                           \\nu &= - \\frac{\\nabla E_{in}(w_t)}{||\\nabla E_{in}(w_t)||}\n\\end{align}\n\\tag{20}</script><p>所以把公式（20）带入公式（15），可以得到公式（21）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nw_{t+1} &= w_t - \\eta \\cdot \\frac{\\nabla E_{in}(w_t)}{||\\nabla E_{in}(w_t)||}\n\\end{align}\n\\tag{21}</script><p>从公式（21）可以看出，每次更新权值，w都是减少一点（具体多少要看我们谁的哪个的步长，已经误差大小），按照此种方式更新可以找到使得最小的w。此种方式称作梯度下降（gradient descent）。</p>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"6-Choose-Step-Length-for-Gradient-Descent\"><a href=\"#6-Choose-Step-Length-for-Gradient-Descent\" class=\"headerlink\" title=\"6. Choose Step Length for Gradient Descent\"></a>6. Choose Step Length for Gradient Descent</h2><p>由上面的公式（21）可以看出，w受 步长大小 $\\eta$ 和 误差大小的影响。在一定的 $\\eta$ 下，越接近谷底，纠正的也越来越小；但是如果选择的 $\\eta$ 太大，一个新就更新到对面的山峰上面去了（可能会导致纠正后误差更大），或者  $\\eta$ 太小，更新好久还没有更新到需要的准确度。所以选择适当的 $\\eta$ 很重要。如图四所示。<br><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/09288541bf7cddb805bd091f25cf104e438fc179/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter8-4%20Choice%20of%20eta.png\" alt=\"Choice of Step Length\"></p>\n<center> 图四 Choice of Step Length <sup>[3]</sup></center>\n\n<p>因为 $\\eta$ 与 梯度大小 ${||\\nabla E_{in}(w_t)||}$ 正比，所以我们可以得到公式（22）</p>\n<script type=\"math/tex; mode=display\">\n\\eta_{new} = \\frac{ \\eta_{old}}{||\\nabla E_{in}(w_t)||}\n\\tag{22}</script><p>结合公式（21）（22），我们调整的公式（21）为（23）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nw_{t+1} &= w_t - \\eta_{new} \\cdot {\\nabla E_{in}(w_t)}\n\\end{align}\n\\tag{21}</script><p>此时的 $\\eta$ 被称作固定的学习速率（fixed learning rate）。最终得到Logstic Regression 的步骤如下：</p>\n<ol>\n<li>设置权值w为 $w_0$，迭代次数他，并计算梯度 $\\nabla E_{in}(w_t) = \\frac{1}{N} \\sum\\limits_{i=1}^{n} \\theta \\left( -y_iw_t^Tx_i \\right)\\left( -y_ix_i \\right)$</li>\n<li>不断迭代，并更新权值向量w，$w_{t+1} = w_t - \\eta_{new} \\cdot {\\nabla E_{in}(w_t)}$，直到误差函数的导数近似于0，或者迭代一定的次数。</li>\n</ol>\n<p>Gradient Descent 劣势分析：</p>\n<ol>\n<li>不稳定:如果选择的步长太大太小，都会对算法有影响</li>\n<li>局部最优：如果函数不是凸函数的话，可能存在多个局部最优点，那样的话，Gradient Descent只能找到最近的局部最优。</li>\n<li>计算复杂度大，为O(N)，因为导数需要对所有的点进行一次遍历</li>\n</ol>\n<h2 id=\"-5\"><a href=\"#-5\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"7-Stochastic-Gradient-Descent-Another-Approach-of-Gradient-Descent\"><a href=\"#7-Stochastic-Gradient-Descent-Another-Approach-of-Gradient-Descent\" class=\"headerlink\" title=\"7. Stochastic Gradient Descent - Another Approach of Gradient Descent\"></a>7. Stochastic Gradient Descent - Another Approach of Gradient Descent</h2><p>上面讨论了 Gradient Descent,以及计算复杂度大的问题，这一节我们讨论另一种方法，可以将计算复杂度降成O(1)级别。这种方法就是Stochastic Gradient Descent（随机梯度），用符号 $\\nabla_w err(w, w_i, y_i)$ 表示， 核心思想是：通过N个样本中随机抽取一个样本点求出的梯度取代原来的期望梯度。 随机梯度值可以看做真实的梯度值加上一个噪音，使用随机梯度取代真实梯度做梯度下降的算法称作随机梯度下降 stochastic gradient descent(SGD)。在迭代次数足够多的情况下，平均的随机梯度和平均的真实梯度相差不大。真实梯度与随机梯度的关系如公式（22）所示</p>\n<script type=\"math/tex; mode=display\">\n\\nabla_w E_{in}(w_t) = \\varepsilon_{i} \\cdot \\nabla_w err( w, x_i, y_i)\n\\tag{22}</script><p>Logistic Regression 的SGD的迭代如公式(23)所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nw_{t+1} &= w_t - \\eta \\cdot( \\nabla_w err( w, x_i, y_i) )(y_ix_i) \\\\\n        &= w_t + \\eta  \\underbrace{\\theta (-y_i w_t^T x_i)(y_i x_i)}_{- \\nabla_w err( w, x_i, y_i)}(y_ix_i)\n\\end{align}\n\\tag{23}</script><p>对比之前的PLA算法的公式（如公式（24）），容易发现两个公式很类似，因此logistic Regession 的SGD算法又叫”软”PLA，因为权值并没有那么绝对不是1就是0，而是一个在0~1之间的值。如果 $\\eta = 1$ 且 $w_t^T x_i \\approx \\infty$ 始终是一个很大的值，则logistic Regession 的SGD相当于是PLA算法。</p>\n<script type=\"math/tex; mode=display\">\nw_{t+1} = w_t + \\underbrace{1}_\\eta  \\underbrace{ \\left[\\left[ sign(w_tTx_i) \\neq y_i\\right]\\right]}_{\\nu}(y_ix_i)\n\\tag{24}</script><p>SGD算法关键是找出两个最佳的参数: 迭代次数 $t$ 和学习步长 $\\eta$。</p>\n<ol>\n<li>对于迭代次数 $t$ 只能假设足够步数后是已经做到足够好，即通常设置一个大的数值即可；</li>\n<li>学习步长 $\\eta$通常也很难选定(老师推荐：0.1126)。</li>\n</ol>\n<p>SGD算法的优缺点：</p>\n<ol>\n<li>优点：计算简单快速，适用于大数据或者流式数据；</li>\n<li>缺点：不稳定，需要一定的调试时间。</li>\n</ol>\n<h2 id=\"-6\"><a href=\"#-6\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先引入了 Logistic Regression</li>\n<li>然后对比了 Logistic Regression 和 Linear Regression, Linear Classification。</li>\n<li>接着分析Logistic Regression 的误差方程，梯度方程，并用Gradient Descent 来最小化误差，并分析如何选择步长。</li>\n</ol>\n<h2 id=\"-7\"><a href=\"#-7\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\10\\10 - 1 - Logistic Regression Problem (14-33)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\10\\10 - 2 - Logistic Regression Error (15-58)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\10\\10 - 4 - Gradient Descent (19-18)</p>\n<p><br></p>\n<h2 id=\"-8\"><a href=\"#-8\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"10.How can Machine Learn? - Nonlinear Transformation","date":"2017-10-15T06:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# How can Machine Learn? - Nonlinear Transformation\n\n> 之前我们讨论的都是 线性模型，这一节我们讨论如何处理非线性模型的问题\n\n## 1. Quadratic Hypotheses\n线性模型可以通过VC Bound 进行约束，从而保证了数据量足够大，并且有算法找到合适的权值w。 但是对于非线性模型，我们如何处理呢？怎么能肯定的说非线性模型的机器学习是可行的呢？\n\n1.首先，非线性模型很显然是线性不可分(non-linear separable)的，如图一所示。可以看到无论怎样都不可能用一条直线分开，图中能刚好用一个圆分割，所以管它叫圈圈可分（Circular Separable），用圆的公式稍做变形可以得到这个圆的公式（1）\n\n$$\nh_{SEP}(x) = sign(-x_1^2 - x_1^2 + 0.6)\n\\tag{$1$}\n$$\n\n\n![Circular Separable](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f960da372873fcd5f4a105768a922e4c46ef95e2/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-1%20Circular%20Separable.png)\n<center> 图一 Circular Separable <sup>[1]</sup></center>\n\n\n2.类似于之前对Linear Model的公式变化，我们对公式（1）稍作调整，可以得到公式类似的结果，如公式（2)所示。 公式（2）的是z 不是 x，这个公式把 $\\{(x_n, y_n)\\}$ 转换成了线性可分的 $\\{(z_n, y_n)\\}$ ，这种转换成为特征转化(feature transform)用符号 $\\Phi$ 表示，经过特征转换后的图如图二所示。\n\n$$\n\\begin{align}\nh_{SEP}(x) &= sign(-x_1^2 - x_1^2 + 0.6) \\\\\n           &= sign( \\underbrace{0.6}_{w_0} \\times \\underbrace{1}_{x_0} + \\underbrace{(-1)}_{w_1} \\times \\underbrace{x_1^2}_{z_1} + \\underbrace{(-1)}_{w_2} \\times \\underbrace{x_2^2}_{z_2}) \\\\\n           &= sign(w^T z)\n\n\\end{align}\n\\tag{$2$}\n$$\n\n\n![Feature Transform](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/60a2e80f50a6f2be747b5585e544b06f76b598a5/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-2%20Feature%20Transform.png)\n<center> 图二 Circular Separable <sup>[1]</sup></center>\n\n\n3.那么如果能在z空间线性可分，反过来能不能在x空间圆形可分呢？\n答案是能的，下面进行证明。首先我们通过带入不同的值，得到不同情况下的空间图，如表格1所示\n\n表格1\n\nw              | 假设函数h(x)              |     形状                |\n:-------------:|:------------------------:|:-----------------------:|\n(0.6, -1, -1)  | sign(0.6-x_1^2-1x_2^2)   |   圆形（circle）         |\n(0.6, -1, -2)  | sign(0.6-x_1^2-2x_2^2)   |   椭圆形（ellipse）      |\n(0.6, -1, +2)  | sign(0.6-x_1^2+2x_2^2)   |   抛物线（hyperbola）    |\n(0.6, +1, +2)  | sign(0.6+x_1^2+2x_2^2)   |   constant:全为+1       |\n\n\n然后我们可以初步看出，可以表示多种图形，但是这并不是全部图形，比如说圆形的话是经过圆形的圆。所以我们用公式（3）来表示任意的二次曲面图形。\n$$\n\\Phi_2(x) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_2^2)\n\\tag{$3$}\n$$\n\n\n可以发现无论怎样的情况，z空间都能对应到x空间去，也就是说通过算法在z空间里找到合适的权值w，就可以对应在x空间得到我们需要的图形。所以这种方法是可行的。\n\n\n<br><br>\n----------------------------------\n\n## 2. Nonlinear Transform\n上一部分我们定义了什么了二次hypothesis: z空间，那么这部分将介绍如何设计一个好的二次hypothesis来达到良好的分类效果。那么目标就是在z空间中设计一个最佳的分类线。\n\n\n整个过程就是通过特征转换的映射关系，把X空间的问题换到Z空间去做线性分类，具体步骤如下，也可以参考图三。\n\n1. 通过特征转换函数 $\\Phi$，将在X空间中不可分的数据集 $\\{(x_n, y_n)\\}$ 转换成在Z空间中可分的数据集 $\\{(z_n, y_n)\\}$ ；\n2. 使用线性分类算法通过数据集 $\\{(z_n, y_n)\\}$ 获得寻找最优权值向量 $w$；\n3. 回到X空间得到需要的返回假设函数 $g(x) = sign(w^T \\Phi(x_n))$.\n\n\n![The Nonlinear Transform Steps](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/7262c716aad5cb27418d5529cbca571437cc1d8a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-3%20The%20Nonlinear%20Transform%20Steps.png)\n<center> 图三 The Nonlinear Transform Steps <sup>[1]</sup></center>\n\n\n其实，我们以前处理机器学习问题的时候，已经做过类似的特征变换了。比如数字识别问题，我们从原始的像素值特征转换为一些实际的concrete特征，比如密度、对称性等等，这也用到了feature transform的思想。如图四所示\n\n![The Nonlinear Transform Example](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a60f915427ebe7a48375f1277bcb9787e7f4e9f6/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-4%20The%20Nonlinear%20Transform%20Examplepng.png)\n<center> 图四 The Nonlinear Transform Example <sup>[1]</sup></center>\n\n\n这种非线性模型算法结合了非线性转换和线性算法，因此包含两个重要的特征：转换函数和线性模型。这种求解非线性分类的思路不仅可以解决二次分类的问题，也可以用在三次感知器、三次回归，甚至多项式回归的问题上。\n\n\n\n\n<br><br>\n----------------------------------\n\n## 3. Price of Nonlinear Transform\n这一节我们要通过评估使用非线性转换说需要的代价来评估这种方法是否值得使用。下面将从2个问题去讨论：1.空间复杂度 2.能否保证机器能学习（模型的泛化能力会变差）\n\n1.首先，若x空间含有d个类别，即d个特征，特征维度是d维的，那么二次多项式个数，即z空间特征维度是如公式（4）所示\n\n$$\nd^˘ = 1 + C_d^0 + C_d^1 + d = \\frac{d+(d+3)}{2} + 1 = C_{2+d}^2\n\\tag{$4$}\n$$\n\n比如说d为2的时候，那么二次多项式为 $(1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_2^2)$ 共6个\n进一步推导到更高维度Q的多项式，那么z空间的特征维度如公式（5）所示\n\n$$\nd^˘ = C_{Q+d}^Q = C_{Q+d}^d = O(Q^d)\n\\tag{$5$}\n$$\n\n由上式可以看出，计算z域特征维度个数的时间复杂度是Q的d次方，随着Q和d的增大，计算量会变得很大。同时，空间复杂度也大。也就是说，这种特征变换的一个代价是计算的时间、空间复杂度都比较大。如图五所示\n\n![Model Complexity Price](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/1b76f6dba7e21f6ed3b2000ed8a8927b1116b820/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-5%20Model%20Complexity%20Price.png)\n<center> 图五 Model Complexity Price <sup>[2]</sup></center>\n\n\n2.另一方面，关于泛化能力的问题，因为z域中特征个数随着Q和d增加变得很大，同时权重w也会增大，即自由度增加，VC Dimension增大。根据之前课程的讨论：VC Dimension过大，模型的泛化能力会比较差。\n下面举例说明。首先分类结果如图六所示\n\n![Generalization Issue](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/2893a8362c568f176df9527a0f6b1e518e6fea3c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-6%20Generalization%20Issue.png)\n<center> 图六 Generalization Issue <sup>[2]</sup></center>\n\n上图中，左边是用直线进行线性分类，存在分类错误的点；右边是用四次曲线进行非线性分类，所有点都分类正确。\n\n1. 从分类结果来看：单从平面上这些训练数据来看，右边的图（四次曲线）的分类效果更好\n2. 但是从泛化能力来看的话：四次曲线模型很容易带来过拟合（下一节会讨论）的问题，虽然它的 $E_{in}$ 比较小，从泛化能力上来说，还是左边的分类器更好一些。也就是说VC Dimension过大会带来过拟合问题，$d^˘+1$ 不能太大了。\n\n\n\n\n> 那么如何选择合适的Q，来保证不会出现过拟合问题，确保模型的泛化能力足够强呢？一般情况下，为了尽量减少特征自由度，我们会根据训练样本的分布情况，人为地减少、省略一些项。但是，这种人为地删减特征又将会带来一些“自我分析”代价，虽然对训练样本分类效果好，但是对训练样本外的样本，不一定效果好。所以，一般情况下，还是要保存所有的多项式特征，避免对训练样本的人为选择。这种人为的判断好坏已经是人类的大脑处理过后的结果，在机器学习中应避免。\n\n\n\n\n<br><br>\n----------------------------------\n\n## 4. Structured Hypothesis Sets\n这一节我们先通过举例，最终总结出从x空间到y空间的多项式变化\n\n1.首先如果d为1维的话，如公式（6)所示，多项式中只有常数项\n\n$$\n\\Phi_0(x) = (1)\n\\tag{$6$}\n$$\n\n2.如果d为2维的时候，如公式（7）所示，多项式中包含了1维的多项式\n\n$$\n\\Phi_1(x) = ( \\Phi_0(x), x_1, x_2, \\dots, x_d)\n\\tag{$7$}\n$$\n\n3.如果d为3维的时候，如公式（8）所示，多项式中包含了2维的多项式\n\n$$\n\\Phi_2(x) = ( \\Phi_1(x), x_1^2, x1x_2, \\dots, x_d^2)\n\\tag{$8$}\n$$\n\n\n3.以此类推，如果d为Q维的时候，如公式（9）所示，多项式中包含了(Q-1)维的多项式\n\n$$\n\\Phi_Q(x) = ( \\Phi_{Q-1}(x), x_1^{Q}, x1^{Q-1} x_2, \\dots, x_d^Q)\n\\tag{$9$}\n$$\n\n并且可以发现，不能维度的Hypotheses存在以下关系，如公式（10）所示\n\n$$\nH_{\\Phi_0}  \\subset H_{\\Phi_1}  \\subset H_{\\Phi_2}  \\subset \\dots  \\subset H_{\\Phi_Q}\n\\tag{$9$}\n$$\n\n上述过程如图七所示，另外我们把这种结构叫做Structured Hypothesis Sets，如图八所示\n\n![Polynomial Transform Revisited](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b837983ab4b54c354be992a602f91cb714b9e885/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-7%20Polynomial%20Transform%20Revisited.png)\n<center> 图七 Polynomial Transform Revisited <sup>[3]</sup></center>\n\n![Structured Hypothesis Sets](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/767617e08ac2b435e9abc3efe9e1dc5495194e5f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-8%20Structured%20Hypothesis%20Set.png)\n<center> 图八 Structured Hypothesis Sets <sup>[3]</sup></center>\n\n从图八可以看出，随着变换多项式的阶数d增大，虽然 $E_{in}$ 逐渐减小，但是model complexity会逐渐增大，造成 $E_{out}$ 很大，所以阶数不能太高。所以，如果选择的阶数d很大，确实能使 $E_{in}$ 接近于0，但是泛化能力通常很差，我们把这种情况叫做tempting sin。所以，一般最合适的做法是先从低阶开始，如先选择一阶hypothesis，看看 $E_{in}$ 是否足够小，如果 $E_{in}$ 足够小的话就选择一阶，如果 $E_{in}$ 太大不满足需求，那么我们就逐次增加阶数，直到满足要求为止。也就是说，尽量选择低阶的hypothes，这样才能得到较强的泛化能力。\n\n\n<br><br>\n----------------------------------\n\n# Summary\n1. 首先介绍了非线性分类模型，通过非线性的特征变化，将非线性模型映射到一个线性空间进行相信分类。\n2. 接着分析非线性模型的代价：时间和空间复杂度高，而且随着特征纬度的增加，模型的泛化能力变差\n3. 最后我们通过数学分析得到如何能在使用非线性转化的过程中，尽可能的提高模型泛化能力：尽可能使用简单模型（低阶）。\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\12\\12 - 1 - Quadratic Hypothesis (23-47)\n\n[2] 机器学习基石(台湾大学-林轩田)\\12\\12 - 3 - Price of Nonlinear Transform (15-37)\n\n[3] 机器学习基石(台湾大学-林轩田)\\12\\12 - 4 - Structured Hypothesis Sets (09-36)\n\n<br><br>\n----------------------------------","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-15-10.How can Machine Learn - Nonlinear Transformation.md","raw":"---\ntitle: 10.How can Machine Learn? - Nonlinear Transformation\ndate: 2017-10-15 14:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# How can Machine Learn? - Nonlinear Transformation\n\n> 之前我们讨论的都是 线性模型，这一节我们讨论如何处理非线性模型的问题\n\n## 1. Quadratic Hypotheses\n线性模型可以通过VC Bound 进行约束，从而保证了数据量足够大，并且有算法找到合适的权值w。 但是对于非线性模型，我们如何处理呢？怎么能肯定的说非线性模型的机器学习是可行的呢？\n\n1.首先，非线性模型很显然是线性不可分(non-linear separable)的，如图一所示。可以看到无论怎样都不可能用一条直线分开，图中能刚好用一个圆分割，所以管它叫圈圈可分（Circular Separable），用圆的公式稍做变形可以得到这个圆的公式（1）\n\n$$\nh_{SEP}(x) = sign(-x_1^2 - x_1^2 + 0.6)\n\\tag{$1$}\n$$\n\n\n![Circular Separable](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f960da372873fcd5f4a105768a922e4c46ef95e2/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-1%20Circular%20Separable.png)\n<center> 图一 Circular Separable <sup>[1]</sup></center>\n\n\n2.类似于之前对Linear Model的公式变化，我们对公式（1）稍作调整，可以得到公式类似的结果，如公式（2)所示。 公式（2）的是z 不是 x，这个公式把 $\\{(x_n, y_n)\\}$ 转换成了线性可分的 $\\{(z_n, y_n)\\}$ ，这种转换成为特征转化(feature transform)用符号 $\\Phi$ 表示，经过特征转换后的图如图二所示。\n\n$$\n\\begin{align}\nh_{SEP}(x) &= sign(-x_1^2 - x_1^2 + 0.6) \\\\\n           &= sign( \\underbrace{0.6}_{w_0} \\times \\underbrace{1}_{x_0} + \\underbrace{(-1)}_{w_1} \\times \\underbrace{x_1^2}_{z_1} + \\underbrace{(-1)}_{w_2} \\times \\underbrace{x_2^2}_{z_2}) \\\\\n           &= sign(w^T z)\n\n\\end{align}\n\\tag{$2$}\n$$\n\n\n![Feature Transform](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/60a2e80f50a6f2be747b5585e544b06f76b598a5/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-2%20Feature%20Transform.png)\n<center> 图二 Circular Separable <sup>[1]</sup></center>\n\n\n3.那么如果能在z空间线性可分，反过来能不能在x空间圆形可分呢？\n答案是能的，下面进行证明。首先我们通过带入不同的值，得到不同情况下的空间图，如表格1所示\n\n表格1\n\nw              | 假设函数h(x)              |     形状                |\n:-------------:|:------------------------:|:-----------------------:|\n(0.6, -1, -1)  | sign(0.6-x_1^2-1x_2^2)   |   圆形（circle）         |\n(0.6, -1, -2)  | sign(0.6-x_1^2-2x_2^2)   |   椭圆形（ellipse）      |\n(0.6, -1, +2)  | sign(0.6-x_1^2+2x_2^2)   |   抛物线（hyperbola）    |\n(0.6, +1, +2)  | sign(0.6+x_1^2+2x_2^2)   |   constant:全为+1       |\n\n\n然后我们可以初步看出，可以表示多种图形，但是这并不是全部图形，比如说圆形的话是经过圆形的圆。所以我们用公式（3）来表示任意的二次曲面图形。\n$$\n\\Phi_2(x) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_2^2)\n\\tag{$3$}\n$$\n\n\n可以发现无论怎样的情况，z空间都能对应到x空间去，也就是说通过算法在z空间里找到合适的权值w，就可以对应在x空间得到我们需要的图形。所以这种方法是可行的。\n\n\n<br><br>\n----------------------------------\n\n## 2. Nonlinear Transform\n上一部分我们定义了什么了二次hypothesis: z空间，那么这部分将介绍如何设计一个好的二次hypothesis来达到良好的分类效果。那么目标就是在z空间中设计一个最佳的分类线。\n\n\n整个过程就是通过特征转换的映射关系，把X空间的问题换到Z空间去做线性分类，具体步骤如下，也可以参考图三。\n\n1. 通过特征转换函数 $\\Phi$，将在X空间中不可分的数据集 $\\{(x_n, y_n)\\}$ 转换成在Z空间中可分的数据集 $\\{(z_n, y_n)\\}$ ；\n2. 使用线性分类算法通过数据集 $\\{(z_n, y_n)\\}$ 获得寻找最优权值向量 $w$；\n3. 回到X空间得到需要的返回假设函数 $g(x) = sign(w^T \\Phi(x_n))$.\n\n\n![The Nonlinear Transform Steps](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/7262c716aad5cb27418d5529cbca571437cc1d8a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-3%20The%20Nonlinear%20Transform%20Steps.png)\n<center> 图三 The Nonlinear Transform Steps <sup>[1]</sup></center>\n\n\n其实，我们以前处理机器学习问题的时候，已经做过类似的特征变换了。比如数字识别问题，我们从原始的像素值特征转换为一些实际的concrete特征，比如密度、对称性等等，这也用到了feature transform的思想。如图四所示\n\n![The Nonlinear Transform Example](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a60f915427ebe7a48375f1277bcb9787e7f4e9f6/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-4%20The%20Nonlinear%20Transform%20Examplepng.png)\n<center> 图四 The Nonlinear Transform Example <sup>[1]</sup></center>\n\n\n这种非线性模型算法结合了非线性转换和线性算法，因此包含两个重要的特征：转换函数和线性模型。这种求解非线性分类的思路不仅可以解决二次分类的问题，也可以用在三次感知器、三次回归，甚至多项式回归的问题上。\n\n\n\n\n<br><br>\n----------------------------------\n\n## 3. Price of Nonlinear Transform\n这一节我们要通过评估使用非线性转换说需要的代价来评估这种方法是否值得使用。下面将从2个问题去讨论：1.空间复杂度 2.能否保证机器能学习（模型的泛化能力会变差）\n\n1.首先，若x空间含有d个类别，即d个特征，特征维度是d维的，那么二次多项式个数，即z空间特征维度是如公式（4）所示\n\n$$\nd^˘ = 1 + C_d^0 + C_d^1 + d = \\frac{d+(d+3)}{2} + 1 = C_{2+d}^2\n\\tag{$4$}\n$$\n\n比如说d为2的时候，那么二次多项式为 $(1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_2^2)$ 共6个\n进一步推导到更高维度Q的多项式，那么z空间的特征维度如公式（5）所示\n\n$$\nd^˘ = C_{Q+d}^Q = C_{Q+d}^d = O(Q^d)\n\\tag{$5$}\n$$\n\n由上式可以看出，计算z域特征维度个数的时间复杂度是Q的d次方，随着Q和d的增大，计算量会变得很大。同时，空间复杂度也大。也就是说，这种特征变换的一个代价是计算的时间、空间复杂度都比较大。如图五所示\n\n![Model Complexity Price](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/1b76f6dba7e21f6ed3b2000ed8a8927b1116b820/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-5%20Model%20Complexity%20Price.png)\n<center> 图五 Model Complexity Price <sup>[2]</sup></center>\n\n\n2.另一方面，关于泛化能力的问题，因为z域中特征个数随着Q和d增加变得很大，同时权重w也会增大，即自由度增加，VC Dimension增大。根据之前课程的讨论：VC Dimension过大，模型的泛化能力会比较差。\n下面举例说明。首先分类结果如图六所示\n\n![Generalization Issue](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/2893a8362c568f176df9527a0f6b1e518e6fea3c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-6%20Generalization%20Issue.png)\n<center> 图六 Generalization Issue <sup>[2]</sup></center>\n\n上图中，左边是用直线进行线性分类，存在分类错误的点；右边是用四次曲线进行非线性分类，所有点都分类正确。\n\n1. 从分类结果来看：单从平面上这些训练数据来看，右边的图（四次曲线）的分类效果更好\n2. 但是从泛化能力来看的话：四次曲线模型很容易带来过拟合（下一节会讨论）的问题，虽然它的 $E_{in}$ 比较小，从泛化能力上来说，还是左边的分类器更好一些。也就是说VC Dimension过大会带来过拟合问题，$d^˘+1$ 不能太大了。\n\n\n\n\n> 那么如何选择合适的Q，来保证不会出现过拟合问题，确保模型的泛化能力足够强呢？一般情况下，为了尽量减少特征自由度，我们会根据训练样本的分布情况，人为地减少、省略一些项。但是，这种人为地删减特征又将会带来一些“自我分析”代价，虽然对训练样本分类效果好，但是对训练样本外的样本，不一定效果好。所以，一般情况下，还是要保存所有的多项式特征，避免对训练样本的人为选择。这种人为的判断好坏已经是人类的大脑处理过后的结果，在机器学习中应避免。\n\n\n\n\n<br><br>\n----------------------------------\n\n## 4. Structured Hypothesis Sets\n这一节我们先通过举例，最终总结出从x空间到y空间的多项式变化\n\n1.首先如果d为1维的话，如公式（6)所示，多项式中只有常数项\n\n$$\n\\Phi_0(x) = (1)\n\\tag{$6$}\n$$\n\n2.如果d为2维的时候，如公式（7）所示，多项式中包含了1维的多项式\n\n$$\n\\Phi_1(x) = ( \\Phi_0(x), x_1, x_2, \\dots, x_d)\n\\tag{$7$}\n$$\n\n3.如果d为3维的时候，如公式（8）所示，多项式中包含了2维的多项式\n\n$$\n\\Phi_2(x) = ( \\Phi_1(x), x_1^2, x1x_2, \\dots, x_d^2)\n\\tag{$8$}\n$$\n\n\n3.以此类推，如果d为Q维的时候，如公式（9）所示，多项式中包含了(Q-1)维的多项式\n\n$$\n\\Phi_Q(x) = ( \\Phi_{Q-1}(x), x_1^{Q}, x1^{Q-1} x_2, \\dots, x_d^Q)\n\\tag{$9$}\n$$\n\n并且可以发现，不能维度的Hypotheses存在以下关系，如公式（10）所示\n\n$$\nH_{\\Phi_0}  \\subset H_{\\Phi_1}  \\subset H_{\\Phi_2}  \\subset \\dots  \\subset H_{\\Phi_Q}\n\\tag{$9$}\n$$\n\n上述过程如图七所示，另外我们把这种结构叫做Structured Hypothesis Sets，如图八所示\n\n![Polynomial Transform Revisited](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b837983ab4b54c354be992a602f91cb714b9e885/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-7%20Polynomial%20Transform%20Revisited.png)\n<center> 图七 Polynomial Transform Revisited <sup>[3]</sup></center>\n\n![Structured Hypothesis Sets](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/767617e08ac2b435e9abc3efe9e1dc5495194e5f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-8%20Structured%20Hypothesis%20Set.png)\n<center> 图八 Structured Hypothesis Sets <sup>[3]</sup></center>\n\n从图八可以看出，随着变换多项式的阶数d增大，虽然 $E_{in}$ 逐渐减小，但是model complexity会逐渐增大，造成 $E_{out}$ 很大，所以阶数不能太高。所以，如果选择的阶数d很大，确实能使 $E_{in}$ 接近于0，但是泛化能力通常很差，我们把这种情况叫做tempting sin。所以，一般最合适的做法是先从低阶开始，如先选择一阶hypothesis，看看 $E_{in}$ 是否足够小，如果 $E_{in}$ 足够小的话就选择一阶，如果 $E_{in}$ 太大不满足需求，那么我们就逐次增加阶数，直到满足要求为止。也就是说，尽量选择低阶的hypothes，这样才能得到较强的泛化能力。\n\n\n<br><br>\n----------------------------------\n\n# Summary\n1. 首先介绍了非线性分类模型，通过非线性的特征变化，将非线性模型映射到一个线性空间进行相信分类。\n2. 接着分析非线性模型的代价：时间和空间复杂度高，而且随着特征纬度的增加，模型的泛化能力变差\n3. 最后我们通过数学分析得到如何能在使用非线性转化的过程中，尽可能的提高模型泛化能力：尽可能使用简单模型（低阶）。\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\12\\12 - 1 - Quadratic Hypothesis (23-47)\n\n[2] 机器学习基石(台湾大学-林轩田)\\12\\12 - 3 - Price of Nonlinear Transform (15-37)\n\n[3] 机器学习基石(台湾大学-林轩田)\\12\\12 - 4 - Structured Hypothesis Sets (09-36)\n\n<br><br>\n----------------------------------","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-15-10.How can Machine Learn - Nonlinear Transformation","published":1,"updated":"2018-04-14T19:42:06.506Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2ej001trwtj4a0yclsg","content":"<h1 id=\"How-can-Machine-Learn-Nonlinear-Transformation\"><a href=\"#How-can-Machine-Learn-Nonlinear-Transformation\" class=\"headerlink\" title=\"How can Machine Learn? - Nonlinear Transformation\"></a>How can Machine Learn? - Nonlinear Transformation</h1><blockquote>\n<p>之前我们讨论的都是 线性模型，这一节我们讨论如何处理非线性模型的问题</p>\n</blockquote>\n<h2 id=\"1-Quadratic-Hypotheses\"><a href=\"#1-Quadratic-Hypotheses\" class=\"headerlink\" title=\"1. Quadratic Hypotheses\"></a>1. Quadratic Hypotheses</h2><p>线性模型可以通过VC Bound 进行约束，从而保证了数据量足够大，并且有算法找到合适的权值w。 但是对于非线性模型，我们如何处理呢？怎么能肯定的说非线性模型的机器学习是可行的呢？</p>\n<p>1.首先，非线性模型很显然是线性不可分(non-linear separable)的，如图一所示。可以看到无论怎样都不可能用一条直线分开，图中能刚好用一个圆分割，所以管它叫圈圈可分（Circular Separable），用圆的公式稍做变形可以得到这个圆的公式（1）</p>\n<script type=\"math/tex; mode=display\">\nh_{SEP}(x) = sign(-x_1^2 - x_1^2 + 0.6)\n\\tag{$1$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f960da372873fcd5f4a105768a922e4c46ef95e2/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-1%20Circular%20Separable.png\" alt=\"Circular Separable\"></p>\n<center> 图一 Circular Separable <sup>[1]</sup></center>\n\n\n<p>2.类似于之前对Linear Model的公式变化，我们对公式（1）稍作调整，可以得到公式类似的结果，如公式（2)所示。 公式（2）的是z 不是 x，这个公式把 $\\{(x_n, y_n)\\}$ 转换成了线性可分的 $\\{(z_n, y_n)\\}$ ，这种转换成为特征转化(feature transform)用符号 $\\Phi$ 表示，经过特征转换后的图如图二所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh_{SEP}(x) &= sign(-x_1^2 - x_1^2 + 0.6) \\\\\n           &= sign( \\underbrace{0.6}_{w_0} \\times \\underbrace{1}_{x_0} + \\underbrace{(-1)}_{w_1} \\times \\underbrace{x_1^2}_{z_1} + \\underbrace{(-1)}_{w_2} \\times \\underbrace{x_2^2}_{z_2}) \\\\\n           &= sign(w^T z)\n\n\\end{align}\n\\tag{$2$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/60a2e80f50a6f2be747b5585e544b06f76b598a5/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-2%20Feature%20Transform.png\" alt=\"Feature Transform\"></p>\n<center> 图二 Circular Separable <sup>[1]</sup></center>\n\n\n<p>3.那么如果能在z空间线性可分，反过来能不能在x空间圆形可分呢？<br>答案是能的，下面进行证明。首先我们通过带入不同的值，得到不同情况下的空间图，如表格1所示</p>\n<p>表格1</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">w</th>\n<th style=\"text-align:center\">假设函数h(x)</th>\n<th style=\"text-align:center\">形状</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">(0.6, -1, -1)</td>\n<td style=\"text-align:center\">sign(0.6-x_1^2-1x_2^2)</td>\n<td style=\"text-align:center\">圆形（circle）</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(0.6, -1, -2)</td>\n<td style=\"text-align:center\">sign(0.6-x_1^2-2x_2^2)</td>\n<td style=\"text-align:center\">椭圆形（ellipse）</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(0.6, -1, +2)</td>\n<td style=\"text-align:center\">sign(0.6-x_1^2+2x_2^2)</td>\n<td style=\"text-align:center\">抛物线（hyperbola）</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(0.6, +1, +2)</td>\n<td style=\"text-align:center\">sign(0.6+x_1^2+2x_2^2)</td>\n<td style=\"text-align:center\">constant:全为+1</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>然后我们可以初步看出，可以表示多种图形，但是这并不是全部图形，比如说圆形的话是经过圆形的圆。所以我们用公式（3）来表示任意的二次曲面图形。</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_2(x) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_2^2)\n\\tag{$3$}</script><p>可以发现无论怎样的情况，z空间都能对应到x空间去，也就是说通过算法在z空间里找到合适的权值w，就可以对应在x空间得到我们需要的图形。所以这种方法是可行的。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Nonlinear-Transform\"><a href=\"#2-Nonlinear-Transform\" class=\"headerlink\" title=\"2. Nonlinear Transform\"></a>2. Nonlinear Transform</h2><p>上一部分我们定义了什么了二次hypothesis: z空间，那么这部分将介绍如何设计一个好的二次hypothesis来达到良好的分类效果。那么目标就是在z空间中设计一个最佳的分类线。</p>\n<p>整个过程就是通过特征转换的映射关系，把X空间的问题换到Z空间去做线性分类，具体步骤如下，也可以参考图三。</p>\n<ol>\n<li>通过特征转换函数 $\\Phi$，将在X空间中不可分的数据集 $\\{(x_n, y_n)\\}$ 转换成在Z空间中可分的数据集 $\\{(z_n, y_n)\\}$ ；</li>\n<li>使用线性分类算法通过数据集 $\\{(z_n, y_n)\\}$ 获得寻找最优权值向量 $w$；</li>\n<li>回到X空间得到需要的返回假设函数 $g(x) = sign(w^T \\Phi(x_n))$.</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/7262c716aad5cb27418d5529cbca571437cc1d8a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-3%20The%20Nonlinear%20Transform%20Steps.png\" alt=\"The Nonlinear Transform Steps\"></p>\n<center> 图三 The Nonlinear Transform Steps <sup>[1]</sup></center>\n\n\n<p>其实，我们以前处理机器学习问题的时候，已经做过类似的特征变换了。比如数字识别问题，我们从原始的像素值特征转换为一些实际的concrete特征，比如密度、对称性等等，这也用到了feature transform的思想。如图四所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a60f915427ebe7a48375f1277bcb9787e7f4e9f6/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-4%20The%20Nonlinear%20Transform%20Examplepng.png\" alt=\"The Nonlinear Transform Example\"></p>\n<center> 图四 The Nonlinear Transform Example <sup>[1]</sup></center>\n\n\n<p>这种非线性模型算法结合了非线性转换和线性算法，因此包含两个重要的特征：转换函数和线性模型。这种求解非线性分类的思路不仅可以解决二次分类的问题，也可以用在三次感知器、三次回归，甚至多项式回归的问题上。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Price-of-Nonlinear-Transform\"><a href=\"#3-Price-of-Nonlinear-Transform\" class=\"headerlink\" title=\"3. Price of Nonlinear Transform\"></a>3. Price of Nonlinear Transform</h2><p>这一节我们要通过评估使用非线性转换说需要的代价来评估这种方法是否值得使用。下面将从2个问题去讨论：1.空间复杂度 2.能否保证机器能学习（模型的泛化能力会变差）</p>\n<p>1.首先，若x空间含有d个类别，即d个特征，特征维度是d维的，那么二次多项式个数，即z空间特征维度是如公式（4）所示</p>\n<script type=\"math/tex; mode=display\">\nd^˘ = 1 + C_d^0 + C_d^1 + d = \\frac{d+(d+3)}{2} + 1 = C_{2+d}^2\n\\tag{$4$}</script><p>比如说d为2的时候，那么二次多项式为 $(1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_2^2)$ 共6个<br>进一步推导到更高维度Q的多项式，那么z空间的特征维度如公式（5）所示</p>\n<script type=\"math/tex; mode=display\">\nd^˘ = C_{Q+d}^Q = C_{Q+d}^d = O(Q^d)\n\\tag{$5$}</script><p>由上式可以看出，计算z域特征维度个数的时间复杂度是Q的d次方，随着Q和d的增大，计算量会变得很大。同时，空间复杂度也大。也就是说，这种特征变换的一个代价是计算的时间、空间复杂度都比较大。如图五所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/1b76f6dba7e21f6ed3b2000ed8a8927b1116b820/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-5%20Model%20Complexity%20Price.png\" alt=\"Model Complexity Price\"></p>\n<center> 图五 Model Complexity Price <sup>[2]</sup></center>\n\n\n<p>2.另一方面，关于泛化能力的问题，因为z域中特征个数随着Q和d增加变得很大，同时权重w也会增大，即自由度增加，VC Dimension增大。根据之前课程的讨论：VC Dimension过大，模型的泛化能力会比较差。<br>下面举例说明。首先分类结果如图六所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/2893a8362c568f176df9527a0f6b1e518e6fea3c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-6%20Generalization%20Issue.png\" alt=\"Generalization Issue\"></p>\n<center> 图六 Generalization Issue <sup>[2]</sup></center>\n\n<p>上图中，左边是用直线进行线性分类，存在分类错误的点；右边是用四次曲线进行非线性分类，所有点都分类正确。</p>\n<ol>\n<li>从分类结果来看：单从平面上这些训练数据来看，右边的图（四次曲线）的分类效果更好</li>\n<li>但是从泛化能力来看的话：四次曲线模型很容易带来过拟合（下一节会讨论）的问题，虽然它的 $E_{in}$ 比较小，从泛化能力上来说，还是左边的分类器更好一些。也就是说VC Dimension过大会带来过拟合问题，$d^˘+1$ 不能太大了。</li>\n</ol>\n<blockquote>\n<p>那么如何选择合适的Q，来保证不会出现过拟合问题，确保模型的泛化能力足够强呢？一般情况下，为了尽量减少特征自由度，我们会根据训练样本的分布情况，人为地减少、省略一些项。但是，这种人为地删减特征又将会带来一些“自我分析”代价，虽然对训练样本分类效果好，但是对训练样本外的样本，不一定效果好。所以，一般情况下，还是要保存所有的多项式特征，避免对训练样本的人为选择。这种人为的判断好坏已经是人类的大脑处理过后的结果，在机器学习中应避免。</p>\n</blockquote>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"4-Structured-Hypothesis-Sets\"><a href=\"#4-Structured-Hypothesis-Sets\" class=\"headerlink\" title=\"4. Structured Hypothesis Sets\"></a>4. Structured Hypothesis Sets</h2><p>这一节我们先通过举例，最终总结出从x空间到y空间的多项式变化</p>\n<p>1.首先如果d为1维的话，如公式（6)所示，多项式中只有常数项</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_0(x) = (1)\n\\tag{$6$}</script><p>2.如果d为2维的时候，如公式（7）所示，多项式中包含了1维的多项式</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_1(x) = ( \\Phi_0(x), x_1, x_2, \\dots, x_d)\n\\tag{$7$}</script><p>3.如果d为3维的时候，如公式（8）所示，多项式中包含了2维的多项式</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_2(x) = ( \\Phi_1(x), x_1^2, x1x_2, \\dots, x_d^2)\n\\tag{$8$}</script><p>3.以此类推，如果d为Q维的时候，如公式（9）所示，多项式中包含了(Q-1)维的多项式</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_Q(x) = ( \\Phi_{Q-1}(x), x_1^{Q}, x1^{Q-1} x_2, \\dots, x_d^Q)\n\\tag{$9$}</script><p>并且可以发现，不能维度的Hypotheses存在以下关系，如公式（10）所示</p>\n<script type=\"math/tex; mode=display\">\nH_{\\Phi_0}  \\subset H_{\\Phi_1}  \\subset H_{\\Phi_2}  \\subset \\dots  \\subset H_{\\Phi_Q}\n\\tag{$9$}</script><p>上述过程如图七所示，另外我们把这种结构叫做Structured Hypothesis Sets，如图八所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b837983ab4b54c354be992a602f91cb714b9e885/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-7%20Polynomial%20Transform%20Revisited.png\" alt=\"Polynomial Transform Revisited\"></p>\n<center> 图七 Polynomial Transform Revisited <sup>[3]</sup></center>\n\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/767617e08ac2b435e9abc3efe9e1dc5495194e5f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-8%20Structured%20Hypothesis%20Set.png\" alt=\"Structured Hypothesis Sets\"></p>\n<center> 图八 Structured Hypothesis Sets <sup>[3]</sup></center>\n\n<p>从图八可以看出，随着变换多项式的阶数d增大，虽然 $E_{in}$ 逐渐减小，但是model complexity会逐渐增大，造成 $E_{out}$ 很大，所以阶数不能太高。所以，如果选择的阶数d很大，确实能使 $E_{in}$ 接近于0，但是泛化能力通常很差，我们把这种情况叫做tempting sin。所以，一般最合适的做法是先从低阶开始，如先选择一阶hypothesis，看看 $E_{in}$ 是否足够小，如果 $E_{in}$ 足够小的话就选择一阶，如果 $E_{in}$ 太大不满足需求，那么我们就逐次增加阶数，直到满足要求为止。也就是说，尽量选择低阶的hypothes，这样才能得到较强的泛化能力。</p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了非线性分类模型，通过非线性的特征变化，将非线性模型映射到一个线性空间进行相信分类。</li>\n<li>接着分析非线性模型的代价：时间和空间复杂度高，而且随着特征纬度的增加，模型的泛化能力变差</li>\n<li>最后我们通过数学分析得到如何能在使用非线性转化的过程中，尽可能的提高模型泛化能力：尽可能使用简单模型（低阶）。</li>\n</ol>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\12\\12 - 1 - Quadratic Hypothesis (23-47)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\12\\12 - 3 - Price of Nonlinear Transform (15-37)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\12\\12 - 4 - Structured Hypothesis Sets (09-36)</p>\n<h2 id=\"-5\"><a href=\"#-5\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Nonlinear-Transformation\"><a href=\"#How-can-Machine-Learn-Nonlinear-Transformation\" class=\"headerlink\" title=\"How can Machine Learn? - Nonlinear Transformation\"></a>How can Machine Learn? - Nonlinear Transformation</h1><blockquote>\n<p>之前我们讨论的都是 线性模型，这一节我们讨论如何处理非线性模型的问题</p>\n</blockquote>\n<h2 id=\"1-Quadratic-Hypotheses\"><a href=\"#1-Quadratic-Hypotheses\" class=\"headerlink\" title=\"1. Quadratic Hypotheses\"></a>1. Quadratic Hypotheses</h2><p>线性模型可以通过VC Bound 进行约束，从而保证了数据量足够大，并且有算法找到合适的权值w。 但是对于非线性模型，我们如何处理呢？怎么能肯定的说非线性模型的机器学习是可行的呢？</p>\n<p>1.首先，非线性模型很显然是线性不可分(non-linear separable)的，如图一所示。可以看到无论怎样都不可能用一条直线分开，图中能刚好用一个圆分割，所以管它叫圈圈可分（Circular Separable），用圆的公式稍做变形可以得到这个圆的公式（1）</p>\n<script type=\"math/tex; mode=display\">\nh_{SEP}(x) = sign(-x_1^2 - x_1^2 + 0.6)\n\\tag{$1$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f960da372873fcd5f4a105768a922e4c46ef95e2/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-1%20Circular%20Separable.png\" alt=\"Circular Separable\"></p>\n<center> 图一 Circular Separable <sup>[1]</sup></center>\n\n\n<p>2.类似于之前对Linear Model的公式变化，我们对公式（1）稍作调整，可以得到公式类似的结果，如公式（2)所示。 公式（2）的是z 不是 x，这个公式把 $\\{(x_n, y_n)\\}$ 转换成了线性可分的 $\\{(z_n, y_n)\\}$ ，这种转换成为特征转化(feature transform)用符号 $\\Phi$ 表示，经过特征转换后的图如图二所示。</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh_{SEP}(x) &= sign(-x_1^2 - x_1^2 + 0.6) \\\\\n           &= sign( \\underbrace{0.6}_{w_0} \\times \\underbrace{1}_{x_0} + \\underbrace{(-1)}_{w_1} \\times \\underbrace{x_1^2}_{z_1} + \\underbrace{(-1)}_{w_2} \\times \\underbrace{x_2^2}_{z_2}) \\\\\n           &= sign(w^T z)\n\n\\end{align}\n\\tag{$2$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/60a2e80f50a6f2be747b5585e544b06f76b598a5/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-2%20Feature%20Transform.png\" alt=\"Feature Transform\"></p>\n<center> 图二 Circular Separable <sup>[1]</sup></center>\n\n\n<p>3.那么如果能在z空间线性可分，反过来能不能在x空间圆形可分呢？<br>答案是能的，下面进行证明。首先我们通过带入不同的值，得到不同情况下的空间图，如表格1所示</p>\n<p>表格1</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">w</th>\n<th style=\"text-align:center\">假设函数h(x)</th>\n<th style=\"text-align:center\">形状</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">(0.6, -1, -1)</td>\n<td style=\"text-align:center\">sign(0.6-x_1^2-1x_2^2)</td>\n<td style=\"text-align:center\">圆形（circle）</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(0.6, -1, -2)</td>\n<td style=\"text-align:center\">sign(0.6-x_1^2-2x_2^2)</td>\n<td style=\"text-align:center\">椭圆形（ellipse）</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(0.6, -1, +2)</td>\n<td style=\"text-align:center\">sign(0.6-x_1^2+2x_2^2)</td>\n<td style=\"text-align:center\">抛物线（hyperbola）</td>\n<td></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">(0.6, +1, +2)</td>\n<td style=\"text-align:center\">sign(0.6+x_1^2+2x_2^2)</td>\n<td style=\"text-align:center\">constant:全为+1</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>然后我们可以初步看出，可以表示多种图形，但是这并不是全部图形，比如说圆形的话是经过圆形的圆。所以我们用公式（3）来表示任意的二次曲面图形。</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_2(x) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_2^2)\n\\tag{$3$}</script><p>可以发现无论怎样的情况，z空间都能对应到x空间去，也就是说通过算法在z空间里找到合适的权值w，就可以对应在x空间得到我们需要的图形。所以这种方法是可行的。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Nonlinear-Transform\"><a href=\"#2-Nonlinear-Transform\" class=\"headerlink\" title=\"2. Nonlinear Transform\"></a>2. Nonlinear Transform</h2><p>上一部分我们定义了什么了二次hypothesis: z空间，那么这部分将介绍如何设计一个好的二次hypothesis来达到良好的分类效果。那么目标就是在z空间中设计一个最佳的分类线。</p>\n<p>整个过程就是通过特征转换的映射关系，把X空间的问题换到Z空间去做线性分类，具体步骤如下，也可以参考图三。</p>\n<ol>\n<li>通过特征转换函数 $\\Phi$，将在X空间中不可分的数据集 $\\{(x_n, y_n)\\}$ 转换成在Z空间中可分的数据集 $\\{(z_n, y_n)\\}$ ；</li>\n<li>使用线性分类算法通过数据集 $\\{(z_n, y_n)\\}$ 获得寻找最优权值向量 $w$；</li>\n<li>回到X空间得到需要的返回假设函数 $g(x) = sign(w^T \\Phi(x_n))$.</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/7262c716aad5cb27418d5529cbca571437cc1d8a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-3%20The%20Nonlinear%20Transform%20Steps.png\" alt=\"The Nonlinear Transform Steps\"></p>\n<center> 图三 The Nonlinear Transform Steps <sup>[1]</sup></center>\n\n\n<p>其实，我们以前处理机器学习问题的时候，已经做过类似的特征变换了。比如数字识别问题，我们从原始的像素值特征转换为一些实际的concrete特征，比如密度、对称性等等，这也用到了feature transform的思想。如图四所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a60f915427ebe7a48375f1277bcb9787e7f4e9f6/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-4%20The%20Nonlinear%20Transform%20Examplepng.png\" alt=\"The Nonlinear Transform Example\"></p>\n<center> 图四 The Nonlinear Transform Example <sup>[1]</sup></center>\n\n\n<p>这种非线性模型算法结合了非线性转换和线性算法，因此包含两个重要的特征：转换函数和线性模型。这种求解非线性分类的思路不仅可以解决二次分类的问题，也可以用在三次感知器、三次回归，甚至多项式回归的问题上。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Price-of-Nonlinear-Transform\"><a href=\"#3-Price-of-Nonlinear-Transform\" class=\"headerlink\" title=\"3. Price of Nonlinear Transform\"></a>3. Price of Nonlinear Transform</h2><p>这一节我们要通过评估使用非线性转换说需要的代价来评估这种方法是否值得使用。下面将从2个问题去讨论：1.空间复杂度 2.能否保证机器能学习（模型的泛化能力会变差）</p>\n<p>1.首先，若x空间含有d个类别，即d个特征，特征维度是d维的，那么二次多项式个数，即z空间特征维度是如公式（4）所示</p>\n<script type=\"math/tex; mode=display\">\nd^˘ = 1 + C_d^0 + C_d^1 + d = \\frac{d+(d+3)}{2} + 1 = C_{2+d}^2\n\\tag{$4$}</script><p>比如说d为2的时候，那么二次多项式为 $(1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_2^2)$ 共6个<br>进一步推导到更高维度Q的多项式，那么z空间的特征维度如公式（5）所示</p>\n<script type=\"math/tex; mode=display\">\nd^˘ = C_{Q+d}^Q = C_{Q+d}^d = O(Q^d)\n\\tag{$5$}</script><p>由上式可以看出，计算z域特征维度个数的时间复杂度是Q的d次方，随着Q和d的增大，计算量会变得很大。同时，空间复杂度也大。也就是说，这种特征变换的一个代价是计算的时间、空间复杂度都比较大。如图五所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/1b76f6dba7e21f6ed3b2000ed8a8927b1116b820/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-5%20Model%20Complexity%20Price.png\" alt=\"Model Complexity Price\"></p>\n<center> 图五 Model Complexity Price <sup>[2]</sup></center>\n\n\n<p>2.另一方面，关于泛化能力的问题，因为z域中特征个数随着Q和d增加变得很大，同时权重w也会增大，即自由度增加，VC Dimension增大。根据之前课程的讨论：VC Dimension过大，模型的泛化能力会比较差。<br>下面举例说明。首先分类结果如图六所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/2893a8362c568f176df9527a0f6b1e518e6fea3c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-6%20Generalization%20Issue.png\" alt=\"Generalization Issue\"></p>\n<center> 图六 Generalization Issue <sup>[2]</sup></center>\n\n<p>上图中，左边是用直线进行线性分类，存在分类错误的点；右边是用四次曲线进行非线性分类，所有点都分类正确。</p>\n<ol>\n<li>从分类结果来看：单从平面上这些训练数据来看，右边的图（四次曲线）的分类效果更好</li>\n<li>但是从泛化能力来看的话：四次曲线模型很容易带来过拟合（下一节会讨论）的问题，虽然它的 $E_{in}$ 比较小，从泛化能力上来说，还是左边的分类器更好一些。也就是说VC Dimension过大会带来过拟合问题，$d^˘+1$ 不能太大了。</li>\n</ol>\n<blockquote>\n<p>那么如何选择合适的Q，来保证不会出现过拟合问题，确保模型的泛化能力足够强呢？一般情况下，为了尽量减少特征自由度，我们会根据训练样本的分布情况，人为地减少、省略一些项。但是，这种人为地删减特征又将会带来一些“自我分析”代价，虽然对训练样本分类效果好，但是对训练样本外的样本，不一定效果好。所以，一般情况下，还是要保存所有的多项式特征，避免对训练样本的人为选择。这种人为的判断好坏已经是人类的大脑处理过后的结果，在机器学习中应避免。</p>\n</blockquote>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"4-Structured-Hypothesis-Sets\"><a href=\"#4-Structured-Hypothesis-Sets\" class=\"headerlink\" title=\"4. Structured Hypothesis Sets\"></a>4. Structured Hypothesis Sets</h2><p>这一节我们先通过举例，最终总结出从x空间到y空间的多项式变化</p>\n<p>1.首先如果d为1维的话，如公式（6)所示，多项式中只有常数项</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_0(x) = (1)\n\\tag{$6$}</script><p>2.如果d为2维的时候，如公式（7）所示，多项式中包含了1维的多项式</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_1(x) = ( \\Phi_0(x), x_1, x_2, \\dots, x_d)\n\\tag{$7$}</script><p>3.如果d为3维的时候，如公式（8）所示，多项式中包含了2维的多项式</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_2(x) = ( \\Phi_1(x), x_1^2, x1x_2, \\dots, x_d^2)\n\\tag{$8$}</script><p>3.以此类推，如果d为Q维的时候，如公式（9）所示，多项式中包含了(Q-1)维的多项式</p>\n<script type=\"math/tex; mode=display\">\n\\Phi_Q(x) = ( \\Phi_{Q-1}(x), x_1^{Q}, x1^{Q-1} x_2, \\dots, x_d^Q)\n\\tag{$9$}</script><p>并且可以发现，不能维度的Hypotheses存在以下关系，如公式（10）所示</p>\n<script type=\"math/tex; mode=display\">\nH_{\\Phi_0}  \\subset H_{\\Phi_1}  \\subset H_{\\Phi_2}  \\subset \\dots  \\subset H_{\\Phi_Q}\n\\tag{$9$}</script><p>上述过程如图七所示，另外我们把这种结构叫做Structured Hypothesis Sets，如图八所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b837983ab4b54c354be992a602f91cb714b9e885/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-7%20Polynomial%20Transform%20Revisited.png\" alt=\"Polynomial Transform Revisited\"></p>\n<center> 图七 Polynomial Transform Revisited <sup>[3]</sup></center>\n\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/767617e08ac2b435e9abc3efe9e1dc5495194e5f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter10-8%20Structured%20Hypothesis%20Set.png\" alt=\"Structured Hypothesis Sets\"></p>\n<center> 图八 Structured Hypothesis Sets <sup>[3]</sup></center>\n\n<p>从图八可以看出，随着变换多项式的阶数d增大，虽然 $E_{in}$ 逐渐减小，但是model complexity会逐渐增大，造成 $E_{out}$ 很大，所以阶数不能太高。所以，如果选择的阶数d很大，确实能使 $E_{in}$ 接近于0，但是泛化能力通常很差，我们把这种情况叫做tempting sin。所以，一般最合适的做法是先从低阶开始，如先选择一阶hypothesis，看看 $E_{in}$ 是否足够小，如果 $E_{in}$ 足够小的话就选择一阶，如果 $E_{in}$ 太大不满足需求，那么我们就逐次增加阶数，直到满足要求为止。也就是说，尽量选择低阶的hypothes，这样才能得到较强的泛化能力。</p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了非线性分类模型，通过非线性的特征变化，将非线性模型映射到一个线性空间进行相信分类。</li>\n<li>接着分析非线性模型的代价：时间和空间复杂度高，而且随着特征纬度的增加，模型的泛化能力变差</li>\n<li>最后我们通过数学分析得到如何能在使用非线性转化的过程中，尽可能的提高模型泛化能力：尽可能使用简单模型（低阶）。</li>\n</ol>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\12\\12 - 1 - Quadratic Hypothesis (23-47)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\12\\12 - 3 - Price of Nonlinear Transform (15-37)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\12\\12 - 4 - Structured Hypothesis Sets (09-36)</p>\n<h2 id=\"-5\"><a href=\"#-5\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"13.How can Machine Learn Better? - Validation","date":"2017-10-18T06:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# How can Machine Learn Better? - Validation\n\n> 这一节我们Validation，需要选择的原因是机器学习的算法有很多种，如何评估哪一种适合当前的使用场景是一个值得商榷的问题。所以通过Validation，我们可以对模型的误差有一定的测试，评估，从而可以确定我们所需要的模型。\n\n## 1. Model Selection Problem\n到目前为止，已经学过了许多算法模型，但一个模型需要很多参数的选择，这是本章讨论的重点。\n\n以二元分类为例，在学过的算法有PLA、pocket、线性回归、logistic回归；算法中需要用到迭代方法，就需要选择迭代步骤T；同理也需要选择学习步长 ；处理非线性问题时，需要用到不同的转换函数，可能是线性的，二次式，10次多项式或者10次勒让德多项式；如果加上正则化项，则可以选择L2正则化项，L1正则化项；有了正则化项，则参数 值也需要选择。\n\n在如此多的选项中设计一个适用于各种情况各个数据集的模型显然是不可能的，因此针对不同的情况，对各个参数的选择是必须的。\n\n我们主要是通过误差 E 来进行判断一个模型是否良好的。我们有 $E_{out}$ 和 $E_{in}$\n1. 显然我们不能通过 $E_{out}$ 来进行评估，因为我们不可能拿得到未来要进行测试用的数据。\n2. 但是我们也不能通过 $E_{in}$ 来进行并评估，因为从前面2节的讨论中，我们知道，模型复杂度越高的模型， $E_{in}$ 往往就越高，但是也造成了模型的泛化能力变弱，使得  $E_{out} \\approx E_{in}$ 的条件不成立，根本谈不上机器学习了。\n\n但是我们能否采用另一个相对宽松的误差来作为评估呢？比如说:有这样一个独立于训练样本的测试集，将M个模型在测试集上进行测试，看一下Etest的大小，则选取Etest最小的模型作为最佳模型。\n> 是可以的，因为我们有Hoeffding Inequity作为理论保证\n\n下面我们将讨论这个宽松的误差。\n1.首先，这个最小的测试集选出来的权值满足公式（1），其中 $m^\\ast$ 表示最佳的权值\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\nm^* = \\argmin\\limits_{1 \\leq m \\leq M} (E_m = E_{test}(A_m(D)))\n\\tag{$1$}\n$$\n\n2.这种测试集验证的方法，根据finite-bin Hoffding不等式，可以得到公式（2），并且可以看出，模型个数M越少，测试集数目越大，那么 $O(\\sqrt{\\frac{logM}{N_{test}}}$ 越小，即 $E_{test}(g_{m^\\ast})$ 越接近于 $E_{out}(g_{m^\\ast})$ 。\n\n$$\nE_{out}(g_{m^\\ast}) \\leq E_{test}(g_{m^\\ast}) + O(\\sqrt{\\frac{logM}{N_{test}}}\n\\tag{$2$}\n$$\n\n\n\n下面比较一下使用 $E_{in}$ 和 $E_{test}$ 作为评估基准的方法：\n1. 使用 $E_{in}$ 作为判断基准，使用的数据集就是训练集D本身；不仅使用D来训练不同的 $g_{m^\\ast}$ ，而且又使用D来选择最好的 $g_{m^\\ast}$ ，那么 $g_{m^\\ast}$ 对未知数据并不一定泛化能力好。课堂中举的例子是：，老师用学生做过的练习题训练学生，然后再用一样的题目来对学生进行考试，这种方法即使学生得到高分，也不能说明他的真的掌握了知识。所以最小化 $E_{in}$ 的方法并不科学。\n2. 第使用 $E_{test}$ 作为判断基准，使用的是独立于训练集D之外的测试集。而后者使用的是独立于D的测试集，相当于用一份题目来训练学生后，再用另外一份全新的题目来测试学生的水平，这样更能反映出学生对知识点的理解，所以最小化 $E_{test}$ 更加理想。但是真正的训练集是拿不到的（要实际投入使用才知道）。所以，寻找一种折中的办法，我们可以使用已有的训练集D来创造一个验证集validation set，即从D中划出一部分 $D_{val}$ 作为验证集。D另外的部分作为训练模型使用， $D_{val}$ 独立开来，用来测试各个模型的好坏，最小化 $E_{val}$ ，从而选择最佳的 $g_{m^\\ast}$ ∗。\n\n>下面，我们针对验证集的选择进行讨论。\n\n\n<br><br>\n----------------------------------\n## 2. Validation\n验证集的选择，我们必须遵循几点：\n1. 选择要随机 - 造成数据不平衡，结果有问题\n2. 数据量分配要合理 - 过多的验证集会造成训练集的减少，过少的验证集验证结果可信度不高\n3. 验证集必须是新的，没有被用于训练 - 那就等于是直接用训练集去做验证了\n\n所以为了满足这3个条件，我们下面需要讨论如何来划分数据集\n\n1.首先我们将原本的数据样本D分为两部分： 训练数据 $D_{train}$ 和 验证数据 $D_{val}$， 如公式（3）所示，如图一所示。\n\n$$\n\\underbrace{D}_{size=N} = \\underbrace{D_{train}}_{size=N-K} \\cup \\underbrace{D_{val}}_{size=K}\n\\tag{$3$}\n$$\n\n![Validation Set](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9bf9752d63dd11988ef0024464fe257c99f4827a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-1%20Validation%20Set.png)\n<center> 图一 Validation Set <sup>[1]</sup></center>\n\n\n2.根据Hoeffding Inequity，我们可以得到公式（4）\n\n$$\nE_{out}(g_{m*}) \\leq E_{out}(g_{m^-}) \\leq E_{test}(g_{m^-}) + O(\\sqrt{\\frac{logM}{K}}\n\\tag{$2$}\n$$\n\n下面我们举个例子来解释这种模型选择的方法的优越性，假设有两个模型：一个是5阶多项式HΦ5，一个是10阶多项式HΦ10。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较，分析结果如图二所示\n\n![Validation in Practice](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/70d9d9faae5aed3273361b2fb4a776cf83c9fb27/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-2%20Validation%20in%20Practice.png)\n<center> 图二 Validation in Practice <sup>[1]</sup></center>\n\n图中，横坐标表示验证集数量K，纵坐标表示 $E_{out}$ 大小。黑色水平线表示没有验证集，完全使用 $E_{in}$ 作为判断基准，那么 $H_{\\Phi 10}$ 更好一些，但是这种方法的 $E_{out}$ 比较大，而且与K无关。黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，其 $E_{out}$ 很小，同样也与K无关，实际中很难得到这条虚线。\n\n红色曲线表示使用验证集，但是最终选取的矩是 $g_{m^\\ast}^-$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先减小再增大，当K大于一定值的时候，甚至会超过黑色水平线。蓝色曲线表示也使用验证集，最终选取的矩是 $g_{m^\\ast}$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。\n\n但是也存在问题，当 K 太大的时候，红色曲线会超过黑色直线，也就是上面提到的，选择了太多的数据作为验证集，从而导致训练的数据大大减少，那么模型的泛化能力太差，不能保证 $E_{in} \\approx E_{out}$\n\n>那么如何选择 K呢？\n\n老师的建议是选择 $K = \\frac{N}{5}$\n\n\n\n\n<br><br>\n----------------------------------\n## 3. Leave-One-Out Cross Validation\n上一节，最后我们提到了K值得重要性，下面我们将讨论如何让这个数据取得更加合理。\n\n第一种方法叫做： Leave-One-Out Cross Validation(留一法交叉验证)，这方法是采用一种极端的情况（K=1），也就是说验证集大小为1，即每次只用一组数据对 $g_m$ 进行验证，重复N次，最后求平均误差。\n\n这种算法的优缺点如下：\n- 优点：使得 $g_m^- \\approx g_m$\n- 缺点：①$E_{val}$ 与 $E_{out}$  可能会相差很大 ②时间和空间复杂度非常大 ③稳定性差（例如对于二分类问题，取值只有0和1两种，预测本身存在不稳定的因素，那么对所有的Eloocv计算平均值可能会带来很大的数值跳动）\n\n误差方程的表达式如公式（3）所示。\n\n$$\nE_{loocv}(H, A) = \\frac{1}{N} \\sum\\limits_{n=1}^{N} \\cdot err(g_n^-(x_n), y_n)\n\\tag{$3$}\n$$\n\n\n该算法的具体流程如下：\n\n1.取n=1的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$\n\n2.接着取n=2的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$\n\n3.以此类推，重复N次，直到n=N的点也进行上述的操作\n\n4.把N次误差加起来求平均\n\n课上举了一个例子说明该方法的效果并且总结了Feature的多少与 $E_{in} E_{out} E_{loocv}$的关系。如图三所示。很明显可以看出，使用Ein发生了过拟合，而Eloocv分类效果更好，泛化能力强。\n\n\n![Leave-One-Out in Practice](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a5de867bdf6c5738844287c3de950431325891b0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-4%20Leave-One-Out%20in%20Practice.png)\n<center> 图三 Leave-One-Out in Practice <sup>[2]</sup></center>\n\n\n但是这种方法由于他的缺点太多，所以实际中往往很少使用。更多的是使用下面要讨论的V-Fold Cross Validation\n\n\n<br><br>\n----------------------------------\n## 4. V-Fold Cross Validation\n上面讨论的Leave-One-Out Cross Validation 的优缺点导致了该算法不好用。所以针对该算法的缺点，人们对其改进为V-Fold Cross Validation: 这种算法是把数据N平均分为V分，每次取一份作为验证集，剩下的V-1份作为测试集，重复V次，算出平均误差。那样的话Leave-One-Out Cross Validation 可以看成是V=N的极端情况\n\n该算法的误差方程如公式（4）所示\n\n$$\nE_{cv}(H, A) = \\frac{1}{V} \\sum\\limits_{V=1}^{V} E_{val}^{(v)}(g_v^-)\n\\tag{$4$}\n$$\n\n一般的Validation使用V-Fold Cross Validation来选择最佳的模型。\n\n但是该算法也有一点问题：就是Validation的数据来源是集中的，所以并不能保证交叉验证的效果好，得到的数学模型一定就很好。只有在样本数据足够多的时候，结果才可信，数学模型泛化能力越强。\n\n\n<br><br>\n----------------------------------\n\n# Summary\n1. 首先介绍了validation验证的意义\n2. 然后介绍了Leave-One-Out算法并分析其优劣：该算法在实际应用不多\n3. 最后根据Leave-One-Out算法的缺点，做改进，提出了V-Fold Cross Validation\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\15\\15 - 1 - Model Selection Problem (16-00)\n\n[2] 机器学习基石(台湾大学-林轩田)\\15\\15 - 3 - Leave-One-Out Cross Validation (16-06)\n\n<br><br>\n----------------------------------\n","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-18-13.How can Machine Learn Better - Validation.md","raw":"---\ntitle: 13.How can Machine Learn Better? - Validation\ndate: 2017-10-18 14:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# How can Machine Learn Better? - Validation\n\n> 这一节我们Validation，需要选择的原因是机器学习的算法有很多种，如何评估哪一种适合当前的使用场景是一个值得商榷的问题。所以通过Validation，我们可以对模型的误差有一定的测试，评估，从而可以确定我们所需要的模型。\n\n## 1. Model Selection Problem\n到目前为止，已经学过了许多算法模型，但一个模型需要很多参数的选择，这是本章讨论的重点。\n\n以二元分类为例，在学过的算法有PLA、pocket、线性回归、logistic回归；算法中需要用到迭代方法，就需要选择迭代步骤T；同理也需要选择学习步长 ；处理非线性问题时，需要用到不同的转换函数，可能是线性的，二次式，10次多项式或者10次勒让德多项式；如果加上正则化项，则可以选择L2正则化项，L1正则化项；有了正则化项，则参数 值也需要选择。\n\n在如此多的选项中设计一个适用于各种情况各个数据集的模型显然是不可能的，因此针对不同的情况，对各个参数的选择是必须的。\n\n我们主要是通过误差 E 来进行判断一个模型是否良好的。我们有 $E_{out}$ 和 $E_{in}$\n1. 显然我们不能通过 $E_{out}$ 来进行评估，因为我们不可能拿得到未来要进行测试用的数据。\n2. 但是我们也不能通过 $E_{in}$ 来进行并评估，因为从前面2节的讨论中，我们知道，模型复杂度越高的模型， $E_{in}$ 往往就越高，但是也造成了模型的泛化能力变弱，使得  $E_{out} \\approx E_{in}$ 的条件不成立，根本谈不上机器学习了。\n\n但是我们能否采用另一个相对宽松的误差来作为评估呢？比如说:有这样一个独立于训练样本的测试集，将M个模型在测试集上进行测试，看一下Etest的大小，则选取Etest最小的模型作为最佳模型。\n> 是可以的，因为我们有Hoeffding Inequity作为理论保证\n\n下面我们将讨论这个宽松的误差。\n1.首先，这个最小的测试集选出来的权值满足公式（1），其中 $m^\\ast$ 表示最佳的权值\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\nm^* = \\argmin\\limits_{1 \\leq m \\leq M} (E_m = E_{test}(A_m(D)))\n\\tag{$1$}\n$$\n\n2.这种测试集验证的方法，根据finite-bin Hoffding不等式，可以得到公式（2），并且可以看出，模型个数M越少，测试集数目越大，那么 $O(\\sqrt{\\frac{logM}{N_{test}}}$ 越小，即 $E_{test}(g_{m^\\ast})$ 越接近于 $E_{out}(g_{m^\\ast})$ 。\n\n$$\nE_{out}(g_{m^\\ast}) \\leq E_{test}(g_{m^\\ast}) + O(\\sqrt{\\frac{logM}{N_{test}}}\n\\tag{$2$}\n$$\n\n\n\n下面比较一下使用 $E_{in}$ 和 $E_{test}$ 作为评估基准的方法：\n1. 使用 $E_{in}$ 作为判断基准，使用的数据集就是训练集D本身；不仅使用D来训练不同的 $g_{m^\\ast}$ ，而且又使用D来选择最好的 $g_{m^\\ast}$ ，那么 $g_{m^\\ast}$ 对未知数据并不一定泛化能力好。课堂中举的例子是：，老师用学生做过的练习题训练学生，然后再用一样的题目来对学生进行考试，这种方法即使学生得到高分，也不能说明他的真的掌握了知识。所以最小化 $E_{in}$ 的方法并不科学。\n2. 第使用 $E_{test}$ 作为判断基准，使用的是独立于训练集D之外的测试集。而后者使用的是独立于D的测试集，相当于用一份题目来训练学生后，再用另外一份全新的题目来测试学生的水平，这样更能反映出学生对知识点的理解，所以最小化 $E_{test}$ 更加理想。但是真正的训练集是拿不到的（要实际投入使用才知道）。所以，寻找一种折中的办法，我们可以使用已有的训练集D来创造一个验证集validation set，即从D中划出一部分 $D_{val}$ 作为验证集。D另外的部分作为训练模型使用， $D_{val}$ 独立开来，用来测试各个模型的好坏，最小化 $E_{val}$ ，从而选择最佳的 $g_{m^\\ast}$ ∗。\n\n>下面，我们针对验证集的选择进行讨论。\n\n\n<br><br>\n----------------------------------\n## 2. Validation\n验证集的选择，我们必须遵循几点：\n1. 选择要随机 - 造成数据不平衡，结果有问题\n2. 数据量分配要合理 - 过多的验证集会造成训练集的减少，过少的验证集验证结果可信度不高\n3. 验证集必须是新的，没有被用于训练 - 那就等于是直接用训练集去做验证了\n\n所以为了满足这3个条件，我们下面需要讨论如何来划分数据集\n\n1.首先我们将原本的数据样本D分为两部分： 训练数据 $D_{train}$ 和 验证数据 $D_{val}$， 如公式（3）所示，如图一所示。\n\n$$\n\\underbrace{D}_{size=N} = \\underbrace{D_{train}}_{size=N-K} \\cup \\underbrace{D_{val}}_{size=K}\n\\tag{$3$}\n$$\n\n![Validation Set](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9bf9752d63dd11988ef0024464fe257c99f4827a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-1%20Validation%20Set.png)\n<center> 图一 Validation Set <sup>[1]</sup></center>\n\n\n2.根据Hoeffding Inequity，我们可以得到公式（4）\n\n$$\nE_{out}(g_{m*}) \\leq E_{out}(g_{m^-}) \\leq E_{test}(g_{m^-}) + O(\\sqrt{\\frac{logM}{K}}\n\\tag{$2$}\n$$\n\n下面我们举个例子来解释这种模型选择的方法的优越性，假设有两个模型：一个是5阶多项式HΦ5，一个是10阶多项式HΦ10。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较，分析结果如图二所示\n\n![Validation in Practice](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/70d9d9faae5aed3273361b2fb4a776cf83c9fb27/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-2%20Validation%20in%20Practice.png)\n<center> 图二 Validation in Practice <sup>[1]</sup></center>\n\n图中，横坐标表示验证集数量K，纵坐标表示 $E_{out}$ 大小。黑色水平线表示没有验证集，完全使用 $E_{in}$ 作为判断基准，那么 $H_{\\Phi 10}$ 更好一些，但是这种方法的 $E_{out}$ 比较大，而且与K无关。黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，其 $E_{out}$ 很小，同样也与K无关，实际中很难得到这条虚线。\n\n红色曲线表示使用验证集，但是最终选取的矩是 $g_{m^\\ast}^-$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先减小再增大，当K大于一定值的时候，甚至会超过黑色水平线。蓝色曲线表示也使用验证集，最终选取的矩是 $g_{m^\\ast}$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。\n\n但是也存在问题，当 K 太大的时候，红色曲线会超过黑色直线，也就是上面提到的，选择了太多的数据作为验证集，从而导致训练的数据大大减少，那么模型的泛化能力太差，不能保证 $E_{in} \\approx E_{out}$\n\n>那么如何选择 K呢？\n\n老师的建议是选择 $K = \\frac{N}{5}$\n\n\n\n\n<br><br>\n----------------------------------\n## 3. Leave-One-Out Cross Validation\n上一节，最后我们提到了K值得重要性，下面我们将讨论如何让这个数据取得更加合理。\n\n第一种方法叫做： Leave-One-Out Cross Validation(留一法交叉验证)，这方法是采用一种极端的情况（K=1），也就是说验证集大小为1，即每次只用一组数据对 $g_m$ 进行验证，重复N次，最后求平均误差。\n\n这种算法的优缺点如下：\n- 优点：使得 $g_m^- \\approx g_m$\n- 缺点：①$E_{val}$ 与 $E_{out}$  可能会相差很大 ②时间和空间复杂度非常大 ③稳定性差（例如对于二分类问题，取值只有0和1两种，预测本身存在不稳定的因素，那么对所有的Eloocv计算平均值可能会带来很大的数值跳动）\n\n误差方程的表达式如公式（3）所示。\n\n$$\nE_{loocv}(H, A) = \\frac{1}{N} \\sum\\limits_{n=1}^{N} \\cdot err(g_n^-(x_n), y_n)\n\\tag{$3$}\n$$\n\n\n该算法的具体流程如下：\n\n1.取n=1的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$\n\n2.接着取n=2的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$\n\n3.以此类推，重复N次，直到n=N的点也进行上述的操作\n\n4.把N次误差加起来求平均\n\n课上举了一个例子说明该方法的效果并且总结了Feature的多少与 $E_{in} E_{out} E_{loocv}$的关系。如图三所示。很明显可以看出，使用Ein发生了过拟合，而Eloocv分类效果更好，泛化能力强。\n\n\n![Leave-One-Out in Practice](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a5de867bdf6c5738844287c3de950431325891b0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-4%20Leave-One-Out%20in%20Practice.png)\n<center> 图三 Leave-One-Out in Practice <sup>[2]</sup></center>\n\n\n但是这种方法由于他的缺点太多，所以实际中往往很少使用。更多的是使用下面要讨论的V-Fold Cross Validation\n\n\n<br><br>\n----------------------------------\n## 4. V-Fold Cross Validation\n上面讨论的Leave-One-Out Cross Validation 的优缺点导致了该算法不好用。所以针对该算法的缺点，人们对其改进为V-Fold Cross Validation: 这种算法是把数据N平均分为V分，每次取一份作为验证集，剩下的V-1份作为测试集，重复V次，算出平均误差。那样的话Leave-One-Out Cross Validation 可以看成是V=N的极端情况\n\n该算法的误差方程如公式（4）所示\n\n$$\nE_{cv}(H, A) = \\frac{1}{V} \\sum\\limits_{V=1}^{V} E_{val}^{(v)}(g_v^-)\n\\tag{$4$}\n$$\n\n一般的Validation使用V-Fold Cross Validation来选择最佳的模型。\n\n但是该算法也有一点问题：就是Validation的数据来源是集中的，所以并不能保证交叉验证的效果好，得到的数学模型一定就很好。只有在样本数据足够多的时候，结果才可信，数学模型泛化能力越强。\n\n\n<br><br>\n----------------------------------\n\n# Summary\n1. 首先介绍了validation验证的意义\n2. 然后介绍了Leave-One-Out算法并分析其优劣：该算法在实际应用不多\n3. 最后根据Leave-One-Out算法的缺点，做改进，提出了V-Fold Cross Validation\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\15\\15 - 1 - Model Selection Problem (16-00)\n\n[2] 机器学习基石(台湾大学-林轩田)\\15\\15 - 3 - Leave-One-Out Cross Validation (16-06)\n\n<br><br>\n----------------------------------\n","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-18-13.How can Machine Learn Better - Validation","published":1,"updated":"2018-04-14T19:42:06.508Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2el001wrwtj2j3d0n3y","content":"<h1 id=\"How-can-Machine-Learn-Better-Validation\"><a href=\"#How-can-Machine-Learn-Better-Validation\" class=\"headerlink\" title=\"How can Machine Learn Better? - Validation\"></a>How can Machine Learn Better? - Validation</h1><blockquote>\n<p>这一节我们Validation，需要选择的原因是机器学习的算法有很多种，如何评估哪一种适合当前的使用场景是一个值得商榷的问题。所以通过Validation，我们可以对模型的误差有一定的测试，评估，从而可以确定我们所需要的模型。</p>\n</blockquote>\n<h2 id=\"1-Model-Selection-Problem\"><a href=\"#1-Model-Selection-Problem\" class=\"headerlink\" title=\"1. Model Selection Problem\"></a>1. Model Selection Problem</h2><p>到目前为止，已经学过了许多算法模型，但一个模型需要很多参数的选择，这是本章讨论的重点。</p>\n<p>以二元分类为例，在学过的算法有PLA、pocket、线性回归、logistic回归；算法中需要用到迭代方法，就需要选择迭代步骤T；同理也需要选择学习步长 ；处理非线性问题时，需要用到不同的转换函数，可能是线性的，二次式，10次多项式或者10次勒让德多项式；如果加上正则化项，则可以选择L2正则化项，L1正则化项；有了正则化项，则参数 值也需要选择。</p>\n<p>在如此多的选项中设计一个适用于各种情况各个数据集的模型显然是不可能的，因此针对不同的情况，对各个参数的选择是必须的。</p>\n<p>我们主要是通过误差 E 来进行判断一个模型是否良好的。我们有 $E_{out}$ 和 $E_{in}$</p>\n<ol>\n<li>显然我们不能通过 $E_{out}$ 来进行评估，因为我们不可能拿得到未来要进行测试用的数据。</li>\n<li>但是我们也不能通过 $E_{in}$ 来进行并评估，因为从前面2节的讨论中，我们知道，模型复杂度越高的模型， $E_{in}$ 往往就越高，但是也造成了模型的泛化能力变弱，使得  $E_{out} \\approx E_{in}$ 的条件不成立，根本谈不上机器学习了。</li>\n</ol>\n<p>但是我们能否采用另一个相对宽松的误差来作为评估呢？比如说:有这样一个独立于训练样本的测试集，将M个模型在测试集上进行测试，看一下Etest的大小，则选取Etest最小的模型作为最佳模型。</p>\n<blockquote>\n<p>是可以的，因为我们有Hoeffding Inequity作为理论保证</p>\n</blockquote>\n<p>下面我们将讨论这个宽松的误差。<br>1.首先，这个最小的测试集选出来的权值满足公式（1），其中 $m^\\ast$ 表示最佳的权值</p>\n<script type=\"math/tex; mode=display\">\n\\DeclareMathOperator*{\\argmin}{argmin}\nm^* = \\argmin\\limits_{1 \\leq m \\leq M} (E_m = E_{test}(A_m(D)))\n\\tag{$1$}</script><p>2.这种测试集验证的方法，根据finite-bin Hoffding不等式，可以得到公式（2），并且可以看出，模型个数M越少，测试集数目越大，那么 $O(\\sqrt{\\frac{logM}{N_{test}}}$ 越小，即 $E_{test}(g_{m^\\ast})$ 越接近于 $E_{out}(g_{m^\\ast})$ 。</p>\n<script type=\"math/tex; mode=display\">\nE_{out}(g_{m^\\ast}) \\leq E_{test}(g_{m^\\ast}) + O(\\sqrt{\\frac{logM}{N_{test}}}\n\\tag{$2$}</script><p>下面比较一下使用 $E_{in}$ 和 $E_{test}$ 作为评估基准的方法：</p>\n<ol>\n<li>使用 $E_{in}$ 作为判断基准，使用的数据集就是训练集D本身；不仅使用D来训练不同的 $g_{m^\\ast}$ ，而且又使用D来选择最好的 $g_{m^\\ast}$ ，那么 $g_{m^\\ast}$ 对未知数据并不一定泛化能力好。课堂中举的例子是：，老师用学生做过的练习题训练学生，然后再用一样的题目来对学生进行考试，这种方法即使学生得到高分，也不能说明他的真的掌握了知识。所以最小化 $E_{in}$ 的方法并不科学。</li>\n<li>第使用 $E_{test}$ 作为判断基准，使用的是独立于训练集D之外的测试集。而后者使用的是独立于D的测试集，相当于用一份题目来训练学生后，再用另外一份全新的题目来测试学生的水平，这样更能反映出学生对知识点的理解，所以最小化 $E_{test}$ 更加理想。但是真正的训练集是拿不到的（要实际投入使用才知道）。所以，寻找一种折中的办法，我们可以使用已有的训练集D来创造一个验证集validation set，即从D中划出一部分 $D_{val}$ 作为验证集。D另外的部分作为训练模型使用， $D_{val}$ 独立开来，用来测试各个模型的好坏，最小化 $E_{val}$ ，从而选择最佳的 $g_{m^\\ast}$ ∗。</li>\n</ol>\n<blockquote>\n<p>下面，我们针对验证集的选择进行讨论。</p>\n</blockquote>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Validation\"><a href=\"#2-Validation\" class=\"headerlink\" title=\"2. Validation\"></a>2. Validation</h2><p>验证集的选择，我们必须遵循几点：</p>\n<ol>\n<li>选择要随机 - 造成数据不平衡，结果有问题</li>\n<li>数据量分配要合理 - 过多的验证集会造成训练集的减少，过少的验证集验证结果可信度不高</li>\n<li>验证集必须是新的，没有被用于训练 - 那就等于是直接用训练集去做验证了</li>\n</ol>\n<p>所以为了满足这3个条件，我们下面需要讨论如何来划分数据集</p>\n<p>1.首先我们将原本的数据样本D分为两部分： 训练数据 $D_{train}$ 和 验证数据 $D_{val}$， 如公式（3）所示，如图一所示。</p>\n<script type=\"math/tex; mode=display\">\n\\underbrace{D}_{size=N} = \\underbrace{D_{train}}_{size=N-K} \\cup \\underbrace{D_{val}}_{size=K}\n\\tag{$3$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9bf9752d63dd11988ef0024464fe257c99f4827a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-1%20Validation%20Set.png\" alt=\"Validation Set\"></p>\n<center> 图一 Validation Set <sup>[1]</sup></center>\n\n\n<p>2.根据Hoeffding Inequity，我们可以得到公式（4）</p>\n<script type=\"math/tex; mode=display\">\nE_{out}(g_{m*}) \\leq E_{out}(g_{m^-}) \\leq E_{test}(g_{m^-}) + O(\\sqrt{\\frac{logM}{K}}\n\\tag{$2$}</script><p>下面我们举个例子来解释这种模型选择的方法的优越性，假设有两个模型：一个是5阶多项式HΦ5，一个是10阶多项式HΦ10。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较，分析结果如图二所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/70d9d9faae5aed3273361b2fb4a776cf83c9fb27/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-2%20Validation%20in%20Practice.png\" alt=\"Validation in Practice\"></p>\n<center> 图二 Validation in Practice <sup>[1]</sup></center>\n\n<p>图中，横坐标表示验证集数量K，纵坐标表示 $E_{out}$ 大小。黑色水平线表示没有验证集，完全使用 $E_{in}$ 作为判断基准，那么 $H_{\\Phi 10}$ 更好一些，但是这种方法的 $E_{out}$ 比较大，而且与K无关。黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，其 $E_{out}$ 很小，同样也与K无关，实际中很难得到这条虚线。</p>\n<p>红色曲线表示使用验证集，但是最终选取的矩是 $g_{m^\\ast}^-$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先减小再增大，当K大于一定值的时候，甚至会超过黑色水平线。蓝色曲线表示也使用验证集，最终选取的矩是 $g_{m^\\ast}$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。</p>\n<p>但是也存在问题，当 K 太大的时候，红色曲线会超过黑色直线，也就是上面提到的，选择了太多的数据作为验证集，从而导致训练的数据大大减少，那么模型的泛化能力太差，不能保证 $E_{in} \\approx E_{out}$</p>\n<blockquote>\n<p>那么如何选择 K呢？</p>\n</blockquote>\n<p>老师的建议是选择 $K = \\frac{N}{5}$</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Leave-One-Out-Cross-Validation\"><a href=\"#3-Leave-One-Out-Cross-Validation\" class=\"headerlink\" title=\"3. Leave-One-Out Cross Validation\"></a>3. Leave-One-Out Cross Validation</h2><p>上一节，最后我们提到了K值得重要性，下面我们将讨论如何让这个数据取得更加合理。</p>\n<p>第一种方法叫做： Leave-One-Out Cross Validation(留一法交叉验证)，这方法是采用一种极端的情况（K=1），也就是说验证集大小为1，即每次只用一组数据对 $g_m$ 进行验证，重复N次，最后求平均误差。</p>\n<p>这种算法的优缺点如下：</p>\n<ul>\n<li>优点：使得 $g_m^- \\approx g_m$</li>\n<li>缺点：①$E_{val}$ 与 $E_{out}$  可能会相差很大 ②时间和空间复杂度非常大 ③稳定性差（例如对于二分类问题，取值只有0和1两种，预测本身存在不稳定的因素，那么对所有的Eloocv计算平均值可能会带来很大的数值跳动）</li>\n</ul>\n<p>误差方程的表达式如公式（3）所示。</p>\n<script type=\"math/tex; mode=display\">\nE_{loocv}(H, A) = \\frac{1}{N} \\sum\\limits_{n=1}^{N} \\cdot err(g_n^-(x_n), y_n)\n\\tag{$3$}</script><p>该算法的具体流程如下：</p>\n<p>1.取n=1的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$</p>\n<p>2.接着取n=2的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$</p>\n<p>3.以此类推，重复N次，直到n=N的点也进行上述的操作</p>\n<p>4.把N次误差加起来求平均</p>\n<p>课上举了一个例子说明该方法的效果并且总结了Feature的多少与 $E_{in} E_{out} E_{loocv}$的关系。如图三所示。很明显可以看出，使用Ein发生了过拟合，而Eloocv分类效果更好，泛化能力强。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a5de867bdf6c5738844287c3de950431325891b0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-4%20Leave-One-Out%20in%20Practice.png\" alt=\"Leave-One-Out in Practice\"></p>\n<center> 图三 Leave-One-Out in Practice <sup>[2]</sup></center>\n\n\n<p>但是这种方法由于他的缺点太多，所以实际中往往很少使用。更多的是使用下面要讨论的V-Fold Cross Validation</p>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"4-V-Fold-Cross-Validation\"><a href=\"#4-V-Fold-Cross-Validation\" class=\"headerlink\" title=\"4. V-Fold Cross Validation\"></a>4. V-Fold Cross Validation</h2><p>上面讨论的Leave-One-Out Cross Validation 的优缺点导致了该算法不好用。所以针对该算法的缺点，人们对其改进为V-Fold Cross Validation: 这种算法是把数据N平均分为V分，每次取一份作为验证集，剩下的V-1份作为测试集，重复V次，算出平均误差。那样的话Leave-One-Out Cross Validation 可以看成是V=N的极端情况</p>\n<p>该算法的误差方程如公式（4）所示</p>\n<script type=\"math/tex; mode=display\">\nE_{cv}(H, A) = \\frac{1}{V} \\sum\\limits_{V=1}^{V} E_{val}^{(v)}(g_v^-)\n\\tag{$4$}</script><p>一般的Validation使用V-Fold Cross Validation来选择最佳的模型。</p>\n<p>但是该算法也有一点问题：就是Validation的数据来源是集中的，所以并不能保证交叉验证的效果好，得到的数学模型一定就很好。只有在样本数据足够多的时候，结果才可信，数学模型泛化能力越强。</p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了validation验证的意义</li>\n<li>然后介绍了Leave-One-Out算法并分析其优劣：该算法在实际应用不多</li>\n<li>最后根据Leave-One-Out算法的缺点，做改进，提出了V-Fold Cross Validation</li>\n</ol>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\15\\15 - 1 - Model Selection Problem (16-00)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\15\\15 - 3 - Leave-One-Out Cross Validation (16-06)</p>\n<h2 id=\"-5\"><a href=\"#-5\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Better-Validation\"><a href=\"#How-can-Machine-Learn-Better-Validation\" class=\"headerlink\" title=\"How can Machine Learn Better? - Validation\"></a>How can Machine Learn Better? - Validation</h1><blockquote>\n<p>这一节我们Validation，需要选择的原因是机器学习的算法有很多种，如何评估哪一种适合当前的使用场景是一个值得商榷的问题。所以通过Validation，我们可以对模型的误差有一定的测试，评估，从而可以确定我们所需要的模型。</p>\n</blockquote>\n<h2 id=\"1-Model-Selection-Problem\"><a href=\"#1-Model-Selection-Problem\" class=\"headerlink\" title=\"1. Model Selection Problem\"></a>1. Model Selection Problem</h2><p>到目前为止，已经学过了许多算法模型，但一个模型需要很多参数的选择，这是本章讨论的重点。</p>\n<p>以二元分类为例，在学过的算法有PLA、pocket、线性回归、logistic回归；算法中需要用到迭代方法，就需要选择迭代步骤T；同理也需要选择学习步长 ；处理非线性问题时，需要用到不同的转换函数，可能是线性的，二次式，10次多项式或者10次勒让德多项式；如果加上正则化项，则可以选择L2正则化项，L1正则化项；有了正则化项，则参数 值也需要选择。</p>\n<p>在如此多的选项中设计一个适用于各种情况各个数据集的模型显然是不可能的，因此针对不同的情况，对各个参数的选择是必须的。</p>\n<p>我们主要是通过误差 E 来进行判断一个模型是否良好的。我们有 $E_{out}$ 和 $E_{in}$</p>\n<ol>\n<li>显然我们不能通过 $E_{out}$ 来进行评估，因为我们不可能拿得到未来要进行测试用的数据。</li>\n<li>但是我们也不能通过 $E_{in}$ 来进行并评估，因为从前面2节的讨论中，我们知道，模型复杂度越高的模型， $E_{in}$ 往往就越高，但是也造成了模型的泛化能力变弱，使得  $E_{out} \\approx E_{in}$ 的条件不成立，根本谈不上机器学习了。</li>\n</ol>\n<p>但是我们能否采用另一个相对宽松的误差来作为评估呢？比如说:有这样一个独立于训练样本的测试集，将M个模型在测试集上进行测试，看一下Etest的大小，则选取Etest最小的模型作为最佳模型。</p>\n<blockquote>\n<p>是可以的，因为我们有Hoeffding Inequity作为理论保证</p>\n</blockquote>\n<p>下面我们将讨论这个宽松的误差。<br>1.首先，这个最小的测试集选出来的权值满足公式（1），其中 $m^\\ast$ 表示最佳的权值</p>\n<script type=\"math/tex; mode=display\">\n\\DeclareMathOperator*{\\argmin}{argmin}\nm^* = \\argmin\\limits_{1 \\leq m \\leq M} (E_m = E_{test}(A_m(D)))\n\\tag{$1$}</script><p>2.这种测试集验证的方法，根据finite-bin Hoffding不等式，可以得到公式（2），并且可以看出，模型个数M越少，测试集数目越大，那么 $O(\\sqrt{\\frac{logM}{N_{test}}}$ 越小，即 $E_{test}(g_{m^\\ast})$ 越接近于 $E_{out}(g_{m^\\ast})$ 。</p>\n<script type=\"math/tex; mode=display\">\nE_{out}(g_{m^\\ast}) \\leq E_{test}(g_{m^\\ast}) + O(\\sqrt{\\frac{logM}{N_{test}}}\n\\tag{$2$}</script><p>下面比较一下使用 $E_{in}$ 和 $E_{test}$ 作为评估基准的方法：</p>\n<ol>\n<li>使用 $E_{in}$ 作为判断基准，使用的数据集就是训练集D本身；不仅使用D来训练不同的 $g_{m^\\ast}$ ，而且又使用D来选择最好的 $g_{m^\\ast}$ ，那么 $g_{m^\\ast}$ 对未知数据并不一定泛化能力好。课堂中举的例子是：，老师用学生做过的练习题训练学生，然后再用一样的题目来对学生进行考试，这种方法即使学生得到高分，也不能说明他的真的掌握了知识。所以最小化 $E_{in}$ 的方法并不科学。</li>\n<li>第使用 $E_{test}$ 作为判断基准，使用的是独立于训练集D之外的测试集。而后者使用的是独立于D的测试集，相当于用一份题目来训练学生后，再用另外一份全新的题目来测试学生的水平，这样更能反映出学生对知识点的理解，所以最小化 $E_{test}$ 更加理想。但是真正的训练集是拿不到的（要实际投入使用才知道）。所以，寻找一种折中的办法，我们可以使用已有的训练集D来创造一个验证集validation set，即从D中划出一部分 $D_{val}$ 作为验证集。D另外的部分作为训练模型使用， $D_{val}$ 独立开来，用来测试各个模型的好坏，最小化 $E_{val}$ ，从而选择最佳的 $g_{m^\\ast}$ ∗。</li>\n</ol>\n<blockquote>\n<p>下面，我们针对验证集的选择进行讨论。</p>\n</blockquote>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Validation\"><a href=\"#2-Validation\" class=\"headerlink\" title=\"2. Validation\"></a>2. Validation</h2><p>验证集的选择，我们必须遵循几点：</p>\n<ol>\n<li>选择要随机 - 造成数据不平衡，结果有问题</li>\n<li>数据量分配要合理 - 过多的验证集会造成训练集的减少，过少的验证集验证结果可信度不高</li>\n<li>验证集必须是新的，没有被用于训练 - 那就等于是直接用训练集去做验证了</li>\n</ol>\n<p>所以为了满足这3个条件，我们下面需要讨论如何来划分数据集</p>\n<p>1.首先我们将原本的数据样本D分为两部分： 训练数据 $D_{train}$ 和 验证数据 $D_{val}$， 如公式（3）所示，如图一所示。</p>\n<script type=\"math/tex; mode=display\">\n\\underbrace{D}_{size=N} = \\underbrace{D_{train}}_{size=N-K} \\cup \\underbrace{D_{val}}_{size=K}\n\\tag{$3$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9bf9752d63dd11988ef0024464fe257c99f4827a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-1%20Validation%20Set.png\" alt=\"Validation Set\"></p>\n<center> 图一 Validation Set <sup>[1]</sup></center>\n\n\n<p>2.根据Hoeffding Inequity，我们可以得到公式（4）</p>\n<script type=\"math/tex; mode=display\">\nE_{out}(g_{m*}) \\leq E_{out}(g_{m^-}) \\leq E_{test}(g_{m^-}) + O(\\sqrt{\\frac{logM}{K}}\n\\tag{$2$}</script><p>下面我们举个例子来解释这种模型选择的方法的优越性，假设有两个模型：一个是5阶多项式HΦ5，一个是10阶多项式HΦ10。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较，分析结果如图二所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/70d9d9faae5aed3273361b2fb4a776cf83c9fb27/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-2%20Validation%20in%20Practice.png\" alt=\"Validation in Practice\"></p>\n<center> 图二 Validation in Practice <sup>[1]</sup></center>\n\n<p>图中，横坐标表示验证集数量K，纵坐标表示 $E_{out}$ 大小。黑色水平线表示没有验证集，完全使用 $E_{in}$ 作为判断基准，那么 $H_{\\Phi 10}$ 更好一些，但是这种方法的 $E_{out}$ 比较大，而且与K无关。黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，其 $E_{out}$ 很小，同样也与K无关，实际中很难得到这条虚线。</p>\n<p>红色曲线表示使用验证集，但是最终选取的矩是 $g_{m^\\ast}^-$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先减小再增大，当K大于一定值的时候，甚至会超过黑色水平线。蓝色曲线表示也使用验证集，最终选取的矩是 $g_{m^\\ast}$ ，其趋势是随着K的增加，它对应的 $E_{out}$ 先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。</p>\n<p>但是也存在问题，当 K 太大的时候，红色曲线会超过黑色直线，也就是上面提到的，选择了太多的数据作为验证集，从而导致训练的数据大大减少，那么模型的泛化能力太差，不能保证 $E_{in} \\approx E_{out}$</p>\n<blockquote>\n<p>那么如何选择 K呢？</p>\n</blockquote>\n<p>老师的建议是选择 $K = \\frac{N}{5}$</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Leave-One-Out-Cross-Validation\"><a href=\"#3-Leave-One-Out-Cross-Validation\" class=\"headerlink\" title=\"3. Leave-One-Out Cross Validation\"></a>3. Leave-One-Out Cross Validation</h2><p>上一节，最后我们提到了K值得重要性，下面我们将讨论如何让这个数据取得更加合理。</p>\n<p>第一种方法叫做： Leave-One-Out Cross Validation(留一法交叉验证)，这方法是采用一种极端的情况（K=1），也就是说验证集大小为1，即每次只用一组数据对 $g_m$ 进行验证，重复N次，最后求平均误差。</p>\n<p>这种算法的优缺点如下：</p>\n<ul>\n<li>优点：使得 $g_m^- \\approx g_m$</li>\n<li>缺点：①$E_{val}$ 与 $E_{out}$  可能会相差很大 ②时间和空间复杂度非常大 ③稳定性差（例如对于二分类问题，取值只有0和1两种，预测本身存在不稳定的因素，那么对所有的Eloocv计算平均值可能会带来很大的数值跳动）</li>\n</ul>\n<p>误差方程的表达式如公式（3）所示。</p>\n<script type=\"math/tex; mode=display\">\nE_{loocv}(H, A) = \\frac{1}{N} \\sum\\limits_{n=1}^{N} \\cdot err(g_n^-(x_n), y_n)\n\\tag{$3$}</script><p>该算法的具体流程如下：</p>\n<p>1.取n=1的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$</p>\n<p>2.接着取n=2的点出来做验证集，其他点保留作为训练集，求出Hypothesis 和对应的 误差 $err(g_n^-(x_n), y_n)$</p>\n<p>3.以此类推，重复N次，直到n=N的点也进行上述的操作</p>\n<p>4.把N次误差加起来求平均</p>\n<p>课上举了一个例子说明该方法的效果并且总结了Feature的多少与 $E_{in} E_{out} E_{loocv}$的关系。如图三所示。很明显可以看出，使用Ein发生了过拟合，而Eloocv分类效果更好，泛化能力强。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/a5de867bdf6c5738844287c3de950431325891b0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter13-4%20Leave-One-Out%20in%20Practice.png\" alt=\"Leave-One-Out in Practice\"></p>\n<center> 图三 Leave-One-Out in Practice <sup>[2]</sup></center>\n\n\n<p>但是这种方法由于他的缺点太多，所以实际中往往很少使用。更多的是使用下面要讨论的V-Fold Cross Validation</p>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"4-V-Fold-Cross-Validation\"><a href=\"#4-V-Fold-Cross-Validation\" class=\"headerlink\" title=\"4. V-Fold Cross Validation\"></a>4. V-Fold Cross Validation</h2><p>上面讨论的Leave-One-Out Cross Validation 的优缺点导致了该算法不好用。所以针对该算法的缺点，人们对其改进为V-Fold Cross Validation: 这种算法是把数据N平均分为V分，每次取一份作为验证集，剩下的V-1份作为测试集，重复V次，算出平均误差。那样的话Leave-One-Out Cross Validation 可以看成是V=N的极端情况</p>\n<p>该算法的误差方程如公式（4）所示</p>\n<script type=\"math/tex; mode=display\">\nE_{cv}(H, A) = \\frac{1}{V} \\sum\\limits_{V=1}^{V} E_{val}^{(v)}(g_v^-)\n\\tag{$4$}</script><p>一般的Validation使用V-Fold Cross Validation来选择最佳的模型。</p>\n<p>但是该算法也有一点问题：就是Validation的数据来源是集中的，所以并不能保证交叉验证的效果好，得到的数学模型一定就很好。只有在样本数据足够多的时候，结果才可信，数学模型泛化能力越强。</p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了validation验证的意义</li>\n<li>然后介绍了Leave-One-Out算法并分析其优劣：该算法在实际应用不多</li>\n<li>最后根据Leave-One-Out算法的缺点，做改进，提出了V-Fold Cross Validation</li>\n</ol>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\15\\15 - 1 - Model Selection Problem (16-00)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\15\\15 - 3 - Leave-One-Out Cross Validation (16-06)</p>\n<h2 id=\"-5\"><a href=\"#-5\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"12.How can Machine Learn Better? - Regularization","date":"2017-10-17T03:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# How can Machine Learn Better? - Regularization\n\n\n## 1. Regularized Hypothesis Set\n正则化的主要思想：将假设函数从高次多项式的数降低到低次，即把复杂的模型变成简单模型。如图一所示的表示高次多项式函数，明显产生了过拟合现象，而左图的表示使用正则化后的低次函数。并且从图中的下方的Hypothesis Set的圈中可以看出，高次的多项式会包含低次的多项式。所以在转换的过程，就是把模型从外圈降至内圈的过程。\n\n> 但是：Regularization 不适用于多个解的模型，因为在降阶的过程中面临着选择最适解\n\n![Regularization Fit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/1ef39e622b59906f5831d3493e000c306845a087/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-1%20Regularization%20Fit.png)\n<center> 图一 Regularization Fit <sup>[1]</sup></center>\n\n\n下面来讨论如何进行降阶：\n1.首先我们在列出10次和2次的多项式，如公式（1）（2）所示。对比可以发现，其实 $H_2$ 可以看成是 $H_{10}$ 的3~10次项的系数为0，如公式（3）所示，其中 $s.t.$ 是subject to的意思，即约束条件\n\n$$\nH_{10} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10}\n\\tag{$1$}\n$$\n\n$$\nH_{2} = w_0 + w_1 x + w_2 x^2\n\\tag{$2$}\n$$\n\n\n$$\nH_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. w_3= w_4 = \\dots = w_{10} =0\n\\tag{$3$}\n$$\n\n2.接着我们稍微放宽一下条件：我们不限定说一定要 3~10次多项式系数为0，我们只要要求满足有8项为0，即3项不为零（包括常数项），如公式（4）所示。这种Hypothesis称为 $H^{\\prime}_2$。并且这个Hypothesis与 $H_2$ $H_{10}$ 的关系如公式（5）所示。显然，  $H^{\\prime}_2$ 比 $H_2$ 更加的Flexible， 而 $H^{\\prime}_2$ 比 $H_{10}$ 的复杂度更低。\n\n$$\nH^{\\prime}_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. \\sum\\limits_{q=0}^{10}(w_q \\neq 0) \\leq 3\n\\tag{$4$}\n$$\n\n$$\nH_2 \\subset  H^{\\prime}_{2} \\subset H_{10}\n\\tag{$5$}\n$$\n\n3.但是这个Hypothesis $H^{\\prime}_2$ 的求解也是一个NP-Hard问题。所以我们继续寻找容易求解的宽松情况。如公式（6）所示，这种Hypothesis我们称为 $H(C)$ 其中C为常数，H(C)称为regularized hypothesis set。这个方程的限定条件是让 权重的平方和小于C，当C增大时，限定的条件越宽松。如果C接近于无穷大，那么就和 $H_{10}$ 没什么区别了,如公式（7）所示\n\n$$\nH^{\\prime}_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. \\sum\\limits_{q=0}^{10} w_q^2 = ||w||^2 \\leq C\n\\tag{$6$}\n$$\n\n$$\nH(0) \\subset H(1.126) \\subset \\dots \\subset H(1126) \\subset \\dots \\subset H(\\infty) \\subset H(10)\n\\tag{$7$}\n$$\n\n这种形式的限定条件是可以进行求解的，我们把求解的满足限定条件的权重w记为wREG。接下来就要探讨如何求解wREG。\n\n\n<br><br>\n----------------------------------\n\n## 2. Weight Decay Regularization\n1.首先我们为了表示方便，把公式（6）做一定的调整：写成向量矩阵的形式，得到公式（8）\n\n$$\n\\min\\limits_{w \\in R^{Q+1}} E_{in}(w) = \\frac{1}{N} \\underbrace{\\sum\\limits_{n=1}{N} (w^Tz_n - y_n) ^ 2}_{(Zw-y)^T(Zw-y)} \\quad\n\ns.t. \\underbrace{\\sum\\limits_{q=0}^Q w_q^2}_{w^Tw} \\leq C\n\\tag{$8$}\n$$\n\n\n\n2.我们的目的是计算 $E_{in}(w)$ 的最小值，其中限定条件是 $||w2|| \\leq C$ 如图二所示，这个限定条件从几何角度上的意思是，权重w被限定在半径为 $\\sqrt C$ 的圆内（红色的圆），而球外的w都不符合要求，即便它是靠近 $E_{in}(w)$ 梯度为零的w。\n\n实际的变化方向如蓝色的向量 $- \\nabla E_{in}$ 所示，其中红色向量normal 限定了切线的法向量的方向，而绿色的向量 $W_{lin}$ 就是我们需要想方法找到让他最短的时刻。\n\n显然，红色和绿色的向量其实就是蓝色向量的相互垂直的分向量，由高中的数学知识，我们知道，当蓝色向量越接近于红色的向量，那么绿色的向量长度越小，即如公式（9）所示\n\n$$\n- \\nabla E_{in}(W_{REG}) \\propto W_{REG}\n\\tag{$9$}\n$$\n\n\n![The Lagrange Multiplier](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/097a94496bbb81ecd7927daf3b0a828b057929f6/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-2%20The%20Lagrange%20Multiplier.png)\n<center> 图二 The Lagrange Multiplier <sup>[2]</sup></center>\n\n\n3.然后我们队公式（9）加入拉格朗日算子 $\\lambda$ ， 进行调整得到公式（10）\n\n$$\n\\begin{align}\n                   & - \\nabla E_{in}(W_{REG}) \\propto W_{REG} \\\\\n\\Longrightarrow    & - \\nabla E_{in}(W_{REG}) \\propto \\frac{2 \\lambda}{N} W_{REG} \\\\\n\\Longrightarrow    & - \\nabla E_{in}(W_{REG}) + \\frac{2 \\lambda}{N} W_{REG} = 0 \\\\\n\\Longrightarrow    & \\frac{2}{N} (Z^TZW_{REG} - Z^Ty) + \\frac{2 \\lambda}{N} W_{REG} = 0 \\\\\n\\Longrightarrow    & W_{REG} = (Z^T Z + \\lambda I )^{-1} Z^Ty\n\\end{align}\n\\tag{$10$}\n$$\n\n上式中包含了求逆矩阵的过程，因为 $Z^TZ$ 是半正定矩阵，如果 $\\lambda$ 大于零，那么 $Z^T Z+\\lambda I$ 一定是正定矩阵，即一定可逆。统计学上把这叫做ridge regression(岭回归)\n\n\n4.如果把公式（10）的反过来求积分，我们可以得到公式（11）。该表达式称为增广错误（augmented error）用 $E_{aug}(w)$ 表示，其中 $w^T w$ 为正则化项（regularizer）。之所以叫做增广错误，是因为比传统的多了一正则化项。\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{align}\n                   & - \\nabla E_{in}(W_{REG}) \\propto W_{REG} \\\\\n\\Longrightarrow    & E_{aug}(w) =E_{in} + \\frac{\\lambda}{N} w^Tw \\\\\n\\Longrightarrow    & W_{REG} = \\argmin\\limits_w E_{in}(w) + \\frac{\\lambda}{N} w^Tw\n\\end{align}\n\\tag{$11$}\n$$\n\n所以我们只需要调整不同的 $\\lambda$ 的值就可以对模型复杂度进行一定的调整，如图三所示。\n\n![Result of Weith Decay Regularization](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/ebda6e4b5050786832ddb317e28aaa7807be465c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-3%20Result%20of%20Weith%20Decay%20Regularization.png)\n<center> 图三 Result of Weith Decay Regularization <sup>[2]</sup></center>\n\n从图中可以看出，当λ=0时，发生了过拟合；当λ=0.0001时，拟合的效果很好；当λ=0.01和λ=1时，发生了欠拟合。我们可以把λ看成是一种penality，即对hypothesis复杂度的惩罚，λ越大，w就越小，对应于C值越小，即这种惩罚越大，拟合曲线就会越平滑，高阶项就会削弱，容易发生欠拟合。λ一般取比较小的值就能达到良好的拟合效果，过大过小都有问题，但究竟取什么值，要根据具体训练数据和模型进行分析与调试。\n\n这种regularization不仅可以用在多项式的hypothesis中，还可以应用在logistic regression等其他hypothesis中，都可以达到防止过拟合的效果。\n\n但是这种方法有一个问题：我们目前讨论的多项式是形如x,x2,x3,⋯,xn的形式，若x的范围限定在[-1,1]之间，那么可能导致 $x_n$ 相对于低阶的值要小得多，则其对于的w非常大，相当于要给高阶项设置很大的惩罚。 也就是说我们无论让每个w的元素设定不同的 $\\lambda$ ， 从而如果x的范围很大的话，那么会使得部分数据很小。\n\n> 克服这个问题的方法是用Legendre Polynomials代替x,x2,x3,⋯,xn这种形式，Legendre Polynomials各项之间是正交的，用它进行多项式拟合的效果更好。\n\n\n<br><br>\n----------------------------------\n\n\n## 3. Regularization and VC Theory\n本节介绍正则化与VC理论的关系。即从VC理论的角度说明为什么正则化的效果好\n> 我们将从2个角度来讨论这个问题： 1) 误差方程 2） VC Dimension\n\n\n1.首先对比 Augmented Error 和 VC Bound， 如图四所示\n\n![Augmented Error and VC Bound](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/50e614a04c16b422bc446eac1bca54f9306c630f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-4%20Augmented%20Error%20and%20VC%20Bound.png)\n<center> 图四 Augmented Error and VC Bound <sup>[3]</sup></center>\n\n根据Augmented Error和VC Bound的表达式，$\\Omega (w)$ 包含于 $Ω(H)$ 之内，所以，$E_{aug}(w)$ 比 $E_{in}$ 更接近于 $E_{out}(w)$\n\n\n2.从VC Dimension的角度。根据VC Dimension理论，整个hypothesis set的 $d_{vc}=d˘+1$ ，这是因为所有的w都考虑了，没有任何限制条件。而引入限定条件的 $d_{vc}(H(C))=d_{EFF}(H,A)$ ，即有效的VC Dimension。很显然，$d_{vc}$ 比较大，因为它代表了整个hypothesis set，但是 $d_{EFF}(H,A)$ 比较小，因为由于regularized的影响，限定了w只取一小部分。\n\n所以当 $\\lambda = 0$的时候，所有的w都没有收到惩罚，所以都考虑了，此时的 $d_{vc}$ 比较大，容易产生overfitting，当 $\\lambda > 0$ 的时候， 部分w的乘以0而被放弃考虑，所以  $d_{vc}$ 比较小，模型的复杂度就降下来了，就解决了overfitting的问题。当然如果 $\\lambda$ 太大，使得过多的w被舍弃，那么就会出现 underfitting的情况了\n\n\n<br><br>\n----------------------------------\n\n\n\n## 4. General Regularizers\nRegularizers主要有 L0, L1 和L2，前面用到的是 L2的Regularizer\n关于范数的概念老师课上没有讲的很明白，可以参考这两篇博客，后续我也会写关于范数的博客\n> @ TODO L0、L1与L2范数\n\n[机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995)\n\n[机器学习中的范数规则化之（二）核范数与规则项参数选择](http://blog.csdn.net/zouxy09/article/details/24972869)\n\n<br><br>\n----------------------------------\n\n\n\n\n\n# Summary\n1. 这一节首先介绍了Regularization：在Hypothesis Set的基础上 加入 Regularizer 作为 Penality。使得最终结果中的部分w被惩罚掉而不被考虑，从而实现了降低模型复杂度的功能。\n2. 接着我们讨论了为什么 Regularization 能让模型复杂度降低，并且模型的泛化能力更强\n3. 最后我们讨论了常用的范数:L0 L1 L2 范数\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\14\\14 - 1 - Regularized Hypothesis Set (19-16)\n\n[2] 机器学习基石(台湾大学-林轩田)\\14\\14 - 2 - Weight Decay Regularization (24-08)\n\n[3] 机器学习基石(台湾大学-林轩田)\\14\\14 - 3 - Regularization and VC Theory (08-15)\n\n\n<br><br>\n----------------------------------\n","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-17-12.How can Machine Learn Better - Regularization.md","raw":"---\ntitle: 12.How can Machine Learn Better? - Regularization\ndate: 2017-10-17 11:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# How can Machine Learn Better? - Regularization\n\n\n## 1. Regularized Hypothesis Set\n正则化的主要思想：将假设函数从高次多项式的数降低到低次，即把复杂的模型变成简单模型。如图一所示的表示高次多项式函数，明显产生了过拟合现象，而左图的表示使用正则化后的低次函数。并且从图中的下方的Hypothesis Set的圈中可以看出，高次的多项式会包含低次的多项式。所以在转换的过程，就是把模型从外圈降至内圈的过程。\n\n> 但是：Regularization 不适用于多个解的模型，因为在降阶的过程中面临着选择最适解\n\n![Regularization Fit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/1ef39e622b59906f5831d3493e000c306845a087/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-1%20Regularization%20Fit.png)\n<center> 图一 Regularization Fit <sup>[1]</sup></center>\n\n\n下面来讨论如何进行降阶：\n1.首先我们在列出10次和2次的多项式，如公式（1）（2）所示。对比可以发现，其实 $H_2$ 可以看成是 $H_{10}$ 的3~10次项的系数为0，如公式（3）所示，其中 $s.t.$ 是subject to的意思，即约束条件\n\n$$\nH_{10} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10}\n\\tag{$1$}\n$$\n\n$$\nH_{2} = w_0 + w_1 x + w_2 x^2\n\\tag{$2$}\n$$\n\n\n$$\nH_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. w_3= w_4 = \\dots = w_{10} =0\n\\tag{$3$}\n$$\n\n2.接着我们稍微放宽一下条件：我们不限定说一定要 3~10次多项式系数为0，我们只要要求满足有8项为0，即3项不为零（包括常数项），如公式（4）所示。这种Hypothesis称为 $H^{\\prime}_2$。并且这个Hypothesis与 $H_2$ $H_{10}$ 的关系如公式（5）所示。显然，  $H^{\\prime}_2$ 比 $H_2$ 更加的Flexible， 而 $H^{\\prime}_2$ 比 $H_{10}$ 的复杂度更低。\n\n$$\nH^{\\prime}_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. \\sum\\limits_{q=0}^{10}(w_q \\neq 0) \\leq 3\n\\tag{$4$}\n$$\n\n$$\nH_2 \\subset  H^{\\prime}_{2} \\subset H_{10}\n\\tag{$5$}\n$$\n\n3.但是这个Hypothesis $H^{\\prime}_2$ 的求解也是一个NP-Hard问题。所以我们继续寻找容易求解的宽松情况。如公式（6）所示，这种Hypothesis我们称为 $H(C)$ 其中C为常数，H(C)称为regularized hypothesis set。这个方程的限定条件是让 权重的平方和小于C，当C增大时，限定的条件越宽松。如果C接近于无穷大，那么就和 $H_{10}$ 没什么区别了,如公式（7）所示\n\n$$\nH^{\\prime}_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. \\sum\\limits_{q=0}^{10} w_q^2 = ||w||^2 \\leq C\n\\tag{$6$}\n$$\n\n$$\nH(0) \\subset H(1.126) \\subset \\dots \\subset H(1126) \\subset \\dots \\subset H(\\infty) \\subset H(10)\n\\tag{$7$}\n$$\n\n这种形式的限定条件是可以进行求解的，我们把求解的满足限定条件的权重w记为wREG。接下来就要探讨如何求解wREG。\n\n\n<br><br>\n----------------------------------\n\n## 2. Weight Decay Regularization\n1.首先我们为了表示方便，把公式（6）做一定的调整：写成向量矩阵的形式，得到公式（8）\n\n$$\n\\min\\limits_{w \\in R^{Q+1}} E_{in}(w) = \\frac{1}{N} \\underbrace{\\sum\\limits_{n=1}{N} (w^Tz_n - y_n) ^ 2}_{(Zw-y)^T(Zw-y)} \\quad\n\ns.t. \\underbrace{\\sum\\limits_{q=0}^Q w_q^2}_{w^Tw} \\leq C\n\\tag{$8$}\n$$\n\n\n\n2.我们的目的是计算 $E_{in}(w)$ 的最小值，其中限定条件是 $||w2|| \\leq C$ 如图二所示，这个限定条件从几何角度上的意思是，权重w被限定在半径为 $\\sqrt C$ 的圆内（红色的圆），而球外的w都不符合要求，即便它是靠近 $E_{in}(w)$ 梯度为零的w。\n\n实际的变化方向如蓝色的向量 $- \\nabla E_{in}$ 所示，其中红色向量normal 限定了切线的法向量的方向，而绿色的向量 $W_{lin}$ 就是我们需要想方法找到让他最短的时刻。\n\n显然，红色和绿色的向量其实就是蓝色向量的相互垂直的分向量，由高中的数学知识，我们知道，当蓝色向量越接近于红色的向量，那么绿色的向量长度越小，即如公式（9）所示\n\n$$\n- \\nabla E_{in}(W_{REG}) \\propto W_{REG}\n\\tag{$9$}\n$$\n\n\n![The Lagrange Multiplier](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/097a94496bbb81ecd7927daf3b0a828b057929f6/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-2%20The%20Lagrange%20Multiplier.png)\n<center> 图二 The Lagrange Multiplier <sup>[2]</sup></center>\n\n\n3.然后我们队公式（9）加入拉格朗日算子 $\\lambda$ ， 进行调整得到公式（10）\n\n$$\n\\begin{align}\n                   & - \\nabla E_{in}(W_{REG}) \\propto W_{REG} \\\\\n\\Longrightarrow    & - \\nabla E_{in}(W_{REG}) \\propto \\frac{2 \\lambda}{N} W_{REG} \\\\\n\\Longrightarrow    & - \\nabla E_{in}(W_{REG}) + \\frac{2 \\lambda}{N} W_{REG} = 0 \\\\\n\\Longrightarrow    & \\frac{2}{N} (Z^TZW_{REG} - Z^Ty) + \\frac{2 \\lambda}{N} W_{REG} = 0 \\\\\n\\Longrightarrow    & W_{REG} = (Z^T Z + \\lambda I )^{-1} Z^Ty\n\\end{align}\n\\tag{$10$}\n$$\n\n上式中包含了求逆矩阵的过程，因为 $Z^TZ$ 是半正定矩阵，如果 $\\lambda$ 大于零，那么 $Z^T Z+\\lambda I$ 一定是正定矩阵，即一定可逆。统计学上把这叫做ridge regression(岭回归)\n\n\n4.如果把公式（10）的反过来求积分，我们可以得到公式（11）。该表达式称为增广错误（augmented error）用 $E_{aug}(w)$ 表示，其中 $w^T w$ 为正则化项（regularizer）。之所以叫做增广错误，是因为比传统的多了一正则化项。\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{align}\n                   & - \\nabla E_{in}(W_{REG}) \\propto W_{REG} \\\\\n\\Longrightarrow    & E_{aug}(w) =E_{in} + \\frac{\\lambda}{N} w^Tw \\\\\n\\Longrightarrow    & W_{REG} = \\argmin\\limits_w E_{in}(w) + \\frac{\\lambda}{N} w^Tw\n\\end{align}\n\\tag{$11$}\n$$\n\n所以我们只需要调整不同的 $\\lambda$ 的值就可以对模型复杂度进行一定的调整，如图三所示。\n\n![Result of Weith Decay Regularization](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/ebda6e4b5050786832ddb317e28aaa7807be465c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-3%20Result%20of%20Weith%20Decay%20Regularization.png)\n<center> 图三 Result of Weith Decay Regularization <sup>[2]</sup></center>\n\n从图中可以看出，当λ=0时，发生了过拟合；当λ=0.0001时，拟合的效果很好；当λ=0.01和λ=1时，发生了欠拟合。我们可以把λ看成是一种penality，即对hypothesis复杂度的惩罚，λ越大，w就越小，对应于C值越小，即这种惩罚越大，拟合曲线就会越平滑，高阶项就会削弱，容易发生欠拟合。λ一般取比较小的值就能达到良好的拟合效果，过大过小都有问题，但究竟取什么值，要根据具体训练数据和模型进行分析与调试。\n\n这种regularization不仅可以用在多项式的hypothesis中，还可以应用在logistic regression等其他hypothesis中，都可以达到防止过拟合的效果。\n\n但是这种方法有一个问题：我们目前讨论的多项式是形如x,x2,x3,⋯,xn的形式，若x的范围限定在[-1,1]之间，那么可能导致 $x_n$ 相对于低阶的值要小得多，则其对于的w非常大，相当于要给高阶项设置很大的惩罚。 也就是说我们无论让每个w的元素设定不同的 $\\lambda$ ， 从而如果x的范围很大的话，那么会使得部分数据很小。\n\n> 克服这个问题的方法是用Legendre Polynomials代替x,x2,x3,⋯,xn这种形式，Legendre Polynomials各项之间是正交的，用它进行多项式拟合的效果更好。\n\n\n<br><br>\n----------------------------------\n\n\n## 3. Regularization and VC Theory\n本节介绍正则化与VC理论的关系。即从VC理论的角度说明为什么正则化的效果好\n> 我们将从2个角度来讨论这个问题： 1) 误差方程 2） VC Dimension\n\n\n1.首先对比 Augmented Error 和 VC Bound， 如图四所示\n\n![Augmented Error and VC Bound](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/50e614a04c16b422bc446eac1bca54f9306c630f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-4%20Augmented%20Error%20and%20VC%20Bound.png)\n<center> 图四 Augmented Error and VC Bound <sup>[3]</sup></center>\n\n根据Augmented Error和VC Bound的表达式，$\\Omega (w)$ 包含于 $Ω(H)$ 之内，所以，$E_{aug}(w)$ 比 $E_{in}$ 更接近于 $E_{out}(w)$\n\n\n2.从VC Dimension的角度。根据VC Dimension理论，整个hypothesis set的 $d_{vc}=d˘+1$ ，这是因为所有的w都考虑了，没有任何限制条件。而引入限定条件的 $d_{vc}(H(C))=d_{EFF}(H,A)$ ，即有效的VC Dimension。很显然，$d_{vc}$ 比较大，因为它代表了整个hypothesis set，但是 $d_{EFF}(H,A)$ 比较小，因为由于regularized的影响，限定了w只取一小部分。\n\n所以当 $\\lambda = 0$的时候，所有的w都没有收到惩罚，所以都考虑了，此时的 $d_{vc}$ 比较大，容易产生overfitting，当 $\\lambda > 0$ 的时候， 部分w的乘以0而被放弃考虑，所以  $d_{vc}$ 比较小，模型的复杂度就降下来了，就解决了overfitting的问题。当然如果 $\\lambda$ 太大，使得过多的w被舍弃，那么就会出现 underfitting的情况了\n\n\n<br><br>\n----------------------------------\n\n\n\n## 4. General Regularizers\nRegularizers主要有 L0, L1 和L2，前面用到的是 L2的Regularizer\n关于范数的概念老师课上没有讲的很明白，可以参考这两篇博客，后续我也会写关于范数的博客\n> @ TODO L0、L1与L2范数\n\n[机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995)\n\n[机器学习中的范数规则化之（二）核范数与规则项参数选择](http://blog.csdn.net/zouxy09/article/details/24972869)\n\n<br><br>\n----------------------------------\n\n\n\n\n\n# Summary\n1. 这一节首先介绍了Regularization：在Hypothesis Set的基础上 加入 Regularizer 作为 Penality。使得最终结果中的部分w被惩罚掉而不被考虑，从而实现了降低模型复杂度的功能。\n2. 接着我们讨论了为什么 Regularization 能让模型复杂度降低，并且模型的泛化能力更强\n3. 最后我们讨论了常用的范数:L0 L1 L2 范数\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\14\\14 - 1 - Regularized Hypothesis Set (19-16)\n\n[2] 机器学习基石(台湾大学-林轩田)\\14\\14 - 2 - Weight Decay Regularization (24-08)\n\n[3] 机器学习基石(台湾大学-林轩田)\\14\\14 - 3 - Regularization and VC Theory (08-15)\n\n\n<br><br>\n----------------------------------\n","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-17-12.How can Machine Learn Better - Regularization","published":1,"updated":"2018-04-14T19:42:06.508Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2en0020rwtjs8k824j4","content":"<h1 id=\"How-can-Machine-Learn-Better-Regularization\"><a href=\"#How-can-Machine-Learn-Better-Regularization\" class=\"headerlink\" title=\"How can Machine Learn Better? - Regularization\"></a>How can Machine Learn Better? - Regularization</h1><h2 id=\"1-Regularized-Hypothesis-Set\"><a href=\"#1-Regularized-Hypothesis-Set\" class=\"headerlink\" title=\"1. Regularized Hypothesis Set\"></a>1. Regularized Hypothesis Set</h2><p>正则化的主要思想：将假设函数从高次多项式的数降低到低次，即把复杂的模型变成简单模型。如图一所示的表示高次多项式函数，明显产生了过拟合现象，而左图的表示使用正则化后的低次函数。并且从图中的下方的Hypothesis Set的圈中可以看出，高次的多项式会包含低次的多项式。所以在转换的过程，就是把模型从外圈降至内圈的过程。</p>\n<blockquote>\n<p>但是：Regularization 不适用于多个解的模型，因为在降阶的过程中面临着选择最适解</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/1ef39e622b59906f5831d3493e000c306845a087/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-1%20Regularization%20Fit.png\" alt=\"Regularization Fit\"></p>\n<center> 图一 Regularization Fit <sup>[1]</sup></center>\n\n\n<p>下面来讨论如何进行降阶：<br>1.首先我们在列出10次和2次的多项式，如公式（1）（2）所示。对比可以发现，其实 $H_2$ 可以看成是 $H_{10}$ 的3~10次项的系数为0，如公式（3）所示，其中 $s.t.$ 是subject to的意思，即约束条件</p>\n<script type=\"math/tex; mode=display\">\nH_{10} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10}\n\\tag{$1$}</script><script type=\"math/tex; mode=display\">\nH_{2} = w_0 + w_1 x + w_2 x^2\n\\tag{$2$}</script><script type=\"math/tex; mode=display\">\nH_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. w_3= w_4 = \\dots = w_{10} =0\n\\tag{$3$}</script><p>2.接着我们稍微放宽一下条件：我们不限定说一定要 3~10次多项式系数为0，我们只要要求满足有8项为0，即3项不为零（包括常数项），如公式（4）所示。这种Hypothesis称为 $H^{\\prime}_2$。并且这个Hypothesis与 $H_2$ $H_{10}$ 的关系如公式（5）所示。显然，  $H^{\\prime}_2$ 比 $H_2$ 更加的Flexible， 而 $H^{\\prime}_2$ 比 $H_{10}$ 的复杂度更低。</p>\n<script type=\"math/tex; mode=display\">\nH^{\\prime}_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. \\sum\\limits_{q=0}^{10}(w_q \\neq 0) \\leq 3\n\\tag{$4$}</script><script type=\"math/tex; mode=display\">\nH_2 \\subset  H^{\\prime}_{2} \\subset H_{10}\n\\tag{$5$}</script><p>3.但是这个Hypothesis $H^{\\prime}_2$ 的求解也是一个NP-Hard问题。所以我们继续寻找容易求解的宽松情况。如公式（6）所示，这种Hypothesis我们称为 $H(C)$ 其中C为常数，H(C)称为regularized hypothesis set。这个方程的限定条件是让 权重的平方和小于C，当C增大时，限定的条件越宽松。如果C接近于无穷大，那么就和 $H_{10}$ 没什么区别了,如公式（7）所示</p>\n<script type=\"math/tex; mode=display\">\nH^{\\prime}_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. \\sum\\limits_{q=0}^{10} w_q^2 = ||w||^2 \\leq C\n\\tag{$6$}</script><script type=\"math/tex; mode=display\">\nH(0) \\subset H(1.126) \\subset \\dots \\subset H(1126) \\subset \\dots \\subset H(\\infty) \\subset H(10)\n\\tag{$7$}</script><p>这种形式的限定条件是可以进行求解的，我们把求解的满足限定条件的权重w记为wREG。接下来就要探讨如何求解wREG。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Weight-Decay-Regularization\"><a href=\"#2-Weight-Decay-Regularization\" class=\"headerlink\" title=\"2. Weight Decay Regularization\"></a>2. Weight Decay Regularization</h2><p>1.首先我们为了表示方便，把公式（6）做一定的调整：写成向量矩阵的形式，得到公式（8）</p>\n<script type=\"math/tex; mode=display\">\n\\min\\limits_{w \\in R^{Q+1}} E_{in}(w) = \\frac{1}{N} \\underbrace{\\sum\\limits_{n=1}{N} (w^Tz_n - y_n) ^ 2}_{(Zw-y)^T(Zw-y)} \\quad\n\ns.t. \\underbrace{\\sum\\limits_{q=0}^Q w_q^2}_{w^Tw} \\leq C\n\\tag{$8$}</script><p>2.我们的目的是计算 $E_{in}(w)$ 的最小值，其中限定条件是 $||w2|| \\leq C$ 如图二所示，这个限定条件从几何角度上的意思是，权重w被限定在半径为 $\\sqrt C$ 的圆内（红色的圆），而球外的w都不符合要求，即便它是靠近 $E_{in}(w)$ 梯度为零的w。</p>\n<p>实际的变化方向如蓝色的向量 $- \\nabla E_{in}$ 所示，其中红色向量normal 限定了切线的法向量的方向，而绿色的向量 $W_{lin}$ 就是我们需要想方法找到让他最短的时刻。</p>\n<p>显然，红色和绿色的向量其实就是蓝色向量的相互垂直的分向量，由高中的数学知识，我们知道，当蓝色向量越接近于红色的向量，那么绿色的向量长度越小，即如公式（9）所示</p>\n<script type=\"math/tex; mode=display\">\n- \\nabla E_{in}(W_{REG}) \\propto W_{REG}\n\\tag{$9$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/097a94496bbb81ecd7927daf3b0a828b057929f6/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-2%20The%20Lagrange%20Multiplier.png\" alt=\"The Lagrange Multiplier\"></p>\n<center> 图二 The Lagrange Multiplier <sup>[2]</sup></center>\n\n\n<p>3.然后我们队公式（9）加入拉格朗日算子 $\\lambda$ ， 进行调整得到公式（10）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n                   & - \\nabla E_{in}(W_{REG}) \\propto W_{REG} \\\\\n\\Longrightarrow    & - \\nabla E_{in}(W_{REG}) \\propto \\frac{2 \\lambda}{N} W_{REG} \\\\\n\\Longrightarrow    & - \\nabla E_{in}(W_{REG}) + \\frac{2 \\lambda}{N} W_{REG} = 0 \\\\\n\\Longrightarrow    & \\frac{2}{N} (Z^TZW_{REG} - Z^Ty) + \\frac{2 \\lambda}{N} W_{REG} = 0 \\\\\n\\Longrightarrow    & W_{REG} = (Z^T Z + \\lambda I )^{-1} Z^Ty\n\\end{align}\n\\tag{$10$}</script><p>上式中包含了求逆矩阵的过程，因为 $Z^TZ$ 是半正定矩阵，如果 $\\lambda$ 大于零，那么 $Z^T Z+\\lambda I$ 一定是正定矩阵，即一定可逆。统计学上把这叫做ridge regression(岭回归)</p>\n<p>4.如果把公式（10）的反过来求积分，我们可以得到公式（11）。该表达式称为增广错误（augmented error）用 $E_{aug}(w)$ 表示，其中 $w^T w$ 为正则化项（regularizer）。之所以叫做增广错误，是因为比传统的多了一正则化项。</p>\n<script type=\"math/tex; mode=display\">\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{align}\n                   & - \\nabla E_{in}(W_{REG}) \\propto W_{REG} \\\\\n\\Longrightarrow    & E_{aug}(w) =E_{in} + \\frac{\\lambda}{N} w^Tw \\\\\n\\Longrightarrow    & W_{REG} = \\argmin\\limits_w E_{in}(w) + \\frac{\\lambda}{N} w^Tw\n\\end{align}\n\\tag{$11$}</script><p>所以我们只需要调整不同的 $\\lambda$ 的值就可以对模型复杂度进行一定的调整，如图三所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/ebda6e4b5050786832ddb317e28aaa7807be465c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-3%20Result%20of%20Weith%20Decay%20Regularization.png\" alt=\"Result of Weith Decay Regularization\"></p>\n<center> 图三 Result of Weith Decay Regularization <sup>[2]</sup></center>\n\n<p>从图中可以看出，当λ=0时，发生了过拟合；当λ=0.0001时，拟合的效果很好；当λ=0.01和λ=1时，发生了欠拟合。我们可以把λ看成是一种penality，即对hypothesis复杂度的惩罚，λ越大，w就越小，对应于C值越小，即这种惩罚越大，拟合曲线就会越平滑，高阶项就会削弱，容易发生欠拟合。λ一般取比较小的值就能达到良好的拟合效果，过大过小都有问题，但究竟取什么值，要根据具体训练数据和模型进行分析与调试。</p>\n<p>这种regularization不仅可以用在多项式的hypothesis中，还可以应用在logistic regression等其他hypothesis中，都可以达到防止过拟合的效果。</p>\n<p>但是这种方法有一个问题：我们目前讨论的多项式是形如x,x2,x3,⋯,xn的形式，若x的范围限定在[-1,1]之间，那么可能导致 $x_n$ 相对于低阶的值要小得多，则其对于的w非常大，相当于要给高阶项设置很大的惩罚。 也就是说我们无论让每个w的元素设定不同的 $\\lambda$ ， 从而如果x的范围很大的话，那么会使得部分数据很小。</p>\n<blockquote>\n<p>克服这个问题的方法是用Legendre Polynomials代替x,x2,x3,⋯,xn这种形式，Legendre Polynomials各项之间是正交的，用它进行多项式拟合的效果更好。</p>\n</blockquote>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Regularization-and-VC-Theory\"><a href=\"#3-Regularization-and-VC-Theory\" class=\"headerlink\" title=\"3. Regularization and VC Theory\"></a>3. Regularization and VC Theory</h2><p>本节介绍正则化与VC理论的关系。即从VC理论的角度说明为什么正则化的效果好</p>\n<blockquote>\n<p>我们将从2个角度来讨论这个问题： 1) 误差方程 2） VC Dimension</p>\n</blockquote>\n<p>1.首先对比 Augmented Error 和 VC Bound， 如图四所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/50e614a04c16b422bc446eac1bca54f9306c630f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-4%20Augmented%20Error%20and%20VC%20Bound.png\" alt=\"Augmented Error and VC Bound\"></p>\n<center> 图四 Augmented Error and VC Bound <sup>[3]</sup></center>\n\n<p>根据Augmented Error和VC Bound的表达式，$\\Omega (w)$ 包含于 $Ω(H)$ 之内，所以，$E_{aug}(w)$ 比 $E_{in}$ 更接近于 $E_{out}(w)$</p>\n<p>2.从VC Dimension的角度。根据VC Dimension理论，整个hypothesis set的 $d_{vc}=d˘+1$ ，这是因为所有的w都考虑了，没有任何限制条件。而引入限定条件的 $d_{vc}(H(C))=d_{EFF}(H,A)$ ，即有效的VC Dimension。很显然，$d_{vc}$ 比较大，因为它代表了整个hypothesis set，但是 $d_{EFF}(H,A)$ 比较小，因为由于regularized的影响，限定了w只取一小部分。</p>\n<p>所以当 $\\lambda = 0$的时候，所有的w都没有收到惩罚，所以都考虑了，此时的 $d_{vc}$ 比较大，容易产生overfitting，当 $\\lambda &gt; 0$ 的时候， 部分w的乘以0而被放弃考虑，所以  $d_{vc}$ 比较小，模型的复杂度就降下来了，就解决了overfitting的问题。当然如果 $\\lambda$ 太大，使得过多的w被舍弃，那么就会出现 underfitting的情况了</p>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"4-General-Regularizers\"><a href=\"#4-General-Regularizers\" class=\"headerlink\" title=\"4. General Regularizers\"></a>4. General Regularizers</h2><p>Regularizers主要有 L0, L1 和L2，前面用到的是 L2的Regularizer<br>关于范数的概念老师课上没有讲的很明白，可以参考这两篇博客，后续我也会写关于范数的博客</p>\n<blockquote>\n<p>@ TODO L0、L1与L2范数</p>\n</blockquote>\n<p><a href=\"http://blog.csdn.net/zouxy09/article/details/24971995\" target=\"_blank\" rel=\"external\">机器学习中的范数规则化之（一）L0、L1与L2范数</a></p>\n<p><a href=\"http://blog.csdn.net/zouxy09/article/details/24972869\" target=\"_blank\" rel=\"external\">机器学习中的范数规则化之（二）核范数与规则项参数选择</a></p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>这一节首先介绍了Regularization：在Hypothesis Set的基础上 加入 Regularizer 作为 Penality。使得最终结果中的部分w被惩罚掉而不被考虑，从而实现了降低模型复杂度的功能。</li>\n<li>接着我们讨论了为什么 Regularization 能让模型复杂度降低，并且模型的泛化能力更强</li>\n<li>最后我们讨论了常用的范数:L0 L1 L2 范数</li>\n</ol>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\14\\14 - 1 - Regularized Hypothesis Set (19-16)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\14\\14 - 2 - Weight Decay Regularization (24-08)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\14\\14 - 3 - Regularization and VC Theory (08-15)</p>\n<h2 id=\"-5\"><a href=\"#-5\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Better-Regularization\"><a href=\"#How-can-Machine-Learn-Better-Regularization\" class=\"headerlink\" title=\"How can Machine Learn Better? - Regularization\"></a>How can Machine Learn Better? - Regularization</h1><h2 id=\"1-Regularized-Hypothesis-Set\"><a href=\"#1-Regularized-Hypothesis-Set\" class=\"headerlink\" title=\"1. Regularized Hypothesis Set\"></a>1. Regularized Hypothesis Set</h2><p>正则化的主要思想：将假设函数从高次多项式的数降低到低次，即把复杂的模型变成简单模型。如图一所示的表示高次多项式函数，明显产生了过拟合现象，而左图的表示使用正则化后的低次函数。并且从图中的下方的Hypothesis Set的圈中可以看出，高次的多项式会包含低次的多项式。所以在转换的过程，就是把模型从外圈降至内圈的过程。</p>\n<blockquote>\n<p>但是：Regularization 不适用于多个解的模型，因为在降阶的过程中面临着选择最适解</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/1ef39e622b59906f5831d3493e000c306845a087/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-1%20Regularization%20Fit.png\" alt=\"Regularization Fit\"></p>\n<center> 图一 Regularization Fit <sup>[1]</sup></center>\n\n\n<p>下面来讨论如何进行降阶：<br>1.首先我们在列出10次和2次的多项式，如公式（1）（2）所示。对比可以发现，其实 $H_2$ 可以看成是 $H_{10}$ 的3~10次项的系数为0，如公式（3）所示，其中 $s.t.$ 是subject to的意思，即约束条件</p>\n<script type=\"math/tex; mode=display\">\nH_{10} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10}\n\\tag{$1$}</script><script type=\"math/tex; mode=display\">\nH_{2} = w_0 + w_1 x + w_2 x^2\n\\tag{$2$}</script><script type=\"math/tex; mode=display\">\nH_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. w_3= w_4 = \\dots = w_{10} =0\n\\tag{$3$}</script><p>2.接着我们稍微放宽一下条件：我们不限定说一定要 3~10次多项式系数为0，我们只要要求满足有8项为0，即3项不为零（包括常数项），如公式（4）所示。这种Hypothesis称为 $H^{\\prime}_2$。并且这个Hypothesis与 $H_2$ $H_{10}$ 的关系如公式（5）所示。显然，  $H^{\\prime}_2$ 比 $H_2$ 更加的Flexible， 而 $H^{\\prime}_2$ 比 $H_{10}$ 的复杂度更低。</p>\n<script type=\"math/tex; mode=display\">\nH^{\\prime}_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. \\sum\\limits_{q=0}^{10}(w_q \\neq 0) \\leq 3\n\\tag{$4$}</script><script type=\"math/tex; mode=display\">\nH_2 \\subset  H^{\\prime}_{2} \\subset H_{10}\n\\tag{$5$}</script><p>3.但是这个Hypothesis $H^{\\prime}_2$ 的求解也是一个NP-Hard问题。所以我们继续寻找容易求解的宽松情况。如公式（6）所示，这种Hypothesis我们称为 $H(C)$ 其中C为常数，H(C)称为regularized hypothesis set。这个方程的限定条件是让 权重的平方和小于C，当C增大时，限定的条件越宽松。如果C接近于无穷大，那么就和 $H_{10}$ 没什么区别了,如公式（7）所示</p>\n<script type=\"math/tex; mode=display\">\nH^{\\prime}_{2} =  w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\dots + w_{10} x^{10} \\quad s.t. \\sum\\limits_{q=0}^{10} w_q^2 = ||w||^2 \\leq C\n\\tag{$6$}</script><script type=\"math/tex; mode=display\">\nH(0) \\subset H(1.126) \\subset \\dots \\subset H(1126) \\subset \\dots \\subset H(\\infty) \\subset H(10)\n\\tag{$7$}</script><p>这种形式的限定条件是可以进行求解的，我们把求解的满足限定条件的权重w记为wREG。接下来就要探讨如何求解wREG。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Weight-Decay-Regularization\"><a href=\"#2-Weight-Decay-Regularization\" class=\"headerlink\" title=\"2. Weight Decay Regularization\"></a>2. Weight Decay Regularization</h2><p>1.首先我们为了表示方便，把公式（6）做一定的调整：写成向量矩阵的形式，得到公式（8）</p>\n<script type=\"math/tex; mode=display\">\n\\min\\limits_{w \\in R^{Q+1}} E_{in}(w) = \\frac{1}{N} \\underbrace{\\sum\\limits_{n=1}{N} (w^Tz_n - y_n) ^ 2}_{(Zw-y)^T(Zw-y)} \\quad\n\ns.t. \\underbrace{\\sum\\limits_{q=0}^Q w_q^2}_{w^Tw} \\leq C\n\\tag{$8$}</script><p>2.我们的目的是计算 $E_{in}(w)$ 的最小值，其中限定条件是 $||w2|| \\leq C$ 如图二所示，这个限定条件从几何角度上的意思是，权重w被限定在半径为 $\\sqrt C$ 的圆内（红色的圆），而球外的w都不符合要求，即便它是靠近 $E_{in}(w)$ 梯度为零的w。</p>\n<p>实际的变化方向如蓝色的向量 $- \\nabla E_{in}$ 所示，其中红色向量normal 限定了切线的法向量的方向，而绿色的向量 $W_{lin}$ 就是我们需要想方法找到让他最短的时刻。</p>\n<p>显然，红色和绿色的向量其实就是蓝色向量的相互垂直的分向量，由高中的数学知识，我们知道，当蓝色向量越接近于红色的向量，那么绿色的向量长度越小，即如公式（9）所示</p>\n<script type=\"math/tex; mode=display\">\n- \\nabla E_{in}(W_{REG}) \\propto W_{REG}\n\\tag{$9$}</script><p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/097a94496bbb81ecd7927daf3b0a828b057929f6/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-2%20The%20Lagrange%20Multiplier.png\" alt=\"The Lagrange Multiplier\"></p>\n<center> 图二 The Lagrange Multiplier <sup>[2]</sup></center>\n\n\n<p>3.然后我们队公式（9）加入拉格朗日算子 $\\lambda$ ， 进行调整得到公式（10）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n                   & - \\nabla E_{in}(W_{REG}) \\propto W_{REG} \\\\\n\\Longrightarrow    & - \\nabla E_{in}(W_{REG}) \\propto \\frac{2 \\lambda}{N} W_{REG} \\\\\n\\Longrightarrow    & - \\nabla E_{in}(W_{REG}) + \\frac{2 \\lambda}{N} W_{REG} = 0 \\\\\n\\Longrightarrow    & \\frac{2}{N} (Z^TZW_{REG} - Z^Ty) + \\frac{2 \\lambda}{N} W_{REG} = 0 \\\\\n\\Longrightarrow    & W_{REG} = (Z^T Z + \\lambda I )^{-1} Z^Ty\n\\end{align}\n\\tag{$10$}</script><p>上式中包含了求逆矩阵的过程，因为 $Z^TZ$ 是半正定矩阵，如果 $\\lambda$ 大于零，那么 $Z^T Z+\\lambda I$ 一定是正定矩阵，即一定可逆。统计学上把这叫做ridge regression(岭回归)</p>\n<p>4.如果把公式（10）的反过来求积分，我们可以得到公式（11）。该表达式称为增广错误（augmented error）用 $E_{aug}(w)$ 表示，其中 $w^T w$ 为正则化项（regularizer）。之所以叫做增广错误，是因为比传统的多了一正则化项。</p>\n<script type=\"math/tex; mode=display\">\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\begin{align}\n                   & - \\nabla E_{in}(W_{REG}) \\propto W_{REG} \\\\\n\\Longrightarrow    & E_{aug}(w) =E_{in} + \\frac{\\lambda}{N} w^Tw \\\\\n\\Longrightarrow    & W_{REG} = \\argmin\\limits_w E_{in}(w) + \\frac{\\lambda}{N} w^Tw\n\\end{align}\n\\tag{$11$}</script><p>所以我们只需要调整不同的 $\\lambda$ 的值就可以对模型复杂度进行一定的调整，如图三所示。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/ebda6e4b5050786832ddb317e28aaa7807be465c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-3%20Result%20of%20Weith%20Decay%20Regularization.png\" alt=\"Result of Weith Decay Regularization\"></p>\n<center> 图三 Result of Weith Decay Regularization <sup>[2]</sup></center>\n\n<p>从图中可以看出，当λ=0时，发生了过拟合；当λ=0.0001时，拟合的效果很好；当λ=0.01和λ=1时，发生了欠拟合。我们可以把λ看成是一种penality，即对hypothesis复杂度的惩罚，λ越大，w就越小，对应于C值越小，即这种惩罚越大，拟合曲线就会越平滑，高阶项就会削弱，容易发生欠拟合。λ一般取比较小的值就能达到良好的拟合效果，过大过小都有问题，但究竟取什么值，要根据具体训练数据和模型进行分析与调试。</p>\n<p>这种regularization不仅可以用在多项式的hypothesis中，还可以应用在logistic regression等其他hypothesis中，都可以达到防止过拟合的效果。</p>\n<p>但是这种方法有一个问题：我们目前讨论的多项式是形如x,x2,x3,⋯,xn的形式，若x的范围限定在[-1,1]之间，那么可能导致 $x_n$ 相对于低阶的值要小得多，则其对于的w非常大，相当于要给高阶项设置很大的惩罚。 也就是说我们无论让每个w的元素设定不同的 $\\lambda$ ， 从而如果x的范围很大的话，那么会使得部分数据很小。</p>\n<blockquote>\n<p>克服这个问题的方法是用Legendre Polynomials代替x,x2,x3,⋯,xn这种形式，Legendre Polynomials各项之间是正交的，用它进行多项式拟合的效果更好。</p>\n</blockquote>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Regularization-and-VC-Theory\"><a href=\"#3-Regularization-and-VC-Theory\" class=\"headerlink\" title=\"3. Regularization and VC Theory\"></a>3. Regularization and VC Theory</h2><p>本节介绍正则化与VC理论的关系。即从VC理论的角度说明为什么正则化的效果好</p>\n<blockquote>\n<p>我们将从2个角度来讨论这个问题： 1) 误差方程 2） VC Dimension</p>\n</blockquote>\n<p>1.首先对比 Augmented Error 和 VC Bound， 如图四所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/50e614a04c16b422bc446eac1bca54f9306c630f/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter12-4%20Augmented%20Error%20and%20VC%20Bound.png\" alt=\"Augmented Error and VC Bound\"></p>\n<center> 图四 Augmented Error and VC Bound <sup>[3]</sup></center>\n\n<p>根据Augmented Error和VC Bound的表达式，$\\Omega (w)$ 包含于 $Ω(H)$ 之内，所以，$E_{aug}(w)$ 比 $E_{in}$ 更接近于 $E_{out}(w)$</p>\n<p>2.从VC Dimension的角度。根据VC Dimension理论，整个hypothesis set的 $d_{vc}=d˘+1$ ，这是因为所有的w都考虑了，没有任何限制条件。而引入限定条件的 $d_{vc}(H(C))=d_{EFF}(H,A)$ ，即有效的VC Dimension。很显然，$d_{vc}$ 比较大，因为它代表了整个hypothesis set，但是 $d_{EFF}(H,A)$ 比较小，因为由于regularized的影响，限定了w只取一小部分。</p>\n<p>所以当 $\\lambda = 0$的时候，所有的w都没有收到惩罚，所以都考虑了，此时的 $d_{vc}$ 比较大，容易产生overfitting，当 $\\lambda &gt; 0$ 的时候， 部分w的乘以0而被放弃考虑，所以  $d_{vc}$ 比较小，模型的复杂度就降下来了，就解决了overfitting的问题。当然如果 $\\lambda$ 太大，使得过多的w被舍弃，那么就会出现 underfitting的情况了</p>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"4-General-Regularizers\"><a href=\"#4-General-Regularizers\" class=\"headerlink\" title=\"4. General Regularizers\"></a>4. General Regularizers</h2><p>Regularizers主要有 L0, L1 和L2，前面用到的是 L2的Regularizer<br>关于范数的概念老师课上没有讲的很明白，可以参考这两篇博客，后续我也会写关于范数的博客</p>\n<blockquote>\n<p>@ TODO L0、L1与L2范数</p>\n</blockquote>\n<p><a href=\"http://blog.csdn.net/zouxy09/article/details/24971995\" target=\"_blank\" rel=\"external\">机器学习中的范数规则化之（一）L0、L1与L2范数</a></p>\n<p><a href=\"http://blog.csdn.net/zouxy09/article/details/24972869\" target=\"_blank\" rel=\"external\">机器学习中的范数规则化之（二）核范数与规则项参数选择</a></p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>这一节首先介绍了Regularization：在Hypothesis Set的基础上 加入 Regularizer 作为 Penality。使得最终结果中的部分w被惩罚掉而不被考虑，从而实现了降低模型复杂度的功能。</li>\n<li>接着我们讨论了为什么 Regularization 能让模型复杂度降低，并且模型的泛化能力更强</li>\n<li>最后我们讨论了常用的范数:L0 L1 L2 范数</li>\n</ol>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\14\\14 - 1 - Regularized Hypothesis Set (19-16)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\14\\14 - 2 - Weight Decay Regularization (24-08)</p>\n<p>[3] 机器学习基石(台湾大学-林轩田)\\14\\14 - 3 - Regularization and VC Theory (08-15)</p>\n<h2 id=\"-5\"><a href=\"#-5\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"14.How can Machine Learn Better? - Three Learning Principles","date":"2017-10-21T03:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# How can Machine Learn Better? - Three Learning Principles\n\n> 这节课， 主要是介绍提高机器学习性能三个实用方法 Occam’s Razor, Sampling Bias, Data Snooping。\n\n## 1. Occam’s Razor\n奥卡姆剃刀定律（Occam’s Razor），这个原理称为“如无必要，勿增实体”（Entities must not be multiplied unnecessarily），就像剃刀一样，将不必要的部分去除掉。将奥卡姆剃刀定律应用在机器学习上意思是使用的模型尽可能的简单。\n\n如下面图一的例子\n\n![Occam's Razor for Learning](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/e2023177a2fc2da3d14e0c799981c3525217a2bd/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter14-1%20Occam's%20Razor%20for%20Learning.png)\n<center> 图一 Occam's Razor for Learning <sup>[1]</sup></center>\n\n\n上图就是一个模型选择的例子，左边的模型很简单，可能有分错的情况；而右边的模型非常复杂，所有的训练样本都分类正确。但是，我们会选择左边的模型，它更简单，符合人类直觉的解释方式。\n这样的结果带来两个问题：\n- 什么模型称得上是简单的？\n- 为什么简单模型比复杂模型要好？\n\n因为机器学习的本质在于通过学习，寻找数据的规则，从而投入到实践中。要在一堆没有规律的数据中找到一个规律既能反映数据的本质规律，又能完全没有错误是非常非常困难的（甚至是不可能的），所以我们要做的并不是要100%完美的在训练数据中找到规律，让 $E_{in}$ 为0，而是应该找到数据本身符合的规律。加上噪音的无处不在，所以在合适的规律的模型中，出现一定的错误是很正常的。\n\n所以在运用模型时，能使用简单的模型，就用简单的模型。简单而又实用的才是最好的！\n\n\n<br><br>\n----------------------------------\n\n## 2. Sampling Bias\n课上举了1948年美国选举，2个候选人A和B。一家报刊提前通过电话采访得到的结果是A会赢B，但是实际的结果却反过来。这是因为这家报刊在抽样调查的过程中只对某一阶层的人进行采访，所以导致了这个错误。\n\n这个例子给我们的教训就是，如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。\n\n\n<br><br>\n----------------------------------\n\n## 3. Data Snooping\n课上举的例子是偷窥数据造成对结果的影响。很显然，我们不应该提前去偷窥数据，但是这个实际上又是不可能避免的，因为我们在开始之前就会根据以往的经验选择性的偏向于某一类模型去处理问题，或者避免再用某一些模型去学习。\n\n有2个方法可能尽可能减少数据偷窥\n1. “看不见”数据。当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。\n2. 保持怀疑。不要对别人的说法信以为真，要通过自己的研究与测试来进行模型选择，这样得到比较正确的结论。\n\n\n\n<br><br>\n----------------------------------\n\n# Summary\n1. 首先介绍了Occam’s Razor - 越简单而有效的模型越好！\n2. 然后说明了Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的\n3. 最后说明了Data Snooping带来的坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\16\\16 - 1 - Occam-'s Razor (10-08)\n\n<br><br>\n----------------------------------\n","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-21-14.How can Machine Learn Better - Three Learning Principles.md","raw":"---\ntitle: 14.How can Machine Learn Better? - Three Learning Principles\ndate: 2017-10-21 11:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# How can Machine Learn Better? - Three Learning Principles\n\n> 这节课， 主要是介绍提高机器学习性能三个实用方法 Occam’s Razor, Sampling Bias, Data Snooping。\n\n## 1. Occam’s Razor\n奥卡姆剃刀定律（Occam’s Razor），这个原理称为“如无必要，勿增实体”（Entities must not be multiplied unnecessarily），就像剃刀一样，将不必要的部分去除掉。将奥卡姆剃刀定律应用在机器学习上意思是使用的模型尽可能的简单。\n\n如下面图一的例子\n\n![Occam's Razor for Learning](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/e2023177a2fc2da3d14e0c799981c3525217a2bd/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter14-1%20Occam's%20Razor%20for%20Learning.png)\n<center> 图一 Occam's Razor for Learning <sup>[1]</sup></center>\n\n\n上图就是一个模型选择的例子，左边的模型很简单，可能有分错的情况；而右边的模型非常复杂，所有的训练样本都分类正确。但是，我们会选择左边的模型，它更简单，符合人类直觉的解释方式。\n这样的结果带来两个问题：\n- 什么模型称得上是简单的？\n- 为什么简单模型比复杂模型要好？\n\n因为机器学习的本质在于通过学习，寻找数据的规则，从而投入到实践中。要在一堆没有规律的数据中找到一个规律既能反映数据的本质规律，又能完全没有错误是非常非常困难的（甚至是不可能的），所以我们要做的并不是要100%完美的在训练数据中找到规律，让 $E_{in}$ 为0，而是应该找到数据本身符合的规律。加上噪音的无处不在，所以在合适的规律的模型中，出现一定的错误是很正常的。\n\n所以在运用模型时，能使用简单的模型，就用简单的模型。简单而又实用的才是最好的！\n\n\n<br><br>\n----------------------------------\n\n## 2. Sampling Bias\n课上举了1948年美国选举，2个候选人A和B。一家报刊提前通过电话采访得到的结果是A会赢B，但是实际的结果却反过来。这是因为这家报刊在抽样调查的过程中只对某一阶层的人进行采访，所以导致了这个错误。\n\n这个例子给我们的教训就是，如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。\n\n\n<br><br>\n----------------------------------\n\n## 3. Data Snooping\n课上举的例子是偷窥数据造成对结果的影响。很显然，我们不应该提前去偷窥数据，但是这个实际上又是不可能避免的，因为我们在开始之前就会根据以往的经验选择性的偏向于某一类模型去处理问题，或者避免再用某一些模型去学习。\n\n有2个方法可能尽可能减少数据偷窥\n1. “看不见”数据。当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。\n2. 保持怀疑。不要对别人的说法信以为真，要通过自己的研究与测试来进行模型选择，这样得到比较正确的结论。\n\n\n\n<br><br>\n----------------------------------\n\n# Summary\n1. 首先介绍了Occam’s Razor - 越简单而有效的模型越好！\n2. 然后说明了Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的\n3. 最后说明了Data Snooping带来的坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\16\\16 - 1 - Occam-'s Razor (10-08)\n\n<br><br>\n----------------------------------\n","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-21-14.How can Machine Learn Better - Three Learning Principles","published":1,"updated":"2018-04-14T19:42:06.509Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2ep0023rwtj18g0l1d0","content":"<h1 id=\"How-can-Machine-Learn-Better-Three-Learning-Principles\"><a href=\"#How-can-Machine-Learn-Better-Three-Learning-Principles\" class=\"headerlink\" title=\"How can Machine Learn Better? - Three Learning Principles\"></a>How can Machine Learn Better? - Three Learning Principles</h1><blockquote>\n<p>这节课， 主要是介绍提高机器学习性能三个实用方法 Occam’s Razor, Sampling Bias, Data Snooping。</p>\n</blockquote>\n<h2 id=\"1-Occam’s-Razor\"><a href=\"#1-Occam’s-Razor\" class=\"headerlink\" title=\"1. Occam’s Razor\"></a>1. Occam’s Razor</h2><p>奥卡姆剃刀定律（Occam’s Razor），这个原理称为“如无必要，勿增实体”（Entities must not be multiplied unnecessarily），就像剃刀一样，将不必要的部分去除掉。将奥卡姆剃刀定律应用在机器学习上意思是使用的模型尽可能的简单。</p>\n<p>如下面图一的例子</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/e2023177a2fc2da3d14e0c799981c3525217a2bd/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter14-1%20Occam&#39;s%20Razor%20for%20Learning.png\" alt=\"Occam&#39;s Razor for Learning\"></p>\n<center> 图一 Occam's Razor for Learning <sup>[1]</sup></center>\n\n\n<p>上图就是一个模型选择的例子，左边的模型很简单，可能有分错的情况；而右边的模型非常复杂，所有的训练样本都分类正确。但是，我们会选择左边的模型，它更简单，符合人类直觉的解释方式。<br>这样的结果带来两个问题：</p>\n<ul>\n<li>什么模型称得上是简单的？</li>\n<li>为什么简单模型比复杂模型要好？</li>\n</ul>\n<p>因为机器学习的本质在于通过学习，寻找数据的规则，从而投入到实践中。要在一堆没有规律的数据中找到一个规律既能反映数据的本质规律，又能完全没有错误是非常非常困难的（甚至是不可能的），所以我们要做的并不是要100%完美的在训练数据中找到规律，让 $E_{in}$ 为0，而是应该找到数据本身符合的规律。加上噪音的无处不在，所以在合适的规律的模型中，出现一定的错误是很正常的。</p>\n<p>所以在运用模型时，能使用简单的模型，就用简单的模型。简单而又实用的才是最好的！</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Sampling-Bias\"><a href=\"#2-Sampling-Bias\" class=\"headerlink\" title=\"2. Sampling Bias\"></a>2. Sampling Bias</h2><p>课上举了1948年美国选举，2个候选人A和B。一家报刊提前通过电话采访得到的结果是A会赢B，但是实际的结果却反过来。这是因为这家报刊在抽样调查的过程中只对某一阶层的人进行采访，所以导致了这个错误。</p>\n<p>这个例子给我们的教训就是，如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Data-Snooping\"><a href=\"#3-Data-Snooping\" class=\"headerlink\" title=\"3. Data Snooping\"></a>3. Data Snooping</h2><p>课上举的例子是偷窥数据造成对结果的影响。很显然，我们不应该提前去偷窥数据，但是这个实际上又是不可能避免的，因为我们在开始之前就会根据以往的经验选择性的偏向于某一类模型去处理问题，或者避免再用某一些模型去学习。</p>\n<p>有2个方法可能尽可能减少数据偷窥</p>\n<ol>\n<li>“看不见”数据。当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。</li>\n<li>保持怀疑。不要对别人的说法信以为真，要通过自己的研究与测试来进行模型选择，这样得到比较正确的结论。</li>\n</ol>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了Occam’s Razor - 越简单而有效的模型越好！</li>\n<li>然后说明了Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的</li>\n<li>最后说明了Data Snooping带来的坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度</li>\n</ol>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\16\\16 - 1 - Occam-‘s Razor (10-08)</p>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Better-Three-Learning-Principles\"><a href=\"#How-can-Machine-Learn-Better-Three-Learning-Principles\" class=\"headerlink\" title=\"How can Machine Learn Better? - Three Learning Principles\"></a>How can Machine Learn Better? - Three Learning Principles</h1><blockquote>\n<p>这节课， 主要是介绍提高机器学习性能三个实用方法 Occam’s Razor, Sampling Bias, Data Snooping。</p>\n</blockquote>\n<h2 id=\"1-Occam’s-Razor\"><a href=\"#1-Occam’s-Razor\" class=\"headerlink\" title=\"1. Occam’s Razor\"></a>1. Occam’s Razor</h2><p>奥卡姆剃刀定律（Occam’s Razor），这个原理称为“如无必要，勿增实体”（Entities must not be multiplied unnecessarily），就像剃刀一样，将不必要的部分去除掉。将奥卡姆剃刀定律应用在机器学习上意思是使用的模型尽可能的简单。</p>\n<p>如下面图一的例子</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/e2023177a2fc2da3d14e0c799981c3525217a2bd/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter14-1%20Occam&#39;s%20Razor%20for%20Learning.png\" alt=\"Occam&#39;s Razor for Learning\"></p>\n<center> 图一 Occam's Razor for Learning <sup>[1]</sup></center>\n\n\n<p>上图就是一个模型选择的例子，左边的模型很简单，可能有分错的情况；而右边的模型非常复杂，所有的训练样本都分类正确。但是，我们会选择左边的模型，它更简单，符合人类直觉的解释方式。<br>这样的结果带来两个问题：</p>\n<ul>\n<li>什么模型称得上是简单的？</li>\n<li>为什么简单模型比复杂模型要好？</li>\n</ul>\n<p>因为机器学习的本质在于通过学习，寻找数据的规则，从而投入到实践中。要在一堆没有规律的数据中找到一个规律既能反映数据的本质规律，又能完全没有错误是非常非常困难的（甚至是不可能的），所以我们要做的并不是要100%完美的在训练数据中找到规律，让 $E_{in}$ 为0，而是应该找到数据本身符合的规律。加上噪音的无处不在，所以在合适的规律的模型中，出现一定的错误是很正常的。</p>\n<p>所以在运用模型时，能使用简单的模型，就用简单的模型。简单而又实用的才是最好的！</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Sampling-Bias\"><a href=\"#2-Sampling-Bias\" class=\"headerlink\" title=\"2. Sampling Bias\"></a>2. Sampling Bias</h2><p>课上举了1948年美国选举，2个候选人A和B。一家报刊提前通过电话采访得到的结果是A会赢B，但是实际的结果却反过来。这是因为这家报刊在抽样调查的过程中只对某一阶层的人进行采访，所以导致了这个错误。</p>\n<p>这个例子给我们的教训就是，如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"3-Data-Snooping\"><a href=\"#3-Data-Snooping\" class=\"headerlink\" title=\"3. Data Snooping\"></a>3. Data Snooping</h2><p>课上举的例子是偷窥数据造成对结果的影响。很显然，我们不应该提前去偷窥数据，但是这个实际上又是不可能避免的，因为我们在开始之前就会根据以往的经验选择性的偏向于某一类模型去处理问题，或者避免再用某一些模型去学习。</p>\n<p>有2个方法可能尽可能减少数据偷窥</p>\n<ol>\n<li>“看不见”数据。当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。</li>\n<li>保持怀疑。不要对别人的说法信以为真，要通过自己的研究与测试来进行模型选择，这样得到比较正确的结论。</li>\n</ol>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>首先介绍了Occam’s Razor - 越简单而有效的模型越好！</li>\n<li>然后说明了Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的</li>\n<li>最后说明了Data Snooping带来的坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度</li>\n</ol>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\16\\16 - 1 - Occam-‘s Razor (10-08)</p>\n<h2 id=\"-4\"><a href=\"#-4\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"15.Summary - Power of Three","date":"2017-10-22T04:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Summary - Power of Three\n\n> 总结整个课程，发现很多内容数量刚好都是三。\n\n## 1. Three Related Fields\n对比三个相关的领域:\n- Data Mining\n- Artificial Intelligence\n- Statistic\n\n\n机器学习是学习问题，而不是优化问题，也就是说，机器学习不仅要求数据在训练集上求得一个较小的误差，而且在测试集上也要表现的好（因为模型最终是要部署在实际的场景中，数据也是没有训练过的），即机器学习既要低误差，又要很好地泛化能力，以保证实际的误差与训练误差相差不大。\n\n### 1) Machine Learning V.S. Data Mining\n机器学习与数据挖掘都叫知识发现（KDD Knowledge Discovery in Dataset）。\n- 两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。\n- 两者是互助的：ML需要大数据的支持才能保持能“学到东西”。\n- 数据挖掘更关注于从大量的数据中的计算问题。\n总的来时，两者密不可分。\n\n### 2) Machine Learning V.S. Artificial Intelligence\nAI是通过特定的方法让机器能做出Intelligent的行为，ML属于AI的一个分支，是AI实现的一种方式\n\n### 3) Machine Learning V.S. Statistic\n统计是通过对已知数据的处理，从而推断出未知的事件的属性\n所以统计学是实现ML的一种方法，统计学里面有许多实用的工具可以用于证明ML。\n\n\n------------------------------------------\n<br>\n<br>\n\n\n## 2. Three Theoretical Bounds\n三个理论基础是保证了机器在满足数据量足够大，且有合适的算法的情况下，可以实现机器学习。\n三个理论基础如下：\n- Hoeffding Inequity（单一假设确认时使用）\n- Multi-Bin Hoffding Inequity（有限多个假设验证时使用）\n- VC Bound（无限多个假设训练时使用）\n\n------------------------------------------\n<br>\n<br>\n\n\n## 3. Three Linear Models\n前面我们讨论的Linear Model 有:\n- Linear Classification (PLA, Pocket)\n- Linear Regression\n- Logistic Regression\n\n具体如图一所示\n\n![Three Linear Models](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d3c8fe5c2f9329f339cd5577a6da9be9c5afacdb/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-1%20Three%20Linear%20Models.png)\n<center> 图一 Three Linear Models <sup>[1]</sup></center>\n\n------------------------------------------\n<br>\n<br>\n\n## 4. Three Key Tools\n3个重要的工具如：\n- Feature Transform - 遇到太复杂的模型，可以映射到线性的空间去做处理 (Nonlinear Transform)\n- Regularization - 通过加入惩罚项，来降低模型的复杂度 (Ridge Regression)\n- Validation - 通过拿出部分数据来作为验证集，用于评估模型，方法（Leave-One-Out Cross Validation, V-Fold Cross Validation\n\n具体如图二所示\n\n![Three Key Tools](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/895d348fc9bd1a9a08b11926ca181de1fa8cfcde/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-2%20Three%20Key%20Tools.png)\n<center> 图二 Three Key Tools <sup>[1]</sup></center>\n\n------------------------------------------\n<br>\n<br>\n\n## 5. Three Learning Principles\n- Occam’s Razor - 越简单而有效的模型越好！\n- Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的\n- Data Snooping坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度\n\n\n------------------------------------------\n<br>\n<br>\n\n## 6. Three Future Directions\n未来机器学习的方向也分为三种：\n- More Transform - 转换也能使得模型更加简单\n- More Regularization - 尽可能降低模型的复杂度\n- Less Label - 更少的Feature，那么模型将更好\n\n具体如图三所示\n\n![Three Key Tools](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cf0f34b806ade1492c029bfefc6ccbaf4dfc42c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-3%20Three%20Future%20Directions.png)\n<center> 图三 Three Future Directions <sup>[1]</sup></center>\n\n\n<br>\n<br>\n------------------------------------------\n\n\n------------------------------------------\n<br>\n<br>\n\n\n## Summary\n1. 总结整个课程\n\n至此，Machine Learning Foundation （机器学习基石）的笔记总结完毕，有部分内容后续补充\n\n<br><br>\n----------------------------------\n\n## Reference\n[1] 机器学习基石(台湾大学-林轩田)\\16\\16 - 4 - Power of Three (08-49)\n","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-22-15.Summary - Power of Three.md","raw":"---\ntitle: 15.Summary - Power of Three\ndate: 2017-10-22 12:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Summary - Power of Three\n\n> 总结整个课程，发现很多内容数量刚好都是三。\n\n## 1. Three Related Fields\n对比三个相关的领域:\n- Data Mining\n- Artificial Intelligence\n- Statistic\n\n\n机器学习是学习问题，而不是优化问题，也就是说，机器学习不仅要求数据在训练集上求得一个较小的误差，而且在测试集上也要表现的好（因为模型最终是要部署在实际的场景中，数据也是没有训练过的），即机器学习既要低误差，又要很好地泛化能力，以保证实际的误差与训练误差相差不大。\n\n### 1) Machine Learning V.S. Data Mining\n机器学习与数据挖掘都叫知识发现（KDD Knowledge Discovery in Dataset）。\n- 两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。\n- 两者是互助的：ML需要大数据的支持才能保持能“学到东西”。\n- 数据挖掘更关注于从大量的数据中的计算问题。\n总的来时，两者密不可分。\n\n### 2) Machine Learning V.S. Artificial Intelligence\nAI是通过特定的方法让机器能做出Intelligent的行为，ML属于AI的一个分支，是AI实现的一种方式\n\n### 3) Machine Learning V.S. Statistic\n统计是通过对已知数据的处理，从而推断出未知的事件的属性\n所以统计学是实现ML的一种方法，统计学里面有许多实用的工具可以用于证明ML。\n\n\n------------------------------------------\n<br>\n<br>\n\n\n## 2. Three Theoretical Bounds\n三个理论基础是保证了机器在满足数据量足够大，且有合适的算法的情况下，可以实现机器学习。\n三个理论基础如下：\n- Hoeffding Inequity（单一假设确认时使用）\n- Multi-Bin Hoffding Inequity（有限多个假设验证时使用）\n- VC Bound（无限多个假设训练时使用）\n\n------------------------------------------\n<br>\n<br>\n\n\n## 3. Three Linear Models\n前面我们讨论的Linear Model 有:\n- Linear Classification (PLA, Pocket)\n- Linear Regression\n- Logistic Regression\n\n具体如图一所示\n\n![Three Linear Models](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d3c8fe5c2f9329f339cd5577a6da9be9c5afacdb/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-1%20Three%20Linear%20Models.png)\n<center> 图一 Three Linear Models <sup>[1]</sup></center>\n\n------------------------------------------\n<br>\n<br>\n\n## 4. Three Key Tools\n3个重要的工具如：\n- Feature Transform - 遇到太复杂的模型，可以映射到线性的空间去做处理 (Nonlinear Transform)\n- Regularization - 通过加入惩罚项，来降低模型的复杂度 (Ridge Regression)\n- Validation - 通过拿出部分数据来作为验证集，用于评估模型，方法（Leave-One-Out Cross Validation, V-Fold Cross Validation\n\n具体如图二所示\n\n![Three Key Tools](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/895d348fc9bd1a9a08b11926ca181de1fa8cfcde/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-2%20Three%20Key%20Tools.png)\n<center> 图二 Three Key Tools <sup>[1]</sup></center>\n\n------------------------------------------\n<br>\n<br>\n\n## 5. Three Learning Principles\n- Occam’s Razor - 越简单而有效的模型越好！\n- Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的\n- Data Snooping坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度\n\n\n------------------------------------------\n<br>\n<br>\n\n## 6. Three Future Directions\n未来机器学习的方向也分为三种：\n- More Transform - 转换也能使得模型更加简单\n- More Regularization - 尽可能降低模型的复杂度\n- Less Label - 更少的Feature，那么模型将更好\n\n具体如图三所示\n\n![Three Key Tools](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cf0f34b806ade1492c029bfefc6ccbaf4dfc42c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-3%20Three%20Future%20Directions.png)\n<center> 图三 Three Future Directions <sup>[1]</sup></center>\n\n\n<br>\n<br>\n------------------------------------------\n\n\n------------------------------------------\n<br>\n<br>\n\n\n## Summary\n1. 总结整个课程\n\n至此，Machine Learning Foundation （机器学习基石）的笔记总结完毕，有部分内容后续补充\n\n<br><br>\n----------------------------------\n\n## Reference\n[1] 机器学习基石(台湾大学-林轩田)\\16\\16 - 4 - Power of Three (08-49)\n","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-22-15.Summary - Power of Three","published":1,"updated":"2018-04-14T19:42:06.509Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2er0027rwtj7b6hskyz","content":"<h1 id=\"Summary-Power-of-Three\"><a href=\"#Summary-Power-of-Three\" class=\"headerlink\" title=\"Summary - Power of Three\"></a>Summary - Power of Three</h1><blockquote>\n<p>总结整个课程，发现很多内容数量刚好都是三。</p>\n</blockquote>\n<h2 id=\"1-Three-Related-Fields\"><a href=\"#1-Three-Related-Fields\" class=\"headerlink\" title=\"1. Three Related Fields\"></a>1. Three Related Fields</h2><p>对比三个相关的领域:</p>\n<ul>\n<li>Data Mining</li>\n<li>Artificial Intelligence</li>\n<li>Statistic</li>\n</ul>\n<p>机器学习是学习问题，而不是优化问题，也就是说，机器学习不仅要求数据在训练集上求得一个较小的误差，而且在测试集上也要表现的好（因为模型最终是要部署在实际的场景中，数据也是没有训练过的），即机器学习既要低误差，又要很好地泛化能力，以保证实际的误差与训练误差相差不大。</p>\n<h3 id=\"1-Machine-Learning-V-S-Data-Mining\"><a href=\"#1-Machine-Learning-V-S-Data-Mining\" class=\"headerlink\" title=\"1) Machine Learning V.S. Data Mining\"></a>1) Machine Learning V.S. Data Mining</h3><p>机器学习与数据挖掘都叫知识发现（KDD Knowledge Discovery in Dataset）。</p>\n<ul>\n<li>两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。</li>\n<li>两者是互助的：ML需要大数据的支持才能保持能“学到东西”。</li>\n<li>数据挖掘更关注于从大量的数据中的计算问题。<br>总的来时，两者密不可分。</li>\n</ul>\n<h3 id=\"2-Machine-Learning-V-S-Artificial-Intelligence\"><a href=\"#2-Machine-Learning-V-S-Artificial-Intelligence\" class=\"headerlink\" title=\"2) Machine Learning V.S. Artificial Intelligence\"></a>2) Machine Learning V.S. Artificial Intelligence</h3><p>AI是通过特定的方法让机器能做出Intelligent的行为，ML属于AI的一个分支，是AI实现的一种方式</p>\n<h3 id=\"3-Machine-Learning-V-S-Statistic\"><a href=\"#3-Machine-Learning-V-S-Statistic\" class=\"headerlink\" title=\"3) Machine Learning V.S. Statistic\"></a>3) Machine Learning V.S. Statistic</h3><p>统计是通过对已知数据的处理，从而推断出未知的事件的属性<br>所以统计学是实现ML的一种方法，统计学里面有许多实用的工具可以用于证明ML。</p>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"2-Three-Theoretical-Bounds\"><a href=\"#2-Three-Theoretical-Bounds\" class=\"headerlink\" title=\"2. Three Theoretical Bounds\"></a>2. Three Theoretical Bounds</h2><p>三个理论基础是保证了机器在满足数据量足够大，且有合适的算法的情况下，可以实现机器学习。<br>三个理论基础如下：</p>\n<ul>\n<li>Hoeffding Inequity（单一假设确认时使用）</li>\n<li>Multi-Bin Hoffding Inequity（有限多个假设验证时使用）</li>\n<li>VC Bound（无限多个假设训练时使用）</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"3-Three-Linear-Models\"><a href=\"#3-Three-Linear-Models\" class=\"headerlink\" title=\"3. Three Linear Models\"></a>3. Three Linear Models</h2><p>前面我们讨论的Linear Model 有:</p>\n<ul>\n<li>Linear Classification (PLA, Pocket)</li>\n<li>Linear Regression</li>\n<li>Logistic Regression</li>\n</ul>\n<p>具体如图一所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d3c8fe5c2f9329f339cd5577a6da9be9c5afacdb/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-1%20Three%20Linear%20Models.png\" alt=\"Three Linear Models\"></p>\n<center> 图一 Three Linear Models <sup>[1]</sup></center>\n\n<hr>\n<p><br><br><br></p>\n<h2 id=\"4-Three-Key-Tools\"><a href=\"#4-Three-Key-Tools\" class=\"headerlink\" title=\"4. Three Key Tools\"></a>4. Three Key Tools</h2><p>3个重要的工具如：</p>\n<ul>\n<li>Feature Transform - 遇到太复杂的模型，可以映射到线性的空间去做处理 (Nonlinear Transform)</li>\n<li>Regularization - 通过加入惩罚项，来降低模型的复杂度 (Ridge Regression)</li>\n<li>Validation - 通过拿出部分数据来作为验证集，用于评估模型，方法（Leave-One-Out Cross Validation, V-Fold Cross Validation</li>\n</ul>\n<p>具体如图二所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/895d348fc9bd1a9a08b11926ca181de1fa8cfcde/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-2%20Three%20Key%20Tools.png\" alt=\"Three Key Tools\"></p>\n<center> 图二 Three Key Tools <sup>[1]</sup></center>\n\n<hr>\n<p><br><br><br></p>\n<h2 id=\"5-Three-Learning-Principles\"><a href=\"#5-Three-Learning-Principles\" class=\"headerlink\" title=\"5. Three Learning Principles\"></a>5. Three Learning Principles</h2><ul>\n<li>Occam’s Razor - 越简单而有效的模型越好！</li>\n<li>Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的</li>\n<li>Data Snooping坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"6-Three-Future-Directions\"><a href=\"#6-Three-Future-Directions\" class=\"headerlink\" title=\"6. Three Future Directions\"></a>6. Three Future Directions</h2><p>未来机器学习的方向也分为三种：</p>\n<ul>\n<li>More Transform - 转换也能使得模型更加简单</li>\n<li>More Regularization - 尽可能降低模型的复杂度</li>\n<li>Less Label - 更少的Feature，那么模型将更好</li>\n</ul>\n<p>具体如图三所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cf0f34b806ade1492c029bfefc6ccbaf4dfc42c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-3%20Three%20Future%20Directions.png\" alt=\"Three Key Tools\"></p>\n<center> 图三 Three Future Directions <sup>[1]</sup></center>\n\n\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2><hr>\n<p><br><br><br></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><ol>\n<li>总结整个课程</li>\n</ol>\n<p>至此，Machine Learning Foundation （机器学习基石）的笔记总结完毕，有部分内容后续补充</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] 机器学习基石(台湾大学-林轩田)\\16\\16 - 4 - Power of Three (08-49)</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Summary-Power-of-Three\"><a href=\"#Summary-Power-of-Three\" class=\"headerlink\" title=\"Summary - Power of Three\"></a>Summary - Power of Three</h1><blockquote>\n<p>总结整个课程，发现很多内容数量刚好都是三。</p>\n</blockquote>\n<h2 id=\"1-Three-Related-Fields\"><a href=\"#1-Three-Related-Fields\" class=\"headerlink\" title=\"1. Three Related Fields\"></a>1. Three Related Fields</h2><p>对比三个相关的领域:</p>\n<ul>\n<li>Data Mining</li>\n<li>Artificial Intelligence</li>\n<li>Statistic</li>\n</ul>\n<p>机器学习是学习问题，而不是优化问题，也就是说，机器学习不仅要求数据在训练集上求得一个较小的误差，而且在测试集上也要表现的好（因为模型最终是要部署在实际的场景中，数据也是没有训练过的），即机器学习既要低误差，又要很好地泛化能力，以保证实际的误差与训练误差相差不大。</p>\n<h3 id=\"1-Machine-Learning-V-S-Data-Mining\"><a href=\"#1-Machine-Learning-V-S-Data-Mining\" class=\"headerlink\" title=\"1) Machine Learning V.S. Data Mining\"></a>1) Machine Learning V.S. Data Mining</h3><p>机器学习与数据挖掘都叫知识发现（KDD Knowledge Discovery in Dataset）。</p>\n<ul>\n<li>两者是一致的：能够找出的有用信息就是我们要求得的近似目标函数的假设。</li>\n<li>两者是互助的：ML需要大数据的支持才能保持能“学到东西”。</li>\n<li>数据挖掘更关注于从大量的数据中的计算问题。<br>总的来时，两者密不可分。</li>\n</ul>\n<h3 id=\"2-Machine-Learning-V-S-Artificial-Intelligence\"><a href=\"#2-Machine-Learning-V-S-Artificial-Intelligence\" class=\"headerlink\" title=\"2) Machine Learning V.S. Artificial Intelligence\"></a>2) Machine Learning V.S. Artificial Intelligence</h3><p>AI是通过特定的方法让机器能做出Intelligent的行为，ML属于AI的一个分支，是AI实现的一种方式</p>\n<h3 id=\"3-Machine-Learning-V-S-Statistic\"><a href=\"#3-Machine-Learning-V-S-Statistic\" class=\"headerlink\" title=\"3) Machine Learning V.S. Statistic\"></a>3) Machine Learning V.S. Statistic</h3><p>统计是通过对已知数据的处理，从而推断出未知的事件的属性<br>所以统计学是实现ML的一种方法，统计学里面有许多实用的工具可以用于证明ML。</p>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"2-Three-Theoretical-Bounds\"><a href=\"#2-Three-Theoretical-Bounds\" class=\"headerlink\" title=\"2. Three Theoretical Bounds\"></a>2. Three Theoretical Bounds</h2><p>三个理论基础是保证了机器在满足数据量足够大，且有合适的算法的情况下，可以实现机器学习。<br>三个理论基础如下：</p>\n<ul>\n<li>Hoeffding Inequity（单一假设确认时使用）</li>\n<li>Multi-Bin Hoffding Inequity（有限多个假设验证时使用）</li>\n<li>VC Bound（无限多个假设训练时使用）</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"3-Three-Linear-Models\"><a href=\"#3-Three-Linear-Models\" class=\"headerlink\" title=\"3. Three Linear Models\"></a>3. Three Linear Models</h2><p>前面我们讨论的Linear Model 有:</p>\n<ul>\n<li>Linear Classification (PLA, Pocket)</li>\n<li>Linear Regression</li>\n<li>Logistic Regression</li>\n</ul>\n<p>具体如图一所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/d3c8fe5c2f9329f339cd5577a6da9be9c5afacdb/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-1%20Three%20Linear%20Models.png\" alt=\"Three Linear Models\"></p>\n<center> 图一 Three Linear Models <sup>[1]</sup></center>\n\n<hr>\n<p><br><br><br></p>\n<h2 id=\"4-Three-Key-Tools\"><a href=\"#4-Three-Key-Tools\" class=\"headerlink\" title=\"4. Three Key Tools\"></a>4. Three Key Tools</h2><p>3个重要的工具如：</p>\n<ul>\n<li>Feature Transform - 遇到太复杂的模型，可以映射到线性的空间去做处理 (Nonlinear Transform)</li>\n<li>Regularization - 通过加入惩罚项，来降低模型的复杂度 (Ridge Regression)</li>\n<li>Validation - 通过拿出部分数据来作为验证集，用于评估模型，方法（Leave-One-Out Cross Validation, V-Fold Cross Validation</li>\n</ul>\n<p>具体如图二所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/895d348fc9bd1a9a08b11926ca181de1fa8cfcde/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-2%20Three%20Key%20Tools.png\" alt=\"Three Key Tools\"></p>\n<center> 图二 Three Key Tools <sup>[1]</sup></center>\n\n<hr>\n<p><br><br><br></p>\n<h2 id=\"5-Three-Learning-Principles\"><a href=\"#5-Three-Learning-Principles\" class=\"headerlink\" title=\"5. Three Learning Principles\"></a>5. Three Learning Principles</h2><ul>\n<li>Occam’s Razor - 越简单而有效的模型越好！</li>\n<li>Sampling Bias的坏处 - 我们在训练时要保证数据的来源，最好是相互独立的</li>\n<li>Data Snooping坏处 - 尽量先选择模型，然后在去查看数据，然后在训练的过程要保持怀疑的态度</li>\n</ul>\n<hr>\n<p><br><br><br></p>\n<h2 id=\"6-Three-Future-Directions\"><a href=\"#6-Three-Future-Directions\" class=\"headerlink\" title=\"6. Three Future Directions\"></a>6. Three Future Directions</h2><p>未来机器学习的方向也分为三种：</p>\n<ul>\n<li>More Transform - 转换也能使得模型更加简单</li>\n<li>More Regularization - 尽可能降低模型的复杂度</li>\n<li>Less Label - 更少的Feature，那么模型将更好</li>\n</ul>\n<p>具体如图三所示</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cf0f34b806ade1492c029bfefc6ccbaf4dfc42c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter15-3%20Three%20Future%20Directions.png\" alt=\"Three Key Tools\"></p>\n<center> 图三 Three Future Directions <sup>[1]</sup></center>\n\n\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2><hr>\n<p><br><br><br></p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><ol>\n<li>总结整个课程</li>\n</ol>\n<p>至此，Machine Learning Foundation （机器学习基石）的笔记总结完毕，有部分内容后续补充</p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] 机器学习基石(台湾大学-林轩田)\\16\\16 - 4 - Power of Three (08-49)</p>\n"},{"title":"11.How can Machine Learn Better? - Overfitting and Solution","date":"2017-10-16T07:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# How can Machine Learn Better? - Overfitting and Solution\n\n## 1. What is Overfitting?\n上一节最后，我们提到了如果线性模型的模型复杂度太大的话，可能会引起Overfitting。同样的很明显会有Underfitting的情况。\n> 那什么是Overfitting，Underfitting呢？\n\n首先根据名字，Overfitting：over fitting，就是在fitting的时候太over了。我们用线性模型去分类/回归处理数据的过程就是一个fitting的过程，所以也就是说我们处理过头了。同理Underfitting就是处理不够到位。\n\n> 那么什么时候才是处理过头呢？什么时候才是处理不到位呢？\n\n就是在处理相对简单的问题的时候用了相对复杂的模型去处理。\n就是在处理相对复杂的问题的时候用了相对简单的模型去处理。\n\n我们用下面的例子来进行说明\n1.首先例子如图一所示(这里用的是Ng的图，因为在林老师的ppt中没找到很好地图同时体现Underfit, good fit overfit)。左图是欠拟合(underfit)，中间的图四好的拟合（good fit），右图是过度拟合（overfit）。单纯从拟合结果来看：明显左边和中间的图 $E_{in}$ 比右图要大。但是从泛化好坏来看，显然左图和中间的图要比右图好。\n假如我们考虑good fit分类出错的点为噪音点（noise），那么Overfit的模型就会受到了严重的干扰。\n\n![Underfit, Good Fit and Overfit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/faeeb6ed9dd42d75c5b4b20d6b9a592c92ce7ece/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-3%20Underfit%20Good%20Fit%20and%20Overfit.png)\n<center> 图一 Underfit, Good Fit and Overfit <sup>[1]</sup></center>\n\n\n2. 接着我们回头看之前总结的VC Dimension 的曲线， 如图二所示。\n\n![Learning Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/17974844acd6bf87d8cf4d68731f9d4cade5b450/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-2%20Learning%20Curve%20.png)\n\n图二 Learning Curve <sup>[1]</sup>\n\n\n图中可以看到在VC Dimension变大时，$E_{in}$ 变小， 但 $E_{out}$ 先变小后变大，而过拟合和欠拟合的情况的主要区别就在于 $E_{out}$ 的变化情况，具体解释如下：\n- 过拟合（Overfitting）发生在VC Dimension较大时，$E_{in}$ 太小， 但 $E_{out}$ 太大（即VC Dimension 太大了） ，表示在训练样本上拟合做的很好，$E_{in}$ 太小，但是过度了，使得泛化能力变差， $E_{out}$ 很大\n- 欠拟合（Underfitting）发生在在VC Dimension较小时，$E_{in}$ 太大，同时 $E_{out}$ 太大（即VC Dimension 太小了），表示在训练样本上拟合做不够好，$E_{in}$ 太大，虽然泛化能力很强（即 $E_{out}$ 也太大）\n\n关于如何解决欠拟合的问题之前也讨论过： 从低到高不断地提高多项式次数，使得VC维提高，达到拟合的效果。\n但过拟合的问题更为复杂，下面会更深入的探讨。\n\n\n3. 下面的图三，可以让我们更加直观的看到overfitting造成的问题\n\n![Cases of Overfitting](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cbdf35a9838985e1f889100ce389c4d316cc0f0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-4%20Case%20of%20Overfitting.png)\n<center> 图三 Case of Overfitting <sup>[2]</sup></center>\n\n从图中可以看到，Overfitting的 $E_{in}$ 都比 good-fit的要低，但是 $E_{out}$ 却很高（泛化能力差）。\n\n\n总结起来有3个因数会导致Overfitting的发生：\n- data size N 太小\n- noise 太多\n- VC Dimension太大\n\n<br><br>\n----------------------------------\n\n\n## 2. Dealing with Overfitting\n上一节我们提出了overfitting并作了分析。总结出3个因数会导致overfitting，下面根据这3个因数，我们有5种方法帮助我们避免overfitting的发生。\n\n- 使用简单的模型(start from simple model)，逐次增加模型的复杂度 - 防止 VC Dimension太大\n- 进行数据清理/裁剪(data cleaning/pruning) - 防止 noise 太多\n- 数据提示（data hinting） - 防止 data size N 太小\n- 正则化（regularization） - 防止 VC Dimension太大\n- 确认（validation） - 提取一部分的数据作为测试集，提前估计模型的泛化强度\n\n\n下面我们分别介绍这5种方法，其中前三种方法比较简单，这里不做深入讨论，而 Regularization 和 Validation 较复杂，这里会用比较多的笔墨进行讨论。\n\n### 1) Start from Simple Model\n上一章中也提到过，由于VC Dimension太大的话，导致 $E_{in}$ 变小的同时 $E_{out}$却在变大。所以我们如果从d=1阶的模型开始debug，如果 $E_{in}$ 不符合要求，那么我们增大d为2阶，然后在进行debug，以此类推，直到 $E_{in}$ 符合我们要求位置，这个时候的 VC Dimension 不会很大，而且我们也得到泛化能力相对较强的模型。\n\n\n### 2) Data Cleaning/Pruning\nData cleaning/pruning就是对训练数据集里label有明显错误的样本进行清理（data cleaning）或者裁剪（pruning)。data cleaning/pruning关键在于如何准确寻找label错误的点或者是noise的点。而处理的方法为\n- 纠正，即数据清理（data cleaning）的方式处理该情况；\n- 删除错误样本，即数据裁剪（data pruning）的方式处理。\n\n处理措施很简单，但是发现样本是噪音或离群点却比较困难。\n\n\n### 3) Data Hinting\nData hinting是针对N不够大的情况，通过data hinting的方法就可以对已知的样本进行简单的处理、变换，从而获得更多的样本。比如说：数字分类问题，可以对已知的数字图片进行轻微的平移或者旋转，从而得到更多的数据，达到扩大训练集的目的。这种通过data hinting得到的数据叫做：virtual examples。\n\n> 需要注意的是，新获取的virtual examples可能不再是iid某个distribution。所以新构建的virtual examples要尽量合理，且是独立同分布的。\n\n### 4) Regularization\nRegularization（正规化）处理属于penalized方法的一种，通过正规化的处理来对原来的方程加上一个regularizer进行penalize，从而使得过渡复杂的模型，变得没那么复杂。\n\n关于Regularization 的讨论看此链接:\n[12. 机器学习基石-How can Machine Learn Better? - Regularization](https://zhichengmle.github.io/2017/10/17/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-17-12.How%20can%20Machine%20Learn%20Better%20-%20Regularization/)\n\n### 5) Validation\n这个是目前最常用的方法之一，通过提前把一部分的数据拿出来作为测试集，因为测试集是随机取出来的，而将来实际的应用中，数据也大体和测试集出入不大，所以用这种方法，可以提前得到实际应用的时候，模型的错误 $E_{out}$ 通过这个作为衡量模型是否合格的条件之一。\n\n关于Validation 的讨论看此链接\n\n[13. 机器学习基石-How can Machine Learn Better? - Validation](https://zhichengmle.github.io/2017/10/18/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-18-13.How%20can%20Machine%20Learn%20Better%20-%20Validation/)\n\n\n\n\n<br><br>\n----------------------------------\n\n# Summary\n1.首先介绍了Overfitting和Underfitting的概念。\n\n2.接着我们着重分析Overfitting，总结了产生Overfitting的原因：\n\n- data size N 太小\n\n- noise 太多\n\n- VC Dimension太大\n\n3.最后我们分析如何最大程度的避免Overfitting。在solution中.\n\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\13\\13 - 1 - What is Overfitting- (10-45)\n\n[2] 机器学习基石(台湾大学-林轩田)\\13\\13 - 2 - The Role of Noise and Data Size (13-36)\n\n\n<br><br>\n----------------------------------\n","source":"_posts/ReadNote-Machine Learning Foudantion -NTU/2017-10-16-11.How can Machine Learn Better - Overfitting and Solution.md","raw":"---\ntitle: 11.How can Machine Learn Better? - Overfitting and Solution\ndate: 2017-10-16 15:23:19\ncategories: [ReadNote]\ntags: [ReadNote-Machine-Learning-Foundation]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# How can Machine Learn Better? - Overfitting and Solution\n\n## 1. What is Overfitting?\n上一节最后，我们提到了如果线性模型的模型复杂度太大的话，可能会引起Overfitting。同样的很明显会有Underfitting的情况。\n> 那什么是Overfitting，Underfitting呢？\n\n首先根据名字，Overfitting：over fitting，就是在fitting的时候太over了。我们用线性模型去分类/回归处理数据的过程就是一个fitting的过程，所以也就是说我们处理过头了。同理Underfitting就是处理不够到位。\n\n> 那么什么时候才是处理过头呢？什么时候才是处理不到位呢？\n\n就是在处理相对简单的问题的时候用了相对复杂的模型去处理。\n就是在处理相对复杂的问题的时候用了相对简单的模型去处理。\n\n我们用下面的例子来进行说明\n1.首先例子如图一所示(这里用的是Ng的图，因为在林老师的ppt中没找到很好地图同时体现Underfit, good fit overfit)。左图是欠拟合(underfit)，中间的图四好的拟合（good fit），右图是过度拟合（overfit）。单纯从拟合结果来看：明显左边和中间的图 $E_{in}$ 比右图要大。但是从泛化好坏来看，显然左图和中间的图要比右图好。\n假如我们考虑good fit分类出错的点为噪音点（noise），那么Overfit的模型就会受到了严重的干扰。\n\n![Underfit, Good Fit and Overfit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/faeeb6ed9dd42d75c5b4b20d6b9a592c92ce7ece/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-3%20Underfit%20Good%20Fit%20and%20Overfit.png)\n<center> 图一 Underfit, Good Fit and Overfit <sup>[1]</sup></center>\n\n\n2. 接着我们回头看之前总结的VC Dimension 的曲线， 如图二所示。\n\n![Learning Curve](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/17974844acd6bf87d8cf4d68731f9d4cade5b450/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-2%20Learning%20Curve%20.png)\n\n图二 Learning Curve <sup>[1]</sup>\n\n\n图中可以看到在VC Dimension变大时，$E_{in}$ 变小， 但 $E_{out}$ 先变小后变大，而过拟合和欠拟合的情况的主要区别就在于 $E_{out}$ 的变化情况，具体解释如下：\n- 过拟合（Overfitting）发生在VC Dimension较大时，$E_{in}$ 太小， 但 $E_{out}$ 太大（即VC Dimension 太大了） ，表示在训练样本上拟合做的很好，$E_{in}$ 太小，但是过度了，使得泛化能力变差， $E_{out}$ 很大\n- 欠拟合（Underfitting）发生在在VC Dimension较小时，$E_{in}$ 太大，同时 $E_{out}$ 太大（即VC Dimension 太小了），表示在训练样本上拟合做不够好，$E_{in}$ 太大，虽然泛化能力很强（即 $E_{out}$ 也太大）\n\n关于如何解决欠拟合的问题之前也讨论过： 从低到高不断地提高多项式次数，使得VC维提高，达到拟合的效果。\n但过拟合的问题更为复杂，下面会更深入的探讨。\n\n\n3. 下面的图三，可以让我们更加直观的看到overfitting造成的问题\n\n![Cases of Overfitting](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cbdf35a9838985e1f889100ce389c4d316cc0f0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-4%20Case%20of%20Overfitting.png)\n<center> 图三 Case of Overfitting <sup>[2]</sup></center>\n\n从图中可以看到，Overfitting的 $E_{in}$ 都比 good-fit的要低，但是 $E_{out}$ 却很高（泛化能力差）。\n\n\n总结起来有3个因数会导致Overfitting的发生：\n- data size N 太小\n- noise 太多\n- VC Dimension太大\n\n<br><br>\n----------------------------------\n\n\n## 2. Dealing with Overfitting\n上一节我们提出了overfitting并作了分析。总结出3个因数会导致overfitting，下面根据这3个因数，我们有5种方法帮助我们避免overfitting的发生。\n\n- 使用简单的模型(start from simple model)，逐次增加模型的复杂度 - 防止 VC Dimension太大\n- 进行数据清理/裁剪(data cleaning/pruning) - 防止 noise 太多\n- 数据提示（data hinting） - 防止 data size N 太小\n- 正则化（regularization） - 防止 VC Dimension太大\n- 确认（validation） - 提取一部分的数据作为测试集，提前估计模型的泛化强度\n\n\n下面我们分别介绍这5种方法，其中前三种方法比较简单，这里不做深入讨论，而 Regularization 和 Validation 较复杂，这里会用比较多的笔墨进行讨论。\n\n### 1) Start from Simple Model\n上一章中也提到过，由于VC Dimension太大的话，导致 $E_{in}$ 变小的同时 $E_{out}$却在变大。所以我们如果从d=1阶的模型开始debug，如果 $E_{in}$ 不符合要求，那么我们增大d为2阶，然后在进行debug，以此类推，直到 $E_{in}$ 符合我们要求位置，这个时候的 VC Dimension 不会很大，而且我们也得到泛化能力相对较强的模型。\n\n\n### 2) Data Cleaning/Pruning\nData cleaning/pruning就是对训练数据集里label有明显错误的样本进行清理（data cleaning）或者裁剪（pruning)。data cleaning/pruning关键在于如何准确寻找label错误的点或者是noise的点。而处理的方法为\n- 纠正，即数据清理（data cleaning）的方式处理该情况；\n- 删除错误样本，即数据裁剪（data pruning）的方式处理。\n\n处理措施很简单，但是发现样本是噪音或离群点却比较困难。\n\n\n### 3) Data Hinting\nData hinting是针对N不够大的情况，通过data hinting的方法就可以对已知的样本进行简单的处理、变换，从而获得更多的样本。比如说：数字分类问题，可以对已知的数字图片进行轻微的平移或者旋转，从而得到更多的数据，达到扩大训练集的目的。这种通过data hinting得到的数据叫做：virtual examples。\n\n> 需要注意的是，新获取的virtual examples可能不再是iid某个distribution。所以新构建的virtual examples要尽量合理，且是独立同分布的。\n\n### 4) Regularization\nRegularization（正规化）处理属于penalized方法的一种，通过正规化的处理来对原来的方程加上一个regularizer进行penalize，从而使得过渡复杂的模型，变得没那么复杂。\n\n关于Regularization 的讨论看此链接:\n[12. 机器学习基石-How can Machine Learn Better? - Regularization](https://zhichengmle.github.io/2017/10/17/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-17-12.How%20can%20Machine%20Learn%20Better%20-%20Regularization/)\n\n### 5) Validation\n这个是目前最常用的方法之一，通过提前把一部分的数据拿出来作为测试集，因为测试集是随机取出来的，而将来实际的应用中，数据也大体和测试集出入不大，所以用这种方法，可以提前得到实际应用的时候，模型的错误 $E_{out}$ 通过这个作为衡量模型是否合格的条件之一。\n\n关于Validation 的讨论看此链接\n\n[13. 机器学习基石-How can Machine Learn Better? - Validation](https://zhichengmle.github.io/2017/10/18/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-18-13.How%20can%20Machine%20Learn%20Better%20-%20Validation/)\n\n\n\n\n<br><br>\n----------------------------------\n\n# Summary\n1.首先介绍了Overfitting和Underfitting的概念。\n\n2.接着我们着重分析Overfitting，总结了产生Overfitting的原因：\n\n- data size N 太小\n\n- noise 太多\n\n- VC Dimension太大\n\n3.最后我们分析如何最大程度的避免Overfitting。在solution中.\n\n\n\n<br><br>\n----------------------------------\n\n# Reference\n[1] 机器学习基石(台湾大学-林轩田)\\13\\13 - 1 - What is Overfitting- (10-45)\n\n[2] 机器学习基石(台湾大学-林轩田)\\13\\13 - 2 - The Role of Noise and Data Size (13-36)\n\n\n<br><br>\n----------------------------------\n","slug":"ReadNote-Machine Learning Foudantion -NTU/2017-10-16-11.How can Machine Learn Better - Overfitting and Solution","published":1,"updated":"2018-04-14T19:42:06.507Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2es002arwtjm0rqsidn","content":"<h1 id=\"How-can-Machine-Learn-Better-Overfitting-and-Solution\"><a href=\"#How-can-Machine-Learn-Better-Overfitting-and-Solution\" class=\"headerlink\" title=\"How can Machine Learn Better? - Overfitting and Solution\"></a>How can Machine Learn Better? - Overfitting and Solution</h1><h2 id=\"1-What-is-Overfitting\"><a href=\"#1-What-is-Overfitting\" class=\"headerlink\" title=\"1. What is Overfitting?\"></a>1. What is Overfitting?</h2><p>上一节最后，我们提到了如果线性模型的模型复杂度太大的话，可能会引起Overfitting。同样的很明显会有Underfitting的情况。</p>\n<blockquote>\n<p>那什么是Overfitting，Underfitting呢？</p>\n</blockquote>\n<p>首先根据名字，Overfitting：over fitting，就是在fitting的时候太over了。我们用线性模型去分类/回归处理数据的过程就是一个fitting的过程，所以也就是说我们处理过头了。同理Underfitting就是处理不够到位。</p>\n<blockquote>\n<p>那么什么时候才是处理过头呢？什么时候才是处理不到位呢？</p>\n</blockquote>\n<p>就是在处理相对简单的问题的时候用了相对复杂的模型去处理。<br>就是在处理相对复杂的问题的时候用了相对简单的模型去处理。</p>\n<p>我们用下面的例子来进行说明<br>1.首先例子如图一所示(这里用的是Ng的图，因为在林老师的ppt中没找到很好地图同时体现Underfit, good fit overfit)。左图是欠拟合(underfit)，中间的图四好的拟合（good fit），右图是过度拟合（overfit）。单纯从拟合结果来看：明显左边和中间的图 $E_{in}$ 比右图要大。但是从泛化好坏来看，显然左图和中间的图要比右图好。<br>假如我们考虑good fit分类出错的点为噪音点（noise），那么Overfit的模型就会受到了严重的干扰。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/faeeb6ed9dd42d75c5b4b20d6b9a592c92ce7ece/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-3%20Underfit%20Good%20Fit%20and%20Overfit.png\" alt=\"Underfit, Good Fit and Overfit\"></p>\n<center> 图一 Underfit, Good Fit and Overfit <sup>[1]</sup></center>\n\n\n<ol>\n<li>接着我们回头看之前总结的VC Dimension 的曲线， 如图二所示。</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/17974844acd6bf87d8cf4d68731f9d4cade5b450/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-2%20Learning%20Curve%20.png\" alt=\"Learning Curve\"></p>\n<p>图二 Learning Curve <sup>[1]</sup></p>\n<p>图中可以看到在VC Dimension变大时，$E_{in}$ 变小， 但 $E_{out}$ 先变小后变大，而过拟合和欠拟合的情况的主要区别就在于 $E_{out}$ 的变化情况，具体解释如下：</p>\n<ul>\n<li>过拟合（Overfitting）发生在VC Dimension较大时，$E_{in}$ 太小， 但 $E_{out}$ 太大（即VC Dimension 太大了） ，表示在训练样本上拟合做的很好，$E_{in}$ 太小，但是过度了，使得泛化能力变差， $E_{out}$ 很大</li>\n<li>欠拟合（Underfitting）发生在在VC Dimension较小时，$E_{in}$ 太大，同时 $E_{out}$ 太大（即VC Dimension 太小了），表示在训练样本上拟合做不够好，$E_{in}$ 太大，虽然泛化能力很强（即 $E_{out}$ 也太大）</li>\n</ul>\n<p>关于如何解决欠拟合的问题之前也讨论过： 从低到高不断地提高多项式次数，使得VC维提高，达到拟合的效果。<br>但过拟合的问题更为复杂，下面会更深入的探讨。</p>\n<ol>\n<li>下面的图三，可以让我们更加直观的看到overfitting造成的问题</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cbdf35a9838985e1f889100ce389c4d316cc0f0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-4%20Case%20of%20Overfitting.png\" alt=\"Cases of Overfitting\"></p>\n<center> 图三 Case of Overfitting <sup>[2]</sup></center>\n\n<p>从图中可以看到，Overfitting的 $E_{in}$ 都比 good-fit的要低，但是 $E_{out}$ 却很高（泛化能力差）。</p>\n<p>总结起来有3个因数会导致Overfitting的发生：</p>\n<ul>\n<li>data size N 太小</li>\n<li>noise 太多</li>\n<li>VC Dimension太大</li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Dealing-with-Overfitting\"><a href=\"#2-Dealing-with-Overfitting\" class=\"headerlink\" title=\"2. Dealing with Overfitting\"></a>2. Dealing with Overfitting</h2><p>上一节我们提出了overfitting并作了分析。总结出3个因数会导致overfitting，下面根据这3个因数，我们有5种方法帮助我们避免overfitting的发生。</p>\n<ul>\n<li>使用简单的模型(start from simple model)，逐次增加模型的复杂度 - 防止 VC Dimension太大</li>\n<li>进行数据清理/裁剪(data cleaning/pruning) - 防止 noise 太多</li>\n<li>数据提示（data hinting） - 防止 data size N 太小</li>\n<li>正则化（regularization） - 防止 VC Dimension太大</li>\n<li>确认（validation） - 提取一部分的数据作为测试集，提前估计模型的泛化强度</li>\n</ul>\n<p>下面我们分别介绍这5种方法，其中前三种方法比较简单，这里不做深入讨论，而 Regularization 和 Validation 较复杂，这里会用比较多的笔墨进行讨论。</p>\n<h3 id=\"1-Start-from-Simple-Model\"><a href=\"#1-Start-from-Simple-Model\" class=\"headerlink\" title=\"1) Start from Simple Model\"></a>1) Start from Simple Model</h3><p>上一章中也提到过，由于VC Dimension太大的话，导致 $E_{in}$ 变小的同时 $E_{out}$却在变大。所以我们如果从d=1阶的模型开始debug，如果 $E_{in}$ 不符合要求，那么我们增大d为2阶，然后在进行debug，以此类推，直到 $E_{in}$ 符合我们要求位置，这个时候的 VC Dimension 不会很大，而且我们也得到泛化能力相对较强的模型。</p>\n<h3 id=\"2-Data-Cleaning-Pruning\"><a href=\"#2-Data-Cleaning-Pruning\" class=\"headerlink\" title=\"2) Data Cleaning/Pruning\"></a>2) Data Cleaning/Pruning</h3><p>Data cleaning/pruning就是对训练数据集里label有明显错误的样本进行清理（data cleaning）或者裁剪（pruning)。data cleaning/pruning关键在于如何准确寻找label错误的点或者是noise的点。而处理的方法为</p>\n<ul>\n<li>纠正，即数据清理（data cleaning）的方式处理该情况；</li>\n<li>删除错误样本，即数据裁剪（data pruning）的方式处理。</li>\n</ul>\n<p>处理措施很简单，但是发现样本是噪音或离群点却比较困难。</p>\n<h3 id=\"3-Data-Hinting\"><a href=\"#3-Data-Hinting\" class=\"headerlink\" title=\"3) Data Hinting\"></a>3) Data Hinting</h3><p>Data hinting是针对N不够大的情况，通过data hinting的方法就可以对已知的样本进行简单的处理、变换，从而获得更多的样本。比如说：数字分类问题，可以对已知的数字图片进行轻微的平移或者旋转，从而得到更多的数据，达到扩大训练集的目的。这种通过data hinting得到的数据叫做：virtual examples。</p>\n<blockquote>\n<p>需要注意的是，新获取的virtual examples可能不再是iid某个distribution。所以新构建的virtual examples要尽量合理，且是独立同分布的。</p>\n</blockquote>\n<h3 id=\"4-Regularization\"><a href=\"#4-Regularization\" class=\"headerlink\" title=\"4) Regularization\"></a>4) Regularization</h3><p>Regularization（正规化）处理属于penalized方法的一种，通过正规化的处理来对原来的方程加上一个regularizer进行penalize，从而使得过渡复杂的模型，变得没那么复杂。</p>\n<p>关于Regularization 的讨论看此链接:<br><a href=\"https://zhichengmle.github.io/2017/10/17/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-17-12.How%20can%20Machine%20Learn%20Better%20-%20Regularization/\">12. 机器学习基石-How can Machine Learn Better? - Regularization</a></p>\n<h3 id=\"5-Validation\"><a href=\"#5-Validation\" class=\"headerlink\" title=\"5) Validation\"></a>5) Validation</h3><p>这个是目前最常用的方法之一，通过提前把一部分的数据拿出来作为测试集，因为测试集是随机取出来的，而将来实际的应用中，数据也大体和测试集出入不大，所以用这种方法，可以提前得到实际应用的时候，模型的错误 $E_{out}$ 通过这个作为衡量模型是否合格的条件之一。</p>\n<p>关于Validation 的讨论看此链接</p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/18/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-18-13.How%20can%20Machine%20Learn%20Better%20-%20Validation/\">13. 机器学习基石-How can Machine Learn Better? - Validation</a></p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><p>1.首先介绍了Overfitting和Underfitting的概念。</p>\n<p>2.接着我们着重分析Overfitting，总结了产生Overfitting的原因：</p>\n<ul>\n<li><p>data size N 太小</p>\n</li>\n<li><p>noise 太多</p>\n</li>\n<li><p>VC Dimension太大</p>\n</li>\n</ul>\n<p>3.最后我们分析如何最大程度的避免Overfitting。在solution中.</p>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\13\\13 - 1 - What is Overfitting- (10-45)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\13\\13 - 2 - The Role of Noise and Data Size (13-36)</p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-can-Machine-Learn-Better-Overfitting-and-Solution\"><a href=\"#How-can-Machine-Learn-Better-Overfitting-and-Solution\" class=\"headerlink\" title=\"How can Machine Learn Better? - Overfitting and Solution\"></a>How can Machine Learn Better? - Overfitting and Solution</h1><h2 id=\"1-What-is-Overfitting\"><a href=\"#1-What-is-Overfitting\" class=\"headerlink\" title=\"1. What is Overfitting?\"></a>1. What is Overfitting?</h2><p>上一节最后，我们提到了如果线性模型的模型复杂度太大的话，可能会引起Overfitting。同样的很明显会有Underfitting的情况。</p>\n<blockquote>\n<p>那什么是Overfitting，Underfitting呢？</p>\n</blockquote>\n<p>首先根据名字，Overfitting：over fitting，就是在fitting的时候太over了。我们用线性模型去分类/回归处理数据的过程就是一个fitting的过程，所以也就是说我们处理过头了。同理Underfitting就是处理不够到位。</p>\n<blockquote>\n<p>那么什么时候才是处理过头呢？什么时候才是处理不到位呢？</p>\n</blockquote>\n<p>就是在处理相对简单的问题的时候用了相对复杂的模型去处理。<br>就是在处理相对复杂的问题的时候用了相对简单的模型去处理。</p>\n<p>我们用下面的例子来进行说明<br>1.首先例子如图一所示(这里用的是Ng的图，因为在林老师的ppt中没找到很好地图同时体现Underfit, good fit overfit)。左图是欠拟合(underfit)，中间的图四好的拟合（good fit），右图是过度拟合（overfit）。单纯从拟合结果来看：明显左边和中间的图 $E_{in}$ 比右图要大。但是从泛化好坏来看，显然左图和中间的图要比右图好。<br>假如我们考虑good fit分类出错的点为噪音点（noise），那么Overfit的模型就会受到了严重的干扰。</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/faeeb6ed9dd42d75c5b4b20d6b9a592c92ce7ece/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-3%20Underfit%20Good%20Fit%20and%20Overfit.png\" alt=\"Underfit, Good Fit and Overfit\"></p>\n<center> 图一 Underfit, Good Fit and Overfit <sup>[1]</sup></center>\n\n\n<ol>\n<li>接着我们回头看之前总结的VC Dimension 的曲线， 如图二所示。</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/17974844acd6bf87d8cf4d68731f9d4cade5b450/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-2%20Learning%20Curve%20.png\" alt=\"Learning Curve\"></p>\n<p>图二 Learning Curve <sup>[1]</sup></p>\n<p>图中可以看到在VC Dimension变大时，$E_{in}$ 变小， 但 $E_{out}$ 先变小后变大，而过拟合和欠拟合的情况的主要区别就在于 $E_{out}$ 的变化情况，具体解释如下：</p>\n<ul>\n<li>过拟合（Overfitting）发生在VC Dimension较大时，$E_{in}$ 太小， 但 $E_{out}$ 太大（即VC Dimension 太大了） ，表示在训练样本上拟合做的很好，$E_{in}$ 太小，但是过度了，使得泛化能力变差， $E_{out}$ 很大</li>\n<li>欠拟合（Underfitting）发生在在VC Dimension较小时，$E_{in}$ 太大，同时 $E_{out}$ 太大（即VC Dimension 太小了），表示在训练样本上拟合做不够好，$E_{in}$ 太大，虽然泛化能力很强（即 $E_{out}$ 也太大）</li>\n</ul>\n<p>关于如何解决欠拟合的问题之前也讨论过： 从低到高不断地提高多项式次数，使得VC维提高，达到拟合的效果。<br>但过拟合的问题更为复杂，下面会更深入的探讨。</p>\n<ol>\n<li>下面的图三，可以让我们更加直观的看到overfitting造成的问题</li>\n</ol>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/9cbdf35a9838985e1f889100ce389c4d316cc0f0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter11-4%20Case%20of%20Overfitting.png\" alt=\"Cases of Overfitting\"></p>\n<center> 图三 Case of Overfitting <sup>[2]</sup></center>\n\n<p>从图中可以看到，Overfitting的 $E_{in}$ 都比 good-fit的要低，但是 $E_{out}$ 却很高（泛化能力差）。</p>\n<p>总结起来有3个因数会导致Overfitting的发生：</p>\n<ul>\n<li>data size N 太小</li>\n<li>noise 太多</li>\n<li>VC Dimension太大</li>\n</ul>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br><br></h2><h2 id=\"2-Dealing-with-Overfitting\"><a href=\"#2-Dealing-with-Overfitting\" class=\"headerlink\" title=\"2. Dealing with Overfitting\"></a>2. Dealing with Overfitting</h2><p>上一节我们提出了overfitting并作了分析。总结出3个因数会导致overfitting，下面根据这3个因数，我们有5种方法帮助我们避免overfitting的发生。</p>\n<ul>\n<li>使用简单的模型(start from simple model)，逐次增加模型的复杂度 - 防止 VC Dimension太大</li>\n<li>进行数据清理/裁剪(data cleaning/pruning) - 防止 noise 太多</li>\n<li>数据提示（data hinting） - 防止 data size N 太小</li>\n<li>正则化（regularization） - 防止 VC Dimension太大</li>\n<li>确认（validation） - 提取一部分的数据作为测试集，提前估计模型的泛化强度</li>\n</ul>\n<p>下面我们分别介绍这5种方法，其中前三种方法比较简单，这里不做深入讨论，而 Regularization 和 Validation 较复杂，这里会用比较多的笔墨进行讨论。</p>\n<h3 id=\"1-Start-from-Simple-Model\"><a href=\"#1-Start-from-Simple-Model\" class=\"headerlink\" title=\"1) Start from Simple Model\"></a>1) Start from Simple Model</h3><p>上一章中也提到过，由于VC Dimension太大的话，导致 $E_{in}$ 变小的同时 $E_{out}$却在变大。所以我们如果从d=1阶的模型开始debug，如果 $E_{in}$ 不符合要求，那么我们增大d为2阶，然后在进行debug，以此类推，直到 $E_{in}$ 符合我们要求位置，这个时候的 VC Dimension 不会很大，而且我们也得到泛化能力相对较强的模型。</p>\n<h3 id=\"2-Data-Cleaning-Pruning\"><a href=\"#2-Data-Cleaning-Pruning\" class=\"headerlink\" title=\"2) Data Cleaning/Pruning\"></a>2) Data Cleaning/Pruning</h3><p>Data cleaning/pruning就是对训练数据集里label有明显错误的样本进行清理（data cleaning）或者裁剪（pruning)。data cleaning/pruning关键在于如何准确寻找label错误的点或者是noise的点。而处理的方法为</p>\n<ul>\n<li>纠正，即数据清理（data cleaning）的方式处理该情况；</li>\n<li>删除错误样本，即数据裁剪（data pruning）的方式处理。</li>\n</ul>\n<p>处理措施很简单，但是发现样本是噪音或离群点却比较困难。</p>\n<h3 id=\"3-Data-Hinting\"><a href=\"#3-Data-Hinting\" class=\"headerlink\" title=\"3) Data Hinting\"></a>3) Data Hinting</h3><p>Data hinting是针对N不够大的情况，通过data hinting的方法就可以对已知的样本进行简单的处理、变换，从而获得更多的样本。比如说：数字分类问题，可以对已知的数字图片进行轻微的平移或者旋转，从而得到更多的数据，达到扩大训练集的目的。这种通过data hinting得到的数据叫做：virtual examples。</p>\n<blockquote>\n<p>需要注意的是，新获取的virtual examples可能不再是iid某个distribution。所以新构建的virtual examples要尽量合理，且是独立同分布的。</p>\n</blockquote>\n<h3 id=\"4-Regularization\"><a href=\"#4-Regularization\" class=\"headerlink\" title=\"4) Regularization\"></a>4) Regularization</h3><p>Regularization（正规化）处理属于penalized方法的一种，通过正规化的处理来对原来的方程加上一个regularizer进行penalize，从而使得过渡复杂的模型，变得没那么复杂。</p>\n<p>关于Regularization 的讨论看此链接:<br><a href=\"https://zhichengmle.github.io/2017/10/17/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-17-12.How%20can%20Machine%20Learn%20Better%20-%20Regularization/\">12. 机器学习基石-How can Machine Learn Better? - Regularization</a></p>\n<h3 id=\"5-Validation\"><a href=\"#5-Validation\" class=\"headerlink\" title=\"5) Validation\"></a>5) Validation</h3><p>这个是目前最常用的方法之一，通过提前把一部分的数据拿出来作为测试集，因为测试集是随机取出来的，而将来实际的应用中，数据也大体和测试集出入不大，所以用这种方法，可以提前得到实际应用的时候，模型的错误 $E_{out}$ 通过这个作为衡量模型是否合格的条件之一。</p>\n<p>关于Validation 的讨论看此链接</p>\n<p><a href=\"https://zhichengmle.github.io/2017/10/18/ReadNote-Machine%20Learning%20Foudantion%20-NTU/2017-10-18-13.How%20can%20Machine%20Learn%20Better%20-%20Validation/\">13. 机器学习基石-How can Machine Learn Better? - Validation</a></p>\n<h2 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><p>1.首先介绍了Overfitting和Underfitting的概念。</p>\n<p>2.接着我们着重分析Overfitting，总结了产生Overfitting的原因：</p>\n<ul>\n<li><p>data size N 太小</p>\n</li>\n<li><p>noise 太多</p>\n</li>\n<li><p>VC Dimension太大</p>\n</li>\n</ul>\n<p>3.最后我们分析如何最大程度的避免Overfitting。在solution中.</p>\n<h2 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"\"></a><br><br></h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] 机器学习基石(台湾大学-林轩田)\\13\\13 - 1 - What is Overfitting- (10-45)</p>\n<p>[2] 机器学习基石(台湾大学-林轩田)\\13\\13 - 2 - The Role of Noise and Data Size (13-36)</p>\n<h2 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"\"></a><br><br></h2>"},{"title":"Stochastic Gradient Descent","date":"2017-12-11T07:25:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Stochastic Gradient Descent\n\n\n## 1. What is Stochastic Gradient Descent\n\nStochastic Gradient Descent(SGD) is similiar with [Batch Gradient Desent](https://zhichengmle.github.io/2017/11/30/Algorithm/Optimize%20Algorithm/2017-11-30-Batch%20Gradient%20Descent/), but it used only 1 example for each iteration. So that it makes some different as well. However, the stochastic gradient descent will not exactly converge into the minimum point. It will bounds around some ratio of the minimum point. Also the cost function will not decrease all the time. It oscillates and tends to converge/expand account for the step size which you have choose.\n\n## 2. Stochastic Gradient Descent Algorithm\n\nSince we use only one example for each iteration, so the weights would be optimize with a random gradient, as a result the direction is unsure. But after loop all examples, the trend of the algorithm will lead to converge. So the this algorithm can never converge exactly to the minimum point. Choose an appropriate step size is of significant importance.\n\nAnother tips to make this algorithm performs better is loop the whole procedure for some times, say 1 to 10 times. This should depend on the dataset, since loop a large data set for 10 times is also compute intensive.\n\nThe algorithm procedure is shown below.\n\n![Sthochastic Gradient Descent Algorithm](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/9cac96cbafaeb85fbc5298a21ec223e43ed84197/__Blog/__Personal%20Understanding/_archive/_images/Stochastic%20Gradient%20Descent-Stochastic%20Gradient%20Descent%20Algorithm.jpg)\n\nSthochastic Gradient Descent Algorithm\n\n\n\n\n## 3. Compute Effort\n\nSince the stochastic use only 1 example each iteration, the compute effort of this algorithm is O(N).\n\n| Batch Gradient Descent          | Stochastic Gradient Descent       |\n| ------------------------------- | --------------------------------- |\n| use 1 example in each iteration | use all example in each iteration |\n| relative compute loose          | relative compute intensive        |\n\n## 4. Visualize Algorithm\n\nThe images below shown the stochastic gradient descent in 1 features and 2 features. It shows that the cost is not alway converge and it eventually converge.\n\n![Visualize Algorithm](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/f0126a7845cc2943671576f3a5622305c1749cc2/__Blog/__Personal%20Understanding/_archive/_images/Stochastic%20Gradient%20Descent-Visualize%203.jpg)\n","source":"_posts/Algorithm/Optimize Algorithm/2017-12-11-Stochastic Gradient Descent.md","raw":"---\ntitle: Stochastic Gradient Descent\ndate: 2017-12-11 15:25:19\ncategories: [Machine-Learning-Algorithm]\ntags: [Machine-Learning-Algorithm]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Stochastic Gradient Descent\n\n\n## 1. What is Stochastic Gradient Descent\n\nStochastic Gradient Descent(SGD) is similiar with [Batch Gradient Desent](https://zhichengmle.github.io/2017/11/30/Algorithm/Optimize%20Algorithm/2017-11-30-Batch%20Gradient%20Descent/), but it used only 1 example for each iteration. So that it makes some different as well. However, the stochastic gradient descent will not exactly converge into the minimum point. It will bounds around some ratio of the minimum point. Also the cost function will not decrease all the time. It oscillates and tends to converge/expand account for the step size which you have choose.\n\n## 2. Stochastic Gradient Descent Algorithm\n\nSince we use only one example for each iteration, so the weights would be optimize with a random gradient, as a result the direction is unsure. But after loop all examples, the trend of the algorithm will lead to converge. So the this algorithm can never converge exactly to the minimum point. Choose an appropriate step size is of significant importance.\n\nAnother tips to make this algorithm performs better is loop the whole procedure for some times, say 1 to 10 times. This should depend on the dataset, since loop a large data set for 10 times is also compute intensive.\n\nThe algorithm procedure is shown below.\n\n![Sthochastic Gradient Descent Algorithm](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/9cac96cbafaeb85fbc5298a21ec223e43ed84197/__Blog/__Personal%20Understanding/_archive/_images/Stochastic%20Gradient%20Descent-Stochastic%20Gradient%20Descent%20Algorithm.jpg)\n\nSthochastic Gradient Descent Algorithm\n\n\n\n\n## 3. Compute Effort\n\nSince the stochastic use only 1 example each iteration, the compute effort of this algorithm is O(N).\n\n| Batch Gradient Descent          | Stochastic Gradient Descent       |\n| ------------------------------- | --------------------------------- |\n| use 1 example in each iteration | use all example in each iteration |\n| relative compute loose          | relative compute intensive        |\n\n## 4. Visualize Algorithm\n\nThe images below shown the stochastic gradient descent in 1 features and 2 features. It shows that the cost is not alway converge and it eventually converge.\n\n![Visualize Algorithm](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/f0126a7845cc2943671576f3a5622305c1749cc2/__Blog/__Personal%20Understanding/_archive/_images/Stochastic%20Gradient%20Descent-Visualize%203.jpg)\n","slug":"Algorithm/Optimize Algorithm/2017-12-11-Stochastic Gradient Descent","published":1,"updated":"2018-04-14T19:42:06.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2h8002yrwtj3fwdk81i","content":"<h1 id=\"Stochastic-Gradient-Descent\"><a href=\"#Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"Stochastic Gradient Descent\"></a>Stochastic Gradient Descent</h1><h2 id=\"1-What-is-Stochastic-Gradient-Descent\"><a href=\"#1-What-is-Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"1. What is Stochastic Gradient Descent\"></a>1. What is Stochastic Gradient Descent</h2><p>Stochastic Gradient Descent(SGD) is similiar with <a href=\"https://zhichengmle.github.io/2017/11/30/Algorithm/Optimize%20Algorithm/2017-11-30-Batch%20Gradient%20Descent/\">Batch Gradient Desent</a>, but it used only 1 example for each iteration. So that it makes some different as well. However, the stochastic gradient descent will not exactly converge into the minimum point. It will bounds around some ratio of the minimum point. Also the cost function will not decrease all the time. It oscillates and tends to converge/expand account for the step size which you have choose.</p>\n<h2 id=\"2-Stochastic-Gradient-Descent-Algorithm\"><a href=\"#2-Stochastic-Gradient-Descent-Algorithm\" class=\"headerlink\" title=\"2. Stochastic Gradient Descent Algorithm\"></a>2. Stochastic Gradient Descent Algorithm</h2><p>Since we use only one example for each iteration, so the weights would be optimize with a random gradient, as a result the direction is unsure. But after loop all examples, the trend of the algorithm will lead to converge. So the this algorithm can never converge exactly to the minimum point. Choose an appropriate step size is of significant importance.</p>\n<p>Another tips to make this algorithm performs better is loop the whole procedure for some times, say 1 to 10 times. This should depend on the dataset, since loop a large data set for 10 times is also compute intensive.</p>\n<p>The algorithm procedure is shown below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/9cac96cbafaeb85fbc5298a21ec223e43ed84197/__Blog/__Personal%20Understanding/_archive/_images/Stochastic%20Gradient%20Descent-Stochastic%20Gradient%20Descent%20Algorithm.jpg\" alt=\"Sthochastic Gradient Descent Algorithm\"></p>\n<p>Sthochastic Gradient Descent Algorithm</p>\n<h2 id=\"3-Compute-Effort\"><a href=\"#3-Compute-Effort\" class=\"headerlink\" title=\"3. Compute Effort\"></a>3. Compute Effort</h2><p>Since the stochastic use only 1 example each iteration, the compute effort of this algorithm is O(N).</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Batch Gradient Descent</th>\n<th>Stochastic Gradient Descent</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>use 1 example in each iteration</td>\n<td>use all example in each iteration</td>\n</tr>\n<tr>\n<td>relative compute loose</td>\n<td>relative compute intensive</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"4-Visualize-Algorithm\"><a href=\"#4-Visualize-Algorithm\" class=\"headerlink\" title=\"4. Visualize Algorithm\"></a>4. Visualize Algorithm</h2><p>The images below shown the stochastic gradient descent in 1 features and 2 features. It shows that the cost is not alway converge and it eventually converge.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/f0126a7845cc2943671576f3a5622305c1749cc2/__Blog/__Personal%20Understanding/_archive/_images/Stochastic%20Gradient%20Descent-Visualize%203.jpg\" alt=\"Visualize Algorithm\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Stochastic-Gradient-Descent\"><a href=\"#Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"Stochastic Gradient Descent\"></a>Stochastic Gradient Descent</h1><h2 id=\"1-What-is-Stochastic-Gradient-Descent\"><a href=\"#1-What-is-Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"1. What is Stochastic Gradient Descent\"></a>1. What is Stochastic Gradient Descent</h2><p>Stochastic Gradient Descent(SGD) is similiar with <a href=\"https://zhichengmle.github.io/2017/11/30/Algorithm/Optimize%20Algorithm/2017-11-30-Batch%20Gradient%20Descent/\">Batch Gradient Desent</a>, but it used only 1 example for each iteration. So that it makes some different as well. However, the stochastic gradient descent will not exactly converge into the minimum point. It will bounds around some ratio of the minimum point. Also the cost function will not decrease all the time. It oscillates and tends to converge/expand account for the step size which you have choose.</p>\n<h2 id=\"2-Stochastic-Gradient-Descent-Algorithm\"><a href=\"#2-Stochastic-Gradient-Descent-Algorithm\" class=\"headerlink\" title=\"2. Stochastic Gradient Descent Algorithm\"></a>2. Stochastic Gradient Descent Algorithm</h2><p>Since we use only one example for each iteration, so the weights would be optimize with a random gradient, as a result the direction is unsure. But after loop all examples, the trend of the algorithm will lead to converge. So the this algorithm can never converge exactly to the minimum point. Choose an appropriate step size is of significant importance.</p>\n<p>Another tips to make this algorithm performs better is loop the whole procedure for some times, say 1 to 10 times. This should depend on the dataset, since loop a large data set for 10 times is also compute intensive.</p>\n<p>The algorithm procedure is shown below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/9cac96cbafaeb85fbc5298a21ec223e43ed84197/__Blog/__Personal%20Understanding/_archive/_images/Stochastic%20Gradient%20Descent-Stochastic%20Gradient%20Descent%20Algorithm.jpg\" alt=\"Sthochastic Gradient Descent Algorithm\"></p>\n<p>Sthochastic Gradient Descent Algorithm</p>\n<h2 id=\"3-Compute-Effort\"><a href=\"#3-Compute-Effort\" class=\"headerlink\" title=\"3. Compute Effort\"></a>3. Compute Effort</h2><p>Since the stochastic use only 1 example each iteration, the compute effort of this algorithm is O(N).</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Batch Gradient Descent</th>\n<th>Stochastic Gradient Descent</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>use 1 example in each iteration</td>\n<td>use all example in each iteration</td>\n</tr>\n<tr>\n<td>relative compute loose</td>\n<td>relative compute intensive</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"4-Visualize-Algorithm\"><a href=\"#4-Visualize-Algorithm\" class=\"headerlink\" title=\"4. Visualize Algorithm\"></a>4. Visualize Algorithm</h2><p>The images below shown the stochastic gradient descent in 1 features and 2 features. It shows that the cost is not alway converge and it eventually converge.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/f0126a7845cc2943671576f3a5622305c1749cc2/__Blog/__Personal%20Understanding/_archive/_images/Stochastic%20Gradient%20Descent-Visualize%203.jpg\" alt=\"Visualize Algorithm\"></p>\n"},{"title":"Gradient Descent","date":"2017-12-12T09:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n\n# Gradient Descent\n\n> There are three gradient descent we current have: batch gradient descent, mini-batch gradient descent, stochastic gradient descent.\n\n## 1. Batch Gradient Descent\n\nA compute intensive gradient descent to converge the value. This algorithm will compute all the data in each iteration.\n\nSee more at the following link.\n\n[Batch Gradient Descent](https://zhichengmle.github.io/2017/11/30/Algorithm/Optimize%20Algorithm/2017-11-30-Batch%20Gradient%20Descent/)\n\n\n## 2. Stochastic Gradient Descent\n\nContrast to Batch Gradient Descent, Stochastic Gradient Descent (SGD) algorithm will compute only one the data in each iteration. To make the performance better. We will repeat the whole process for 1 to 10 times, according to the size of the data set.\n\nSee more at the following link.\n\n[Stochastic Gradient Descent](https://zhichengmle.github.io/2017/12/11/Algorithm/Optimize%20Algorithm/2017-12-11-Stochastic%20Gradient%20Descent/)\n\n\n\n## 3. Mini-Batch Gradient Descent\n\nMini-Batch Gradient Descent is a special algorithm which computes n (1≤n≤all) data in each iteration. That is to say, if the n=1, we got Stochastic Gradient Descent. And if the n=all, we got Batch Gradient Descent.\n\nSee more at the following link.\n\n[Mini-Batch Gradient Descent](https://zhichengmle.github.io/2017/12/12/Algorithm/Optimize%20Algorithm/2017-12-12-Mini-Batch%20Gradient%20Descent/)\n","source":"_posts/Algorithm/Optimize Algorithm/2017-12-12-Gradient Descent.md","raw":"---\ntitle:  Gradient Descent\ndate: 2017-12-12 17:23:19\ncategories: [Machine-Learning-Algorithm]\ntags: [Machine-Learning-Algorithm]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n\n# Gradient Descent\n\n> There are three gradient descent we current have: batch gradient descent, mini-batch gradient descent, stochastic gradient descent.\n\n## 1. Batch Gradient Descent\n\nA compute intensive gradient descent to converge the value. This algorithm will compute all the data in each iteration.\n\nSee more at the following link.\n\n[Batch Gradient Descent](https://zhichengmle.github.io/2017/11/30/Algorithm/Optimize%20Algorithm/2017-11-30-Batch%20Gradient%20Descent/)\n\n\n## 2. Stochastic Gradient Descent\n\nContrast to Batch Gradient Descent, Stochastic Gradient Descent (SGD) algorithm will compute only one the data in each iteration. To make the performance better. We will repeat the whole process for 1 to 10 times, according to the size of the data set.\n\nSee more at the following link.\n\n[Stochastic Gradient Descent](https://zhichengmle.github.io/2017/12/11/Algorithm/Optimize%20Algorithm/2017-12-11-Stochastic%20Gradient%20Descent/)\n\n\n\n## 3. Mini-Batch Gradient Descent\n\nMini-Batch Gradient Descent is a special algorithm which computes n (1≤n≤all) data in each iteration. That is to say, if the n=1, we got Stochastic Gradient Descent. And if the n=all, we got Batch Gradient Descent.\n\nSee more at the following link.\n\n[Mini-Batch Gradient Descent](https://zhichengmle.github.io/2017/12/12/Algorithm/Optimize%20Algorithm/2017-12-12-Mini-Batch%20Gradient%20Descent/)\n","slug":"Algorithm/Optimize Algorithm/2017-12-12-Gradient Descent","published":1,"updated":"2018-04-14T19:42:06.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2ha002zrwtjo3tlfuiz","content":"<h1 id=\"Gradient-Descent\"><a href=\"#Gradient-Descent\" class=\"headerlink\" title=\"Gradient Descent\"></a>Gradient Descent</h1><blockquote>\n<p>There are three gradient descent we current have: batch gradient descent, mini-batch gradient descent, stochastic gradient descent.</p>\n</blockquote>\n<h2 id=\"1-Batch-Gradient-Descent\"><a href=\"#1-Batch-Gradient-Descent\" class=\"headerlink\" title=\"1. Batch Gradient Descent\"></a>1. Batch Gradient Descent</h2><p>A compute intensive gradient descent to converge the value. This algorithm will compute all the data in each iteration.</p>\n<p>See more at the following link.</p>\n<p><a href=\"https://zhichengmle.github.io/2017/11/30/Algorithm/Optimize%20Algorithm/2017-11-30-Batch%20Gradient%20Descent/\">Batch Gradient Descent</a></p>\n<h2 id=\"2-Stochastic-Gradient-Descent\"><a href=\"#2-Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"2. Stochastic Gradient Descent\"></a>2. Stochastic Gradient Descent</h2><p>Contrast to Batch Gradient Descent, Stochastic Gradient Descent (SGD) algorithm will compute only one the data in each iteration. To make the performance better. We will repeat the whole process for 1 to 10 times, according to the size of the data set.</p>\n<p>See more at the following link.</p>\n<p><a href=\"https://zhichengmle.github.io/2017/12/11/Algorithm/Optimize%20Algorithm/2017-12-11-Stochastic%20Gradient%20Descent/\">Stochastic Gradient Descent</a></p>\n<h2 id=\"3-Mini-Batch-Gradient-Descent\"><a href=\"#3-Mini-Batch-Gradient-Descent\" class=\"headerlink\" title=\"3. Mini-Batch Gradient Descent\"></a>3. Mini-Batch Gradient Descent</h2><p>Mini-Batch Gradient Descent is a special algorithm which computes n (1≤n≤all) data in each iteration. That is to say, if the n=1, we got Stochastic Gradient Descent. And if the n=all, we got Batch Gradient Descent.</p>\n<p>See more at the following link.</p>\n<p><a href=\"https://zhichengmle.github.io/2017/12/12/Algorithm/Optimize%20Algorithm/2017-12-12-Mini-Batch%20Gradient%20Descent/\">Mini-Batch Gradient Descent</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Gradient-Descent\"><a href=\"#Gradient-Descent\" class=\"headerlink\" title=\"Gradient Descent\"></a>Gradient Descent</h1><blockquote>\n<p>There are three gradient descent we current have: batch gradient descent, mini-batch gradient descent, stochastic gradient descent.</p>\n</blockquote>\n<h2 id=\"1-Batch-Gradient-Descent\"><a href=\"#1-Batch-Gradient-Descent\" class=\"headerlink\" title=\"1. Batch Gradient Descent\"></a>1. Batch Gradient Descent</h2><p>A compute intensive gradient descent to converge the value. This algorithm will compute all the data in each iteration.</p>\n<p>See more at the following link.</p>\n<p><a href=\"https://zhichengmle.github.io/2017/11/30/Algorithm/Optimize%20Algorithm/2017-11-30-Batch%20Gradient%20Descent/\">Batch Gradient Descent</a></p>\n<h2 id=\"2-Stochastic-Gradient-Descent\"><a href=\"#2-Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"2. Stochastic Gradient Descent\"></a>2. Stochastic Gradient Descent</h2><p>Contrast to Batch Gradient Descent, Stochastic Gradient Descent (SGD) algorithm will compute only one the data in each iteration. To make the performance better. We will repeat the whole process for 1 to 10 times, according to the size of the data set.</p>\n<p>See more at the following link.</p>\n<p><a href=\"https://zhichengmle.github.io/2017/12/11/Algorithm/Optimize%20Algorithm/2017-12-11-Stochastic%20Gradient%20Descent/\">Stochastic Gradient Descent</a></p>\n<h2 id=\"3-Mini-Batch-Gradient-Descent\"><a href=\"#3-Mini-Batch-Gradient-Descent\" class=\"headerlink\" title=\"3. Mini-Batch Gradient Descent\"></a>3. Mini-Batch Gradient Descent</h2><p>Mini-Batch Gradient Descent is a special algorithm which computes n (1≤n≤all) data in each iteration. That is to say, if the n=1, we got Stochastic Gradient Descent. And if the n=all, we got Batch Gradient Descent.</p>\n<p>See more at the following link.</p>\n<p><a href=\"https://zhichengmle.github.io/2017/12/12/Algorithm/Optimize%20Algorithm/2017-12-12-Mini-Batch%20Gradient%20Descent/\">Mini-Batch Gradient Descent</a></p>\n"},{"title":"Maximum Likelihood Estimation","date":"2017-12-12T13:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Maximum Likelihood Estimation\n\n## 1. What is Maximum Likelihood Estimation?\n\nMaximum likelihood estimation (MLE) is a methods which estimats the parameters of a statistical model given data, by exploiting the parameter that maximize the likelihood.[1]\n\nSo by giving specific observation, MLE is a way to figure out the value to make the probability maximum.\n\n### 1.1. What is Likelihood?\n\nGiven an independent and identically distributed sample, we could get the probablity, which is shown as formula (1).\n\n$$\n\\begin{align}\nf(x_1,x_2,...,x_n|\\theta)\n&= f(x_1|\\theta) \\times f(x_2|\\theta) \\times ... \\times f(x_n|\\theta) \\\\\n&= \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\n\\end{align}\n\\tag{1}\n$$\n\nThis could also be written as formula (2).\n\n$$\nL(\\theta;x_1,x_2,...,x_n) = f(x_1,x_2,...,x_n|\\theta) = \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\n\\tag{2}\n$$\n\n### 1.2. What is Log Likelihood?\n\nFor more convenient, we usually use log likelihood which applys natural logarithm onto likelihood. The Logarithm product rule is $\\log(a \\cdot b) = log(a) + log(b)$ [2] By doing that, the product turns to the sum problem.\n\n$$\n\\begin{align}\n\\ln L(\\theta;x_1,x_2,...,x_n)\n&= \\ln \\prod\\limits_{i=1}^{n}f(x_i|\\theta) \\\\\n&= \\sum\\limits_{i=1}^{n}f(x_i|\\theta)\n\\end{align}\n\\tag{3}\n$$\n\n\n\n## 2. Maximum Likelihood Estimation in Regression\n\nIn regression, all the data are independence: therefore the hypothesis accounts for Gaussian Distribution $p(y|x,\\theta) ∼ N(\\epsilon, \\sigma^2)$. To make the correct probability maximum, we apply MLE into this. So the probability of correct could be written as formula (4).\n\n$$\n\\begin{align}\nL(\\theta;x_1,x_2,...,x_n)\n&= \\prod\\limits_{i=1}^{n}p(y|x,\\theta) \\\\\n&= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)}) }{2\\sigma ^2}\\right)\\\\\n\\end{align}\n\\tag{4}\n$$\n\nThen we use log likelihood to make the problem easier to compute. See formula (5).\n\n\n$$\n\\begin{align}\n\\ln L(\\theta;x_1,x_2,...,x_n)\n&= \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2}\\right)\\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2}\\right) \\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) + \\sum\\limits_{1}^{n}\\log \\left(\\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2} \\right)\\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) + \\sum\\limits_{1}^{n}\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2} \\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) - \\underbrace {\\frac{1}{2\\sigma ^2}\\sum\\limits_{1}^{n}\\left( -(y^{(i)} - \\theta ^ T x^{(i)}) \\right)}_{Cost-Function}\\\\\n\\end{align}\n\\tag{5}\n$$\n\nTo make the probability maximum, we need to make the cost function minimum, which is also known as cost function in regression model.\n\n\n\n# Reference\n[1] [Wikipedia-Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)\n\n[2] [RapidTables\n-Logarithm Rules](https://www.rapidtables.com/math/algebra/Logarithm.html#log-rules)\n","source":"_posts/Algorithm/Optimize Algorithm/2017-12-12-Maximum Likelihood Estimation.md","raw":"---\ntitle: Maximum Likelihood Estimation\ndate: 2017-12-12 21:23:19\ncategories: [Machine-Learning-Algorithm]\ntags: [Machine-Learning-Algorithm]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Maximum Likelihood Estimation\n\n## 1. What is Maximum Likelihood Estimation?\n\nMaximum likelihood estimation (MLE) is a methods which estimats the parameters of a statistical model given data, by exploiting the parameter that maximize the likelihood.[1]\n\nSo by giving specific observation, MLE is a way to figure out the value to make the probability maximum.\n\n### 1.1. What is Likelihood?\n\nGiven an independent and identically distributed sample, we could get the probablity, which is shown as formula (1).\n\n$$\n\\begin{align}\nf(x_1,x_2,...,x_n|\\theta)\n&= f(x_1|\\theta) \\times f(x_2|\\theta) \\times ... \\times f(x_n|\\theta) \\\\\n&= \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\n\\end{align}\n\\tag{1}\n$$\n\nThis could also be written as formula (2).\n\n$$\nL(\\theta;x_1,x_2,...,x_n) = f(x_1,x_2,...,x_n|\\theta) = \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\n\\tag{2}\n$$\n\n### 1.2. What is Log Likelihood?\n\nFor more convenient, we usually use log likelihood which applys natural logarithm onto likelihood. The Logarithm product rule is $\\log(a \\cdot b) = log(a) + log(b)$ [2] By doing that, the product turns to the sum problem.\n\n$$\n\\begin{align}\n\\ln L(\\theta;x_1,x_2,...,x_n)\n&= \\ln \\prod\\limits_{i=1}^{n}f(x_i|\\theta) \\\\\n&= \\sum\\limits_{i=1}^{n}f(x_i|\\theta)\n\\end{align}\n\\tag{3}\n$$\n\n\n\n## 2. Maximum Likelihood Estimation in Regression\n\nIn regression, all the data are independence: therefore the hypothesis accounts for Gaussian Distribution $p(y|x,\\theta) ∼ N(\\epsilon, \\sigma^2)$. To make the correct probability maximum, we apply MLE into this. So the probability of correct could be written as formula (4).\n\n$$\n\\begin{align}\nL(\\theta;x_1,x_2,...,x_n)\n&= \\prod\\limits_{i=1}^{n}p(y|x,\\theta) \\\\\n&= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)}) }{2\\sigma ^2}\\right)\\\\\n\\end{align}\n\\tag{4}\n$$\n\nThen we use log likelihood to make the problem easier to compute. See formula (5).\n\n\n$$\n\\begin{align}\n\\ln L(\\theta;x_1,x_2,...,x_n)\n&= \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2}\\right)\\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2}\\right) \\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) + \\sum\\limits_{1}^{n}\\log \\left(\\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2} \\right)\\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) + \\sum\\limits_{1}^{n}\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2} \\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) - \\underbrace {\\frac{1}{2\\sigma ^2}\\sum\\limits_{1}^{n}\\left( -(y^{(i)} - \\theta ^ T x^{(i)}) \\right)}_{Cost-Function}\\\\\n\\end{align}\n\\tag{5}\n$$\n\nTo make the probability maximum, we need to make the cost function minimum, which is also known as cost function in regression model.\n\n\n\n# Reference\n[1] [Wikipedia-Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)\n\n[2] [RapidTables\n-Logarithm Rules](https://www.rapidtables.com/math/algebra/Logarithm.html#log-rules)\n","slug":"Algorithm/Optimize Algorithm/2017-12-12-Maximum Likelihood Estimation","published":1,"updated":"2018-04-14T19:42:06.484Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hb0032rwtj19qfkzf0","content":"<h1 id=\"Maximum-Likelihood-Estimation\"><a href=\"#Maximum-Likelihood-Estimation\" class=\"headerlink\" title=\"Maximum Likelihood Estimation\"></a>Maximum Likelihood Estimation</h1><h2 id=\"1-What-is-Maximum-Likelihood-Estimation\"><a href=\"#1-What-is-Maximum-Likelihood-Estimation\" class=\"headerlink\" title=\"1. What is Maximum Likelihood Estimation?\"></a>1. What is Maximum Likelihood Estimation?</h2><p>Maximum likelihood estimation (MLE) is a methods which estimats the parameters of a statistical model given data, by exploiting the parameter that maximize the likelihood.[1]</p>\n<p>So by giving specific observation, MLE is a way to figure out the value to make the probability maximum.</p>\n<h3 id=\"1-1-What-is-Likelihood\"><a href=\"#1-1-What-is-Likelihood\" class=\"headerlink\" title=\"1.1. What is Likelihood?\"></a>1.1. What is Likelihood?</h3><p>Given an independent and identically distributed sample, we could get the probablity, which is shown as formula (1).</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf(x_1,x_2,...,x_n|\\theta)\n&= f(x_1|\\theta) \\times f(x_2|\\theta) \\times ... \\times f(x_n|\\theta) \\\\\n&= \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\n\\end{align}\n\\tag{1}</script><p>This could also be written as formula (2).</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta;x_1,x_2,...,x_n) = f(x_1,x_2,...,x_n|\\theta) = \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\n\\tag{2}</script><h3 id=\"1-2-What-is-Log-Likelihood\"><a href=\"#1-2-What-is-Log-Likelihood\" class=\"headerlink\" title=\"1.2. What is Log Likelihood?\"></a>1.2. What is Log Likelihood?</h3><p>For more convenient, we usually use log likelihood which applys natural logarithm onto likelihood. The Logarithm product rule is $\\log(a \\cdot b) = log(a) + log(b)$ [2] By doing that, the product turns to the sum problem.</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\ln L(\\theta;x_1,x_2,...,x_n)\n&= \\ln \\prod\\limits_{i=1}^{n}f(x_i|\\theta) \\\\\n&= \\sum\\limits_{i=1}^{n}f(x_i|\\theta)\n\\end{align}\n\\tag{3}</script><h2 id=\"2-Maximum-Likelihood-Estimation-in-Regression\"><a href=\"#2-Maximum-Likelihood-Estimation-in-Regression\" class=\"headerlink\" title=\"2. Maximum Likelihood Estimation in Regression\"></a>2. Maximum Likelihood Estimation in Regression</h2><p>In regression, all the data are independence: therefore the hypothesis accounts for Gaussian Distribution $p(y|x,\\theta) ∼ N(\\epsilon, \\sigma^2)$. To make the correct probability maximum, we apply MLE into this. So the probability of correct could be written as formula (4).</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nL(\\theta;x_1,x_2,...,x_n)\n&= \\prod\\limits_{i=1}^{n}p(y|x,\\theta) \\\\\n&= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)}) }{2\\sigma ^2}\\right)\\\\\n\\end{align}\n\\tag{4}</script><p>Then we use log likelihood to make the problem easier to compute. See formula (5).</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\ln L(\\theta;x_1,x_2,...,x_n)\n&= \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2}\\right)\\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2}\\right) \\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) + \\sum\\limits_{1}^{n}\\log \\left(\\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2} \\right)\\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) + \\sum\\limits_{1}^{n}\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2} \\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) - \\underbrace {\\frac{1}{2\\sigma ^2}\\sum\\limits_{1}^{n}\\left( -(y^{(i)} - \\theta ^ T x^{(i)}) \\right)}_{Cost-Function}\\\\\n\\end{align}\n\\tag{5}</script><p>To make the probability maximum, we need to make the cost function minimum, which is also known as cost function in regression model.</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\" target=\"_blank\" rel=\"external\">Wikipedia-Maximum Likelihood Estimation</a></p>\n<p>[2] <a href=\"https://www.rapidtables.com/math/algebra/Logarithm.html#log-rules\" target=\"_blank\" rel=\"external\">RapidTables<br>-Logarithm Rules</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Maximum-Likelihood-Estimation\"><a href=\"#Maximum-Likelihood-Estimation\" class=\"headerlink\" title=\"Maximum Likelihood Estimation\"></a>Maximum Likelihood Estimation</h1><h2 id=\"1-What-is-Maximum-Likelihood-Estimation\"><a href=\"#1-What-is-Maximum-Likelihood-Estimation\" class=\"headerlink\" title=\"1. What is Maximum Likelihood Estimation?\"></a>1. What is Maximum Likelihood Estimation?</h2><p>Maximum likelihood estimation (MLE) is a methods which estimats the parameters of a statistical model given data, by exploiting the parameter that maximize the likelihood.[1]</p>\n<p>So by giving specific observation, MLE is a way to figure out the value to make the probability maximum.</p>\n<h3 id=\"1-1-What-is-Likelihood\"><a href=\"#1-1-What-is-Likelihood\" class=\"headerlink\" title=\"1.1. What is Likelihood?\"></a>1.1. What is Likelihood?</h3><p>Given an independent and identically distributed sample, we could get the probablity, which is shown as formula (1).</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf(x_1,x_2,...,x_n|\\theta)\n&= f(x_1|\\theta) \\times f(x_2|\\theta) \\times ... \\times f(x_n|\\theta) \\\\\n&= \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\n\\end{align}\n\\tag{1}</script><p>This could also be written as formula (2).</p>\n<script type=\"math/tex; mode=display\">\nL(\\theta;x_1,x_2,...,x_n) = f(x_1,x_2,...,x_n|\\theta) = \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\n\\tag{2}</script><h3 id=\"1-2-What-is-Log-Likelihood\"><a href=\"#1-2-What-is-Log-Likelihood\" class=\"headerlink\" title=\"1.2. What is Log Likelihood?\"></a>1.2. What is Log Likelihood?</h3><p>For more convenient, we usually use log likelihood which applys natural logarithm onto likelihood. The Logarithm product rule is $\\log(a \\cdot b) = log(a) + log(b)$ [2] By doing that, the product turns to the sum problem.</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\ln L(\\theta;x_1,x_2,...,x_n)\n&= \\ln \\prod\\limits_{i=1}^{n}f(x_i|\\theta) \\\\\n&= \\sum\\limits_{i=1}^{n}f(x_i|\\theta)\n\\end{align}\n\\tag{3}</script><h2 id=\"2-Maximum-Likelihood-Estimation-in-Regression\"><a href=\"#2-Maximum-Likelihood-Estimation-in-Regression\" class=\"headerlink\" title=\"2. Maximum Likelihood Estimation in Regression\"></a>2. Maximum Likelihood Estimation in Regression</h2><p>In regression, all the data are independence: therefore the hypothesis accounts for Gaussian Distribution $p(y|x,\\theta) ∼ N(\\epsilon, \\sigma^2)$. To make the correct probability maximum, we apply MLE into this. So the probability of correct could be written as formula (4).</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nL(\\theta;x_1,x_2,...,x_n)\n&= \\prod\\limits_{i=1}^{n}p(y|x,\\theta) \\\\\n&= \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)}) }{2\\sigma ^2}\\right)\\\\\n\\end{align}\n\\tag{4}</script><p>Then we use log likelihood to make the problem easier to compute. See formula (5).</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\ln L(\\theta;x_1,x_2,...,x_n)\n&= \\ln \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2}\\right)\\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2}\\right) \\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) + \\sum\\limits_{1}^{n}\\log \\left(\\exp\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2} \\right)\\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) + \\sum\\limits_{1}^{n}\\left( \\frac{-(y^{(i)} - \\theta ^ T x^{(i)})}{2\\sigma ^2} \\right) \\\\\n&= \\sum\\limits_{1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi}\\sigma} \\right) - \\underbrace {\\frac{1}{2\\sigma ^2}\\sum\\limits_{1}^{n}\\left( -(y^{(i)} - \\theta ^ T x^{(i)}) \\right)}_{Cost-Function}\\\\\n\\end{align}\n\\tag{5}</script><p>To make the probability maximum, we need to make the cost function minimum, which is also known as cost function in regression model.</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\" target=\"_blank\" rel=\"external\">Wikipedia-Maximum Likelihood Estimation</a></p>\n<p>[2] <a href=\"https://www.rapidtables.com/math/algebra/Logarithm.html#log-rules\" target=\"_blank\" rel=\"external\">RapidTables<br>-Logarithm Rules</a></p>\n"},{"title":"Differentiation Rules","date":"2017-11-02T04:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Differentiation Rules\n\n## 1. The Sum Rule\n\nIn calculus, the sum rule in differentiation is a method of finding the derivative of a function that is the sum of two other functions for which derivatives exist.[1]\n\nGiven: $h\\left( x \\right) = f\\left( x \\right) + g\\left( x \\right)$\n\nProofs: $h'\\left( x \\right) = f'\\left( x \\right) + g'\\left( x \\right)$\n\n$$\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) + g\\left(x + \\Delta x\\right) - f \\left( x \\right) - g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right)  - f \\left( x \\right) + g\\left(x + \\Delta x\\right) - g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right)  - f \\left( x \\right) }{\\Delta x} + \\frac{ g\\left(x + \\Delta x\\right)  - g \\left( x \\right) }{\\Delta x} \\\\\n&= f'\\left( x \\right) + g'\\left( x \\right)\n\\end{align}\n$$\n\n## 2. The Product Rule\n\nIn calculus, the product rule is a formula used to find the derivatives of products of two or more functions.[2]\n\nGiven: $h\\left( x \\right) = f\\left( x \\right) \\cdot g\\left( x \\right)$\n\nProofs: $h'\\left( x \\right) = f'\\left( x \\right)g\\left( x \\right) + f\\left( x \\right)g'\\left( x \\right)$\n\n$$\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) \\cdot g\\left(x + \\Delta x\\right) - f \\left( x \\right) \\cdot g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) \\cdot g\\left(x + \\Delta x\\right) - \\left[ f\\left(x \\right) \\cdot g\\left(x + \\Delta x \\right) + f\\left(x \\right) \\cdot g\\left(x + \\Delta x \\right)\\right] - f \\left( x \\right) \\cdot g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left[f\\left(x + \\Delta x\\right) -f\\left( x \\right)\\right] \\cdot g\\left(x + \\Delta x\\right) +  f\\left( x \\right) \\cdot \\left[g\\left(x + \\Delta x\\right) -g\\left( x \\right)\\right] }{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) -f\\left( x \\right)}{\\Delta x} \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} g\\left(x + \\Delta x\\right) + \\lim\\limits_{\\Delta x \\rightarrow0} f\\left(x + \\Delta x\\right)  \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ g\\left(x + \\Delta x\\right) -g\\left( x \\right)}{\\Delta x} \\\\\n&= f'\\left( x \\right)g\\left( x \\right) + f\\left( x \\right)g'\\left( x \\right)\n\\end{align}\n$$\n\n\n## 3. The Quotient Rule\n\nIn calculus, the quotient rule is a method of finding the derivative of a function that is the ratio of two differentiable functions.[3]\n\nGiven: $h\\left( x \\right) = \\frac {f\\left( x \\right)} {g\\left( x \\right)}$\n\nProofs: $h'\\left( x \\right) = \\frac{f'\\left( x \\right)g\\left( x \\right) -f\\left( x \\right) g'\\left( x \\right)}{g\\left( x \\right)^2}$\n\n$$\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{\\frac {f\\left( x + \\Delta x \\right)} {g\\left( x + \\Delta x \\right)} - \\frac{f\\left( x \\right)}{g\\left( x \\right)}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{f\\left( x + \\Delta x \\right) g\\left( x \\right) - f\\left( x \\right)g\\left( x + \\Delta x \\right)}{\\Delta x \\cdot g\\left( x \\right)g\\left( x + \\Delta x \\right)}\\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{f\\left( x + \\Delta x \\right) g\\left( x \\right) - f\\left( x \\right)g\\left( x + \\Delta x \\right)}{\\Delta x}  \\cdot \\frac{1}{g\\left( x \\right)g\\left( x + \\Delta x \\right)}\\\\\n&= \\left[\\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) -f\\left( x \\right)}{\\Delta x} \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} g\\left(x\\right) - \\lim\\limits_{\\Delta x \\rightarrow0} f\\left(x \\right)  \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ g\\left(x + \\Delta x\\right) -g\\left( x \\right)}{\\Delta x}\\right] \\cdot \\frac{1}{g\\left( x \\right)^2}\\\\\n&= \\frac{f'\\left( x \\right)g\\left( x \\right) -f\\left( x \\right) g'\\left( x \\right)}{g\\left( x \\right)^2}\n\\end{align}\n$$\n\n## Reference\n[1] [Wikipedia-Sum rule in differentiation](https://en.wikipedia.org/wiki/Sum_rule_in_differentiation)\n\n[2] [Wikipedia-Product rule](https://en.wikipedia.org/wiki/Product_rule)\n\n[3] [Wikipedia-Quotient_rule](https://en.wikipedia.org/wiki/Quotient_rule)\n\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/Calculus/2017-11-02-Differentiation Rules.md","raw":"---\ntitle: Differentiation Rules\ndate: 2017-11-02 12:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics, Calculus]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Differentiation Rules\n\n## 1. The Sum Rule\n\nIn calculus, the sum rule in differentiation is a method of finding the derivative of a function that is the sum of two other functions for which derivatives exist.[1]\n\nGiven: $h\\left( x \\right) = f\\left( x \\right) + g\\left( x \\right)$\n\nProofs: $h'\\left( x \\right) = f'\\left( x \\right) + g'\\left( x \\right)$\n\n$$\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) + g\\left(x + \\Delta x\\right) - f \\left( x \\right) - g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right)  - f \\left( x \\right) + g\\left(x + \\Delta x\\right) - g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right)  - f \\left( x \\right) }{\\Delta x} + \\frac{ g\\left(x + \\Delta x\\right)  - g \\left( x \\right) }{\\Delta x} \\\\\n&= f'\\left( x \\right) + g'\\left( x \\right)\n\\end{align}\n$$\n\n## 2. The Product Rule\n\nIn calculus, the product rule is a formula used to find the derivatives of products of two or more functions.[2]\n\nGiven: $h\\left( x \\right) = f\\left( x \\right) \\cdot g\\left( x \\right)$\n\nProofs: $h'\\left( x \\right) = f'\\left( x \\right)g\\left( x \\right) + f\\left( x \\right)g'\\left( x \\right)$\n\n$$\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) \\cdot g\\left(x + \\Delta x\\right) - f \\left( x \\right) \\cdot g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) \\cdot g\\left(x + \\Delta x\\right) - \\left[ f\\left(x \\right) \\cdot g\\left(x + \\Delta x \\right) + f\\left(x \\right) \\cdot g\\left(x + \\Delta x \\right)\\right] - f \\left( x \\right) \\cdot g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left[f\\left(x + \\Delta x\\right) -f\\left( x \\right)\\right] \\cdot g\\left(x + \\Delta x\\right) +  f\\left( x \\right) \\cdot \\left[g\\left(x + \\Delta x\\right) -g\\left( x \\right)\\right] }{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) -f\\left( x \\right)}{\\Delta x} \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} g\\left(x + \\Delta x\\right) + \\lim\\limits_{\\Delta x \\rightarrow0} f\\left(x + \\Delta x\\right)  \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ g\\left(x + \\Delta x\\right) -g\\left( x \\right)}{\\Delta x} \\\\\n&= f'\\left( x \\right)g\\left( x \\right) + f\\left( x \\right)g'\\left( x \\right)\n\\end{align}\n$$\n\n\n## 3. The Quotient Rule\n\nIn calculus, the quotient rule is a method of finding the derivative of a function that is the ratio of two differentiable functions.[3]\n\nGiven: $h\\left( x \\right) = \\frac {f\\left( x \\right)} {g\\left( x \\right)}$\n\nProofs: $h'\\left( x \\right) = \\frac{f'\\left( x \\right)g\\left( x \\right) -f\\left( x \\right) g'\\left( x \\right)}{g\\left( x \\right)^2}$\n\n$$\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{\\frac {f\\left( x + \\Delta x \\right)} {g\\left( x + \\Delta x \\right)} - \\frac{f\\left( x \\right)}{g\\left( x \\right)}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{f\\left( x + \\Delta x \\right) g\\left( x \\right) - f\\left( x \\right)g\\left( x + \\Delta x \\right)}{\\Delta x \\cdot g\\left( x \\right)g\\left( x + \\Delta x \\right)}\\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{f\\left( x + \\Delta x \\right) g\\left( x \\right) - f\\left( x \\right)g\\left( x + \\Delta x \\right)}{\\Delta x}  \\cdot \\frac{1}{g\\left( x \\right)g\\left( x + \\Delta x \\right)}\\\\\n&= \\left[\\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) -f\\left( x \\right)}{\\Delta x} \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} g\\left(x\\right) - \\lim\\limits_{\\Delta x \\rightarrow0} f\\left(x \\right)  \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ g\\left(x + \\Delta x\\right) -g\\left( x \\right)}{\\Delta x}\\right] \\cdot \\frac{1}{g\\left( x \\right)^2}\\\\\n&= \\frac{f'\\left( x \\right)g\\left( x \\right) -f\\left( x \\right) g'\\left( x \\right)}{g\\left( x \\right)^2}\n\\end{align}\n$$\n\n## Reference\n[1] [Wikipedia-Sum rule in differentiation](https://en.wikipedia.org/wiki/Sum_rule_in_differentiation)\n\n[2] [Wikipedia-Product rule](https://en.wikipedia.org/wiki/Product_rule)\n\n[3] [Wikipedia-Quotient_rule](https://en.wikipedia.org/wiki/Quotient_rule)\n\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/Calculus/2017-11-02-Differentiation Rules","published":1,"updated":"2018-04-14T19:42:06.491Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hd0033rwtjga4zrzqz","content":"<h1 id=\"Differentiation-Rules\"><a href=\"#Differentiation-Rules\" class=\"headerlink\" title=\"Differentiation Rules\"></a>Differentiation Rules</h1><h2 id=\"1-The-Sum-Rule\"><a href=\"#1-The-Sum-Rule\" class=\"headerlink\" title=\"1. The Sum Rule\"></a>1. The Sum Rule</h2><p>In calculus, the sum rule in differentiation is a method of finding the derivative of a function that is the sum of two other functions for which derivatives exist.[1]</p>\n<p>Given: $h\\left( x \\right) = f\\left( x \\right) + g\\left( x \\right)$</p>\n<p>Proofs: $h’\\left( x \\right) = f’\\left( x \\right) + g’\\left( x \\right)$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) + g\\left(x + \\Delta x\\right) - f \\left( x \\right) - g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right)  - f \\left( x \\right) + g\\left(x + \\Delta x\\right) - g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right)  - f \\left( x \\right) }{\\Delta x} + \\frac{ g\\left(x + \\Delta x\\right)  - g \\left( x \\right) }{\\Delta x} \\\\\n&= f'\\left( x \\right) + g'\\left( x \\right)\n\\end{align}</script><h2 id=\"2-The-Product-Rule\"><a href=\"#2-The-Product-Rule\" class=\"headerlink\" title=\"2. The Product Rule\"></a>2. The Product Rule</h2><p>In calculus, the product rule is a formula used to find the derivatives of products of two or more functions.[2]</p>\n<p>Given: $h\\left( x \\right) = f\\left( x \\right) \\cdot g\\left( x \\right)$</p>\n<p>Proofs: $h’\\left( x \\right) = f’\\left( x \\right)g\\left( x \\right) + f\\left( x \\right)g’\\left( x \\right)$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) \\cdot g\\left(x + \\Delta x\\right) - f \\left( x \\right) \\cdot g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) \\cdot g\\left(x + \\Delta x\\right) - \\left[ f\\left(x \\right) \\cdot g\\left(x + \\Delta x \\right) + f\\left(x \\right) \\cdot g\\left(x + \\Delta x \\right)\\right] - f \\left( x \\right) \\cdot g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left[f\\left(x + \\Delta x\\right) -f\\left( x \\right)\\right] \\cdot g\\left(x + \\Delta x\\right) +  f\\left( x \\right) \\cdot \\left[g\\left(x + \\Delta x\\right) -g\\left( x \\right)\\right] }{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) -f\\left( x \\right)}{\\Delta x} \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} g\\left(x + \\Delta x\\right) + \\lim\\limits_{\\Delta x \\rightarrow0} f\\left(x + \\Delta x\\right)  \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ g\\left(x + \\Delta x\\right) -g\\left( x \\right)}{\\Delta x} \\\\\n&= f'\\left( x \\right)g\\left( x \\right) + f\\left( x \\right)g'\\left( x \\right)\n\\end{align}</script><h2 id=\"3-The-Quotient-Rule\"><a href=\"#3-The-Quotient-Rule\" class=\"headerlink\" title=\"3. The Quotient Rule\"></a>3. The Quotient Rule</h2><p>In calculus, the quotient rule is a method of finding the derivative of a function that is the ratio of two differentiable functions.[3]</p>\n<p>Given: $h\\left( x \\right) = \\frac {f\\left( x \\right)} {g\\left( x \\right)}$</p>\n<p>Proofs: $h’\\left( x \\right) = \\frac{f’\\left( x \\right)g\\left( x \\right) -f\\left( x \\right) g’\\left( x \\right)}{g\\left( x \\right)^2}$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{\\frac {f\\left( x + \\Delta x \\right)} {g\\left( x + \\Delta x \\right)} - \\frac{f\\left( x \\right)}{g\\left( x \\right)}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{f\\left( x + \\Delta x \\right) g\\left( x \\right) - f\\left( x \\right)g\\left( x + \\Delta x \\right)}{\\Delta x \\cdot g\\left( x \\right)g\\left( x + \\Delta x \\right)}\\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{f\\left( x + \\Delta x \\right) g\\left( x \\right) - f\\left( x \\right)g\\left( x + \\Delta x \\right)}{\\Delta x}  \\cdot \\frac{1}{g\\left( x \\right)g\\left( x + \\Delta x \\right)}\\\\\n&= \\left[\\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) -f\\left( x \\right)}{\\Delta x} \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} g\\left(x\\right) - \\lim\\limits_{\\Delta x \\rightarrow0} f\\left(x \\right)  \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ g\\left(x + \\Delta x\\right) -g\\left( x \\right)}{\\Delta x}\\right] \\cdot \\frac{1}{g\\left( x \\right)^2}\\\\\n&= \\frac{f'\\left( x \\right)g\\left( x \\right) -f\\left( x \\right) g'\\left( x \\right)}{g\\left( x \\right)^2}\n\\end{align}</script><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] <a href=\"https://en.wikipedia.org/wiki/Sum_rule_in_differentiation\" target=\"_blank\" rel=\"external\">Wikipedia-Sum rule in differentiation</a></p>\n<p>[2] <a href=\"https://en.wikipedia.org/wiki/Product_rule\" target=\"_blank\" rel=\"external\">Wikipedia-Product rule</a></p>\n<p>[3] <a href=\"https://en.wikipedia.org/wiki/Quotient_rule\" target=\"_blank\" rel=\"external\">Wikipedia-Quotient_rule</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Differentiation-Rules\"><a href=\"#Differentiation-Rules\" class=\"headerlink\" title=\"Differentiation Rules\"></a>Differentiation Rules</h1><h2 id=\"1-The-Sum-Rule\"><a href=\"#1-The-Sum-Rule\" class=\"headerlink\" title=\"1. The Sum Rule\"></a>1. The Sum Rule</h2><p>In calculus, the sum rule in differentiation is a method of finding the derivative of a function that is the sum of two other functions for which derivatives exist.[1]</p>\n<p>Given: $h\\left( x \\right) = f\\left( x \\right) + g\\left( x \\right)$</p>\n<p>Proofs: $h’\\left( x \\right) = f’\\left( x \\right) + g’\\left( x \\right)$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) + g\\left(x + \\Delta x\\right) - f \\left( x \\right) - g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right)  - f \\left( x \\right) + g\\left(x + \\Delta x\\right) - g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right)  - f \\left( x \\right) }{\\Delta x} + \\frac{ g\\left(x + \\Delta x\\right)  - g \\left( x \\right) }{\\Delta x} \\\\\n&= f'\\left( x \\right) + g'\\left( x \\right)\n\\end{align}</script><h2 id=\"2-The-Product-Rule\"><a href=\"#2-The-Product-Rule\" class=\"headerlink\" title=\"2. The Product Rule\"></a>2. The Product Rule</h2><p>In calculus, the product rule is a formula used to find the derivatives of products of two or more functions.[2]</p>\n<p>Given: $h\\left( x \\right) = f\\left( x \\right) \\cdot g\\left( x \\right)$</p>\n<p>Proofs: $h’\\left( x \\right) = f’\\left( x \\right)g\\left( x \\right) + f\\left( x \\right)g’\\left( x \\right)$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) \\cdot g\\left(x + \\Delta x\\right) - f \\left( x \\right) \\cdot g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) \\cdot g\\left(x + \\Delta x\\right) - \\left[ f\\left(x \\right) \\cdot g\\left(x + \\Delta x \\right) + f\\left(x \\right) \\cdot g\\left(x + \\Delta x \\right)\\right] - f \\left( x \\right) \\cdot g \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left[f\\left(x + \\Delta x\\right) -f\\left( x \\right)\\right] \\cdot g\\left(x + \\Delta x\\right) +  f\\left( x \\right) \\cdot \\left[g\\left(x + \\Delta x\\right) -g\\left( x \\right)\\right] }{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) -f\\left( x \\right)}{\\Delta x} \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} g\\left(x + \\Delta x\\right) + \\lim\\limits_{\\Delta x \\rightarrow0} f\\left(x + \\Delta x\\right)  \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ g\\left(x + \\Delta x\\right) -g\\left( x \\right)}{\\Delta x} \\\\\n&= f'\\left( x \\right)g\\left( x \\right) + f\\left( x \\right)g'\\left( x \\right)\n\\end{align}</script><h2 id=\"3-The-Quotient-Rule\"><a href=\"#3-The-Quotient-Rule\" class=\"headerlink\" title=\"3. The Quotient Rule\"></a>3. The Quotient Rule</h2><p>In calculus, the quotient rule is a method of finding the derivative of a function that is the ratio of two differentiable functions.[3]</p>\n<p>Given: $h\\left( x \\right) = \\frac {f\\left( x \\right)} {g\\left( x \\right)}$</p>\n<p>Proofs: $h’\\left( x \\right) = \\frac{f’\\left( x \\right)g\\left( x \\right) -f\\left( x \\right) g’\\left( x \\right)}{g\\left( x \\right)^2}$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nh'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ h\\left(x + \\Delta x\\right) - h \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{\\frac {f\\left( x + \\Delta x \\right)} {g\\left( x + \\Delta x \\right)} - \\frac{f\\left( x \\right)}{g\\left( x \\right)}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{f\\left( x + \\Delta x \\right) g\\left( x \\right) - f\\left( x \\right)g\\left( x + \\Delta x \\right)}{\\Delta x \\cdot g\\left( x \\right)g\\left( x + \\Delta x \\right)}\\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{f\\left( x + \\Delta x \\right) g\\left( x \\right) - f\\left( x \\right)g\\left( x + \\Delta x \\right)}{\\Delta x}  \\cdot \\frac{1}{g\\left( x \\right)g\\left( x + \\Delta x \\right)}\\\\\n&= \\left[\\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) -f\\left( x \\right)}{\\Delta x} \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} g\\left(x\\right) - \\lim\\limits_{\\Delta x \\rightarrow0} f\\left(x \\right)  \\cdot \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ g\\left(x + \\Delta x\\right) -g\\left( x \\right)}{\\Delta x}\\right] \\cdot \\frac{1}{g\\left( x \\right)^2}\\\\\n&= \\frac{f'\\left( x \\right)g\\left( x \\right) -f\\left( x \\right) g'\\left( x \\right)}{g\\left( x \\right)^2}\n\\end{align}</script><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] <a href=\"https://en.wikipedia.org/wiki/Sum_rule_in_differentiation\" target=\"_blank\" rel=\"external\">Wikipedia-Sum rule in differentiation</a></p>\n<p>[2] <a href=\"https://en.wikipedia.org/wiki/Product_rule\" target=\"_blank\" rel=\"external\">Wikipedia-Product rule</a></p>\n<p>[3] <a href=\"https://en.wikipedia.org/wiki/Quotient_rule\" target=\"_blank\" rel=\"external\">Wikipedia-Quotient_rule</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"Batch Gradient Descent","date":"2017-11-30T09:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Batch Gradient Descent\n\n> We use linear regression as example to explain this optimization algorithm.\n\n## 1. Formula\n\n### 1.1. Cost Function\n\n> We prefer residual sum of squared to evaluate linear regression.\n\n$$\n\\begin{align}\nJ(\\theta) &= \\frac{1}{2m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] ^ 2\n\\end{align}\n$$\n\n### 1.2. Visualize Cost Function\n\n\n> E.g. 1 :\n\none parameter only $\\theta_1$ --> $h_{\\theta}(x) = \\theta_1 x_1$\n\n![Learning Curve 1](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b970141d5bbebfd8dfe3f11a17536afea6de3b48/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%201.png)\n<center> 1. Learning Curve 1 <sup>[1]</sup> </center>\n<br>\n\n> E.g. 2 :\n\ntwo parameters $\\theta_0, \\theta_1$ --> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$\n\n![Learning Curve 2](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b970141d5bbebfd8dfe3f11a17536afea6de3b48/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%202.png)\n<center> 2. Learning Curve 2 <sup>[2]</sup> </center>\n<br>\n\nSwitch to contour plot\n\n![Learning Curve 2 - contour](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c3b875d12b29e0a7e4936f49a5529857be0f9474/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%203.png)\n<center> 3. Learning Curve 2 - contour<sup>[2]</sup> </center>\n<br>\n\n\n### 1.3. Gradient Descent Formula\n\nFor all $\\theta_i$\n$$\n\\begin{align}\n\\frac{\\partial J_\\theta}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_i)\n\\end{align}\n$$\n\n> E.g.,\ntwo parameters $\\theta_0, \\theta_1$ --> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$\n\nFor i = 0 :\n$$\n\\frac{\\partial J_\\theta}{\\partial \\theta_0} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_0)\n$$\n\n\nFor i = 1:\n$$\n\\frac{\\partial J_\\theta}{\\partial \\theta_1} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]\\cdot (x_1)\n$$\n\n```\n% Octave\n%% =================== Gradient Descent ===================\n% Add a column(x0) of ones to X\n\nX = [ones(len, 1), data(:,1)];\ntheta = zeros(2, 1);\nalpha = 0.01;\nITERATION = 1500;\njTheta = zeros(ITERATION, 1);\n\nfor iter = 1:ITERATION\n    % Perform a single gradient descent on the parameter vector\n    % Note: since the theta will be updated, a tempTheta is needed to store the data.\n    tempTheta = theta;\n    theta(1) = theta(1) - (alpha / len) * (sum(X * tempTheta - Y));  % ignore the X(:,1) since the values are all ones.\n    theta(2) = theta(2) - (alpha / len) * (sum((X * tempTheta - Y) .* X(:,2)));\n\n    %% =================== Compute Cost ===================\n    jTheta(iter) = sum((X * theta - Y) .^ 2) / (2 * len);\nendfor\n```\n\n## 2. Algorithm\nFor all $\\theta_i$\n$$\n\\begin{align}\n\\theta_i := \\theta_i - \\alpha \\frac{\\partial}{\\partial \\theta_i} J(\\theta_1, \\theta_2, \\dots ,\\theta_n)\n\\end{align}\n$$\n\n> E.g.,\ntwo parameters $\\theta_0, \\theta_1$ --> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$\n\nFor i = 0 :\n$$\n\\begin{align}\n\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]\n\\end{align}\n$$\n\nFor i = 1 :\n$$\n\\begin{align}\n\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]  \\cdot (x_1)\n\\end{align}\n$$\n\n\nIterative for multiple times (depends on data content, data size and step size). Finally, we could see the result as below.\n\n\n![Converge](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/1409d8d48185b2e1dc42315ed56bd61e340b3831/__Blog/__Personal%20Understanding/_archive/_images/Batch%20Gradient%20Descent%20-%20visulize%20converge.jpg)\nVisualize Convergence\n\n## 3. Analyze\n\n\n| Pros                                         | Cons                      |\n| -------------------------------------------- | ------------------------- |\n| Controllable by manuplate stepsize, datasize | Computing effort is large |\n| Easy to program                              |                           |\n\n\n\n## 4. How to Choose Step Size?\n\nChoose an approriate step size is significant. If the step size is too small, it doesn't hurt the result, but it took even more times to converge. If the step size is too large, it may cause the algorithm diverge (not converge).\n\nThe graph below shows that the value is not converge since the step size is too big.\n\n![Large Step Size](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/1409d8d48185b2e1dc42315ed56bd61e340b3831/__Blog/__Personal%20Understanding/_archive/_images/Batch%20Gradient%20Descent%20-%20large%20step%20size.jpg)\nLarge Step Size\n\n\nThe best way, as far as I know, is to decrease the step size according to the iteration times.\n\nE.g.,\n\n$\\alpha^{(t+1)} = \\frac{\\alpha^{t}}{t}$ \n\nor\n\n$\\alpha^{(t+1)} = \\frac{\\alpha^{t}}{\\sqrt t}$\n\n# Reference\n1. 机器学习基石(台湾大学-林轩田)\\lecture_slides-09_handout.pdf\n\n2. Coursera-Standard Ford CS229: Machine Learning - Andrew Ng\n","source":"_posts/Algorithm/Optimize Algorithm/2017-11-30-Batch Gradient Descent.md","raw":"---\ntitle: Batch Gradient Descent\ndate: 2017-11-30 17:23:19\ncategories: [Machine-Learning-Algorithm]\ntags: [Machine-Learning-Algorithm]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Batch Gradient Descent\n\n> We use linear regression as example to explain this optimization algorithm.\n\n## 1. Formula\n\n### 1.1. Cost Function\n\n> We prefer residual sum of squared to evaluate linear regression.\n\n$$\n\\begin{align}\nJ(\\theta) &= \\frac{1}{2m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] ^ 2\n\\end{align}\n$$\n\n### 1.2. Visualize Cost Function\n\n\n> E.g. 1 :\n\none parameter only $\\theta_1$ --> $h_{\\theta}(x) = \\theta_1 x_1$\n\n![Learning Curve 1](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b970141d5bbebfd8dfe3f11a17536afea6de3b48/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%201.png)\n<center> 1. Learning Curve 1 <sup>[1]</sup> </center>\n<br>\n\n> E.g. 2 :\n\ntwo parameters $\\theta_0, \\theta_1$ --> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$\n\n![Learning Curve 2](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b970141d5bbebfd8dfe3f11a17536afea6de3b48/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%202.png)\n<center> 2. Learning Curve 2 <sup>[2]</sup> </center>\n<br>\n\nSwitch to contour plot\n\n![Learning Curve 2 - contour](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c3b875d12b29e0a7e4936f49a5529857be0f9474/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%203.png)\n<center> 3. Learning Curve 2 - contour<sup>[2]</sup> </center>\n<br>\n\n\n### 1.3. Gradient Descent Formula\n\nFor all $\\theta_i$\n$$\n\\begin{align}\n\\frac{\\partial J_\\theta}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_i)\n\\end{align}\n$$\n\n> E.g.,\ntwo parameters $\\theta_0, \\theta_1$ --> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$\n\nFor i = 0 :\n$$\n\\frac{\\partial J_\\theta}{\\partial \\theta_0} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_0)\n$$\n\n\nFor i = 1:\n$$\n\\frac{\\partial J_\\theta}{\\partial \\theta_1} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]\\cdot (x_1)\n$$\n\n```\n% Octave\n%% =================== Gradient Descent ===================\n% Add a column(x0) of ones to X\n\nX = [ones(len, 1), data(:,1)];\ntheta = zeros(2, 1);\nalpha = 0.01;\nITERATION = 1500;\njTheta = zeros(ITERATION, 1);\n\nfor iter = 1:ITERATION\n    % Perform a single gradient descent on the parameter vector\n    % Note: since the theta will be updated, a tempTheta is needed to store the data.\n    tempTheta = theta;\n    theta(1) = theta(1) - (alpha / len) * (sum(X * tempTheta - Y));  % ignore the X(:,1) since the values are all ones.\n    theta(2) = theta(2) - (alpha / len) * (sum((X * tempTheta - Y) .* X(:,2)));\n\n    %% =================== Compute Cost ===================\n    jTheta(iter) = sum((X * theta - Y) .^ 2) / (2 * len);\nendfor\n```\n\n## 2. Algorithm\nFor all $\\theta_i$\n$$\n\\begin{align}\n\\theta_i := \\theta_i - \\alpha \\frac{\\partial}{\\partial \\theta_i} J(\\theta_1, \\theta_2, \\dots ,\\theta_n)\n\\end{align}\n$$\n\n> E.g.,\ntwo parameters $\\theta_0, \\theta_1$ --> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$\n\nFor i = 0 :\n$$\n\\begin{align}\n\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]\n\\end{align}\n$$\n\nFor i = 1 :\n$$\n\\begin{align}\n\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]  \\cdot (x_1)\n\\end{align}\n$$\n\n\nIterative for multiple times (depends on data content, data size and step size). Finally, we could see the result as below.\n\n\n![Converge](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/1409d8d48185b2e1dc42315ed56bd61e340b3831/__Blog/__Personal%20Understanding/_archive/_images/Batch%20Gradient%20Descent%20-%20visulize%20converge.jpg)\nVisualize Convergence\n\n## 3. Analyze\n\n\n| Pros                                         | Cons                      |\n| -------------------------------------------- | ------------------------- |\n| Controllable by manuplate stepsize, datasize | Computing effort is large |\n| Easy to program                              |                           |\n\n\n\n## 4. How to Choose Step Size?\n\nChoose an approriate step size is significant. If the step size is too small, it doesn't hurt the result, but it took even more times to converge. If the step size is too large, it may cause the algorithm diverge (not converge).\n\nThe graph below shows that the value is not converge since the step size is too big.\n\n![Large Step Size](https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/1409d8d48185b2e1dc42315ed56bd61e340b3831/__Blog/__Personal%20Understanding/_archive/_images/Batch%20Gradient%20Descent%20-%20large%20step%20size.jpg)\nLarge Step Size\n\n\nThe best way, as far as I know, is to decrease the step size according to the iteration times.\n\nE.g.,\n\n$\\alpha^{(t+1)} = \\frac{\\alpha^{t}}{t}$ \n\nor\n\n$\\alpha^{(t+1)} = \\frac{\\alpha^{t}}{\\sqrt t}$\n\n# Reference\n1. 机器学习基石(台湾大学-林轩田)\\lecture_slides-09_handout.pdf\n\n2. Coursera-Standard Ford CS229: Machine Learning - Andrew Ng\n","slug":"Algorithm/Optimize Algorithm/2017-11-30-Batch Gradient Descent","published":1,"updated":"2018-04-14T19:42:06.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2he0034rwtjxbuihxi9","content":"<h1 id=\"Batch-Gradient-Descent\"><a href=\"#Batch-Gradient-Descent\" class=\"headerlink\" title=\"Batch Gradient Descent\"></a>Batch Gradient Descent</h1><blockquote>\n<p>We use linear regression as example to explain this optimization algorithm.</p>\n</blockquote>\n<h2 id=\"1-Formula\"><a href=\"#1-Formula\" class=\"headerlink\" title=\"1. Formula\"></a>1. Formula</h2><h3 id=\"1-1-Cost-Function\"><a href=\"#1-1-Cost-Function\" class=\"headerlink\" title=\"1.1. Cost Function\"></a>1.1. Cost Function</h3><blockquote>\n<p>We prefer residual sum of squared to evaluate linear regression.</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nJ(\\theta) &= \\frac{1}{2m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] ^ 2\n\\end{align}</script><h3 id=\"1-2-Visualize-Cost-Function\"><a href=\"#1-2-Visualize-Cost-Function\" class=\"headerlink\" title=\"1.2. Visualize Cost Function\"></a>1.2. Visualize Cost Function</h3><blockquote>\n<p>E.g. 1 :</p>\n</blockquote>\n<p>one parameter only $\\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b970141d5bbebfd8dfe3f11a17536afea6de3b48/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%201.png\" alt=\"Learning Curve 1\"></p>\n<p><center> 1. Learning Curve 1 <sup>[1]</sup> </center><br><br></p>\n<blockquote>\n<p>E.g. 2 :</p>\n</blockquote>\n<p>two parameters $\\theta_0, \\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b970141d5bbebfd8dfe3f11a17536afea6de3b48/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%202.png\" alt=\"Learning Curve 2\"></p>\n<p><center> 2. Learning Curve 2 <sup>[2]</sup> </center><br><br></p>\n<p>Switch to contour plot</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c3b875d12b29e0a7e4936f49a5529857be0f9474/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%203.png\" alt=\"Learning Curve 2 - contour\"></p>\n<p><center> 3. Learning Curve 2 - contour<sup>[2]</sup> </center><br><br></p>\n<h3 id=\"1-3-Gradient-Descent-Formula\"><a href=\"#1-3-Gradient-Descent-Formula\" class=\"headerlink\" title=\"1.3. Gradient Descent Formula\"></a>1.3. Gradient Descent Formula</h3><p>For all $\\theta_i$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\frac{\\partial J_\\theta}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_i)\n\\end{align}</script><blockquote>\n<p>E.g.,<br>two parameters $\\theta_0, \\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$</p>\n</blockquote>\n<p>For i = 0 :</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_\\theta}{\\partial \\theta_0} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_0)</script><p>For i = 1:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_\\theta}{\\partial \\theta_1} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]\\cdot (x_1)</script><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">% Octave</div><div class=\"line\">%% =================== Gradient Descent ===================</div><div class=\"line\">% Add a column(x0) of ones to X</div><div class=\"line\"></div><div class=\"line\">X = [ones(len, 1), data(:,1)];</div><div class=\"line\">theta = zeros(2, 1);</div><div class=\"line\">alpha = 0.01;</div><div class=\"line\">ITERATION = 1500;</div><div class=\"line\">jTheta = zeros(ITERATION, 1);</div><div class=\"line\"></div><div class=\"line\">for iter = 1:ITERATION</div><div class=\"line\">    % Perform a single gradient descent on the parameter vector</div><div class=\"line\">    % Note: since the theta will be updated, a tempTheta is needed to store the data.</div><div class=\"line\">    tempTheta = theta;</div><div class=\"line\">    theta(1) = theta(1) - (alpha / len) * (sum(X * tempTheta - Y));  % ignore the X(:,1) since the values are all ones.</div><div class=\"line\">    theta(2) = theta(2) - (alpha / len) * (sum((X * tempTheta - Y) .* X(:,2)));</div><div class=\"line\"></div><div class=\"line\">    %% =================== Compute Cost ===================</div><div class=\"line\">    jTheta(iter) = sum((X * theta - Y) .^ 2) / (2 * len);</div><div class=\"line\">endfor</div></pre></td></tr></table></figure>\n<h2 id=\"2-Algorithm\"><a href=\"#2-Algorithm\" class=\"headerlink\" title=\"2. Algorithm\"></a>2. Algorithm</h2><p>For all $\\theta_i$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\theta_i := \\theta_i - \\alpha \\frac{\\partial}{\\partial \\theta_i} J(\\theta_1, \\theta_2, \\dots ,\\theta_n)\n\\end{align}</script><blockquote>\n<p>E.g.,<br>two parameters $\\theta_0, \\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$</p>\n</blockquote>\n<p>For i = 0 :</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]\n\\end{align}</script><p>For i = 1 :</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]  \\cdot (x_1)\n\\end{align}</script><p>Iterative for multiple times (depends on data content, data size and step size). Finally, we could see the result as below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/1409d8d48185b2e1dc42315ed56bd61e340b3831/__Blog/__Personal%20Understanding/_archive/_images/Batch%20Gradient%20Descent%20-%20visulize%20converge.jpg\" alt=\"Converge\"><br>Visualize Convergence</p>\n<h2 id=\"3-Analyze\"><a href=\"#3-Analyze\" class=\"headerlink\" title=\"3. Analyze\"></a>3. Analyze</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Controllable by manuplate stepsize, datasize</td>\n<td>Computing effort is large</td>\n</tr>\n<tr>\n<td>Easy to program</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"4-How-to-Choose-Step-Size\"><a href=\"#4-How-to-Choose-Step-Size\" class=\"headerlink\" title=\"4. How to Choose Step Size?\"></a>4. How to Choose Step Size?</h2><p>Choose an approriate step size is significant. If the step size is too small, it doesn’t hurt the result, but it took even more times to converge. If the step size is too large, it may cause the algorithm diverge (not converge).</p>\n<p>The graph below shows that the value is not converge since the step size is too big.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/1409d8d48185b2e1dc42315ed56bd61e340b3831/__Blog/__Personal%20Understanding/_archive/_images/Batch%20Gradient%20Descent%20-%20large%20step%20size.jpg\" alt=\"Large Step Size\"><br>Large Step Size</p>\n<p>The best way, as far as I know, is to decrease the step size according to the iteration times.</p>\n<p>E.g.,</p>\n<p>$\\alpha^{(t+1)} = \\frac{\\alpha^{t}}{t}$ </p>\n<p>or</p>\n<p>$\\alpha^{(t+1)} = \\frac{\\alpha^{t}}{\\sqrt t}$</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li><p>机器学习基石(台湾大学-林轩田)\\lecture_slides-09_handout.pdf</p>\n</li>\n<li><p>Coursera-Standard Ford CS229: Machine Learning - Andrew Ng</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Batch-Gradient-Descent\"><a href=\"#Batch-Gradient-Descent\" class=\"headerlink\" title=\"Batch Gradient Descent\"></a>Batch Gradient Descent</h1><blockquote>\n<p>We use linear regression as example to explain this optimization algorithm.</p>\n</blockquote>\n<h2 id=\"1-Formula\"><a href=\"#1-Formula\" class=\"headerlink\" title=\"1. Formula\"></a>1. Formula</h2><h3 id=\"1-1-Cost-Function\"><a href=\"#1-1-Cost-Function\" class=\"headerlink\" title=\"1.1. Cost Function\"></a>1.1. Cost Function</h3><blockquote>\n<p>We prefer residual sum of squared to evaluate linear regression.</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nJ(\\theta) &= \\frac{1}{2m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] ^ 2\n\\end{align}</script><h3 id=\"1-2-Visualize-Cost-Function\"><a href=\"#1-2-Visualize-Cost-Function\" class=\"headerlink\" title=\"1.2. Visualize Cost Function\"></a>1.2. Visualize Cost Function</h3><blockquote>\n<p>E.g. 1 :</p>\n</blockquote>\n<p>one parameter only $\\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b970141d5bbebfd8dfe3f11a17536afea6de3b48/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%201.png\" alt=\"Learning Curve 1\"></p>\n<p><center> 1. Learning Curve 1 <sup>[1]</sup> </center><br><br></p>\n<blockquote>\n<p>E.g. 2 :</p>\n</blockquote>\n<p>two parameters $\\theta_0, \\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b970141d5bbebfd8dfe3f11a17536afea6de3b48/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%202.png\" alt=\"Learning Curve 2\"></p>\n<p><center> 2. Learning Curve 2 <sup>[2]</sup> </center><br><br></p>\n<p>Switch to contour plot</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/c3b875d12b29e0a7e4936f49a5529857be0f9474/__Blog/__Personal%20Understanding/Algorithm/Supervised%20Learning/linear%20model/images/1.Linear%20Model%203.png\" alt=\"Learning Curve 2 - contour\"></p>\n<p><center> 3. Learning Curve 2 - contour<sup>[2]</sup> </center><br><br></p>\n<h3 id=\"1-3-Gradient-Descent-Formula\"><a href=\"#1-3-Gradient-Descent-Formula\" class=\"headerlink\" title=\"1.3. Gradient Descent Formula\"></a>1.3. Gradient Descent Formula</h3><p>For all $\\theta_i$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\frac{\\partial J_\\theta}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_i)\n\\end{align}</script><blockquote>\n<p>E.g.,<br>two parameters $\\theta_0, \\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$</p>\n</blockquote>\n<p>For i = 0 :</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_\\theta}{\\partial \\theta_0} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_0)</script><p>For i = 1:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_\\theta}{\\partial \\theta_1} = \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]\\cdot (x_1)</script><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">% Octave</div><div class=\"line\">%% =================== Gradient Descent ===================</div><div class=\"line\">% Add a column(x0) of ones to X</div><div class=\"line\"></div><div class=\"line\">X = [ones(len, 1), data(:,1)];</div><div class=\"line\">theta = zeros(2, 1);</div><div class=\"line\">alpha = 0.01;</div><div class=\"line\">ITERATION = 1500;</div><div class=\"line\">jTheta = zeros(ITERATION, 1);</div><div class=\"line\"></div><div class=\"line\">for iter = 1:ITERATION</div><div class=\"line\">    % Perform a single gradient descent on the parameter vector</div><div class=\"line\">    % Note: since the theta will be updated, a tempTheta is needed to store the data.</div><div class=\"line\">    tempTheta = theta;</div><div class=\"line\">    theta(1) = theta(1) - (alpha / len) * (sum(X * tempTheta - Y));  % ignore the X(:,1) since the values are all ones.</div><div class=\"line\">    theta(2) = theta(2) - (alpha / len) * (sum((X * tempTheta - Y) .* X(:,2)));</div><div class=\"line\"></div><div class=\"line\">    %% =================== Compute Cost ===================</div><div class=\"line\">    jTheta(iter) = sum((X * theta - Y) .^ 2) / (2 * len);</div><div class=\"line\">endfor</div></pre></td></tr></table></figure>\n<h2 id=\"2-Algorithm\"><a href=\"#2-Algorithm\" class=\"headerlink\" title=\"2. Algorithm\"></a>2. Algorithm</h2><p>For all $\\theta_i$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\theta_i := \\theta_i - \\alpha \\frac{\\partial}{\\partial \\theta_i} J(\\theta_1, \\theta_2, \\dots ,\\theta_n)\n\\end{align}</script><blockquote>\n<p>E.g.,<br>two parameters $\\theta_0, \\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$</p>\n</blockquote>\n<p>For i = 0 :</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]\n\\end{align}</script><p>For i = 1 :</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{n} \\left[ h_{\\theta}(x_i) - y_i \\right]  \\cdot (x_1)\n\\end{align}</script><p>Iterative for multiple times (depends on data content, data size and step size). Finally, we could see the result as below.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/1409d8d48185b2e1dc42315ed56bd61e340b3831/__Blog/__Personal%20Understanding/_archive/_images/Batch%20Gradient%20Descent%20-%20visulize%20converge.jpg\" alt=\"Converge\"><br>Visualize Convergence</p>\n<h2 id=\"3-Analyze\"><a href=\"#3-Analyze\" class=\"headerlink\" title=\"3. Analyze\"></a>3. Analyze</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Controllable by manuplate stepsize, datasize</td>\n<td>Computing effort is large</td>\n</tr>\n<tr>\n<td>Easy to program</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"4-How-to-Choose-Step-Size\"><a href=\"#4-How-to-Choose-Step-Size\" class=\"headerlink\" title=\"4. How to Choose Step Size?\"></a>4. How to Choose Step Size?</h2><p>Choose an approriate step size is significant. If the step size is too small, it doesn’t hurt the result, but it took even more times to converge. If the step size is too large, it may cause the algorithm diverge (not converge).</p>\n<p>The graph below shows that the value is not converge since the step size is too big.</p>\n<p><img src=\"https://raw.githubusercontent.com/zhichengMLE/MarkdownPhoto/1409d8d48185b2e1dc42315ed56bd61e340b3831/__Blog/__Personal%20Understanding/_archive/_images/Batch%20Gradient%20Descent%20-%20large%20step%20size.jpg\" alt=\"Large Step Size\"><br>Large Step Size</p>\n<p>The best way, as far as I know, is to decrease the step size according to the iteration times.</p>\n<p>E.g.,</p>\n<p>$\\alpha^{(t+1)} = \\frac{\\alpha^{t}}{t}$ </p>\n<p>or</p>\n<p>$\\alpha^{(t+1)} = \\frac{\\alpha^{t}}{\\sqrt t}$</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li><p>机器学习基石(台湾大学-林轩田)\\lecture_slides-09_handout.pdf</p>\n</li>\n<li><p>Coursera-Standard Ford CS229: Machine Learning - Andrew Ng</p>\n</li>\n</ol>\n"},{"title":"Mini-Batch Gradient Descent","date":"2017-12-12T05:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n\n# Mini-Batch Gradient Descent\n\n## 1. What is Mini-Batch Gradient Descent?\n\nMini-Batch Gradient Descent is an algorithm between the Batch Gradient Descent and Stochastic Gradient Descent. Concretly, this use some(not one or all) examples(M) for each iteration.\n\n\n## 2. Compute Effort\n\nThe compute time of this algorithm depends on the examples. It not stable, but the worst case is like Batch Gradient Descent: O($N^2$)\n\nThe table below shows the different among these there Gradient Descent\n\n| Batch Gradient Descent          | Mini-Batch Gradient Descent | Stochastic Gradient Descent       |\n| ------------------------------- | --------------------------- | --------------------------------- |\n| use 1 example in each iteration | use some examples           | use all example in each iteration |\n| relative compute loose          | somewhat in between         | relative compute intensive        |\n\n\n\n### 3. Gradient Descent Formula\n\nFor all $\\theta_i$\n$$\n\\begin{align}\n\\frac{\\partial J_\\theta}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_i)\n\\end{align}\n$$\n\n> E.g.,\ntwo parameters $\\theta_0, \\theta_1$ --> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$\n\nFor i = 0 :\n$$\n\\frac{\\partial J_\\theta}{\\partial \\theta_0} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_0)\n$$\n\n\nFor i = 1:\n$$\n\\frac{\\partial J_\\theta}{\\partial \\theta_1} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right]\\cdot (x_1)\n$$\n\n\nNote that the datasets need to be shuffled before iteration.\n","source":"_posts/Algorithm/Optimize Algorithm/2017-12-12-Mini-Batch Gradient Descent.md","raw":"---\ntitle:  Mini-Batch Gradient Descent\ndate: 2017-12-12 13:23:19\ncategories: [Machine-Learning-Algorithm]\ntags: [Machine-Learning-Algorithm]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n\n# Mini-Batch Gradient Descent\n\n## 1. What is Mini-Batch Gradient Descent?\n\nMini-Batch Gradient Descent is an algorithm between the Batch Gradient Descent and Stochastic Gradient Descent. Concretly, this use some(not one or all) examples(M) for each iteration.\n\n\n## 2. Compute Effort\n\nThe compute time of this algorithm depends on the examples. It not stable, but the worst case is like Batch Gradient Descent: O($N^2$)\n\nThe table below shows the different among these there Gradient Descent\n\n| Batch Gradient Descent          | Mini-Batch Gradient Descent | Stochastic Gradient Descent       |\n| ------------------------------- | --------------------------- | --------------------------------- |\n| use 1 example in each iteration | use some examples           | use all example in each iteration |\n| relative compute loose          | somewhat in between         | relative compute intensive        |\n\n\n\n### 3. Gradient Descent Formula\n\nFor all $\\theta_i$\n$$\n\\begin{align}\n\\frac{\\partial J_\\theta}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_i)\n\\end{align}\n$$\n\n> E.g.,\ntwo parameters $\\theta_0, \\theta_1$ --> $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$\n\nFor i = 0 :\n$$\n\\frac{\\partial J_\\theta}{\\partial \\theta_0} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_0)\n$$\n\n\nFor i = 1:\n$$\n\\frac{\\partial J_\\theta}{\\partial \\theta_1} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right]\\cdot (x_1)\n$$\n\n\nNote that the datasets need to be shuffled before iteration.\n","slug":"Algorithm/Optimize Algorithm/2017-12-12-Mini-Batch Gradient Descent","published":1,"updated":"2018-04-14T19:42:06.484Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hh0038rwtjz3v4kbgt","content":"<h1 id=\"Mini-Batch-Gradient-Descent\"><a href=\"#Mini-Batch-Gradient-Descent\" class=\"headerlink\" title=\"Mini-Batch Gradient Descent\"></a>Mini-Batch Gradient Descent</h1><h2 id=\"1-What-is-Mini-Batch-Gradient-Descent\"><a href=\"#1-What-is-Mini-Batch-Gradient-Descent\" class=\"headerlink\" title=\"1. What is Mini-Batch Gradient Descent?\"></a>1. What is Mini-Batch Gradient Descent?</h2><p>Mini-Batch Gradient Descent is an algorithm between the Batch Gradient Descent and Stochastic Gradient Descent. Concretly, this use some(not one or all) examples(M) for each iteration.</p>\n<h2 id=\"2-Compute-Effort\"><a href=\"#2-Compute-Effort\" class=\"headerlink\" title=\"2. Compute Effort\"></a>2. Compute Effort</h2><p>The compute time of this algorithm depends on the examples. It not stable, but the worst case is like Batch Gradient Descent: O($N^2$)</p>\n<p>The table below shows the different among these there Gradient Descent</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Batch Gradient Descent</th>\n<th>Mini-Batch Gradient Descent</th>\n<th>Stochastic Gradient Descent</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>use 1 example in each iteration</td>\n<td>use some examples</td>\n<td>use all example in each iteration</td>\n</tr>\n<tr>\n<td>relative compute loose</td>\n<td>somewhat in between</td>\n<td>relative compute intensive</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"3-Gradient-Descent-Formula\"><a href=\"#3-Gradient-Descent-Formula\" class=\"headerlink\" title=\"3. Gradient Descent Formula\"></a>3. Gradient Descent Formula</h3><p>For all $\\theta_i$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\frac{\\partial J_\\theta}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_i)\n\\end{align}</script><blockquote>\n<p>E.g.,<br>two parameters $\\theta_0, \\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$</p>\n</blockquote>\n<p>For i = 0 :</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_\\theta}{\\partial \\theta_0} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_0)</script><p>For i = 1:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_\\theta}{\\partial \\theta_1} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right]\\cdot (x_1)</script><p>Note that the datasets need to be shuffled before iteration.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Mini-Batch-Gradient-Descent\"><a href=\"#Mini-Batch-Gradient-Descent\" class=\"headerlink\" title=\"Mini-Batch Gradient Descent\"></a>Mini-Batch Gradient Descent</h1><h2 id=\"1-What-is-Mini-Batch-Gradient-Descent\"><a href=\"#1-What-is-Mini-Batch-Gradient-Descent\" class=\"headerlink\" title=\"1. What is Mini-Batch Gradient Descent?\"></a>1. What is Mini-Batch Gradient Descent?</h2><p>Mini-Batch Gradient Descent is an algorithm between the Batch Gradient Descent and Stochastic Gradient Descent. Concretly, this use some(not one or all) examples(M) for each iteration.</p>\n<h2 id=\"2-Compute-Effort\"><a href=\"#2-Compute-Effort\" class=\"headerlink\" title=\"2. Compute Effort\"></a>2. Compute Effort</h2><p>The compute time of this algorithm depends on the examples. It not stable, but the worst case is like Batch Gradient Descent: O($N^2$)</p>\n<p>The table below shows the different among these there Gradient Descent</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Batch Gradient Descent</th>\n<th>Mini-Batch Gradient Descent</th>\n<th>Stochastic Gradient Descent</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>use 1 example in each iteration</td>\n<td>use some examples</td>\n<td>use all example in each iteration</td>\n</tr>\n<tr>\n<td>relative compute loose</td>\n<td>somewhat in between</td>\n<td>relative compute intensive</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"3-Gradient-Descent-Formula\"><a href=\"#3-Gradient-Descent-Formula\" class=\"headerlink\" title=\"3. Gradient Descent Formula\"></a>3. Gradient Descent Formula</h3><p>For all $\\theta_i$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\frac{\\partial J_\\theta}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_i)\n\\end{align}</script><blockquote>\n<p>E.g.,<br>two parameters $\\theta_0, \\theta_1$ —&gt; $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1$</p>\n</blockquote>\n<p>For i = 0 :</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_\\theta}{\\partial \\theta_0} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right] \\cdot (x_0)</script><p>For i = 1:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial J_\\theta}{\\partial \\theta_1} = \\frac{1}{m} \\sum\\limits_{i=1}^{M} \\left[ h_{\\theta}(x_i) - y_i \\right]\\cdot (x_1)</script><p>Note that the datasets need to be shuffled before iteration.</p>\n"},{"title":"Hexo Next Theme Beautification","date":"2017-11-23T03:23:19.000Z","mathjax":false,"copyright":true,"top":100,"_content":"\n\n# Hexo Next Theme Beautification\n\n> There are a lot of other blog which demostrate how to beautify the Hexo-Next Theme.\n\n1. [hexo下新建页面下如何放多个文章？](http://www.zhihu.com/question/33324071/answer/58775540?group_id=654307162210365440#comment-106092511)\n2. [玩转Hexo博客之Next](http://www.jianshu.com/p/f869d1940985)\n3. [动动手指，NexT主题与Hexo更搭哦（基础篇） ](http://www.arao.me/2015/hexo-next-theme-optimize-base/#hexo_NexT%E4%B8%BB%E9%A2%98%E9%A6%96%E9%A1%B5title%E7%9A%84%E4%BC%98%E5%8C%96)\n4. [动动手指，不限于NexT主题的Hexo优化（SEO篇）](http://www.arao.me/2015/hexo-next-theme-optimize-seo/)\n5. [动动手指，给你的Hexo站点添加最近访客（多说篇）](http://www.arao.me/2015/hexo-next-theme-optimize-duoshuo/)\n6. [hexo的next主题个性化教程：打造炫酷网站 ](http://blog.csdn.net/qq_33699981/article/details/72716951)\n7. [Hexo-Next-主题优化(四)](http://www.jianshu.com/p/4ef35521fee9)\n\n<br>\n<br>\n---------------------------------------","source":"_posts/Others/Hexos/2017-11-23-Hexo Next Theme Beautification.md","raw":"---\ntitle: Hexo Next Theme Beautification\ndate: 2017-11-23 11:23:19\ncategories: [Hexo]\ntags: [Hexo]\nmathjax: false\ncopyright: true\ntop: 100\n---\n\n\n# Hexo Next Theme Beautification\n\n> There are a lot of other blog which demostrate how to beautify the Hexo-Next Theme.\n\n1. [hexo下新建页面下如何放多个文章？](http://www.zhihu.com/question/33324071/answer/58775540?group_id=654307162210365440#comment-106092511)\n2. [玩转Hexo博客之Next](http://www.jianshu.com/p/f869d1940985)\n3. [动动手指，NexT主题与Hexo更搭哦（基础篇） ](http://www.arao.me/2015/hexo-next-theme-optimize-base/#hexo_NexT%E4%B8%BB%E9%A2%98%E9%A6%96%E9%A1%B5title%E7%9A%84%E4%BC%98%E5%8C%96)\n4. [动动手指，不限于NexT主题的Hexo优化（SEO篇）](http://www.arao.me/2015/hexo-next-theme-optimize-seo/)\n5. [动动手指，给你的Hexo站点添加最近访客（多说篇）](http://www.arao.me/2015/hexo-next-theme-optimize-duoshuo/)\n6. [hexo的next主题个性化教程：打造炫酷网站 ](http://blog.csdn.net/qq_33699981/article/details/72716951)\n7. [Hexo-Next-主题优化(四)](http://www.jianshu.com/p/4ef35521fee9)\n\n<br>\n<br>\n---------------------------------------","slug":"Others/Hexos/2017-11-23-Hexo Next Theme Beautification","published":1,"updated":"2018-04-14T19:42:06.499Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hi003arwtj7q3dxkam","content":"<h1 id=\"Hexo-Next-Theme-Beautification\"><a href=\"#Hexo-Next-Theme-Beautification\" class=\"headerlink\" title=\"Hexo Next Theme Beautification\"></a>Hexo Next Theme Beautification</h1><blockquote>\n<p>There are a lot of other blog which demostrate how to beautify the Hexo-Next Theme.</p>\n</blockquote>\n<ol>\n<li><a href=\"http://www.zhihu.com/question/33324071/answer/58775540?group_id=654307162210365440#comment-106092511\" target=\"_blank\" rel=\"external\">hexo下新建页面下如何放多个文章？</a></li>\n<li><a href=\"http://www.jianshu.com/p/f869d1940985\" target=\"_blank\" rel=\"external\">玩转Hexo博客之Next</a></li>\n<li><a href=\"http://www.arao.me/2015/hexo-next-theme-optimize-base/#hexo_NexT%E4%B8%BB%E9%A2%98%E9%A6%96%E9%A1%B5title%E7%9A%84%E4%BC%98%E5%8C%96\" target=\"_blank\" rel=\"external\">动动手指，NexT主题与Hexo更搭哦（基础篇） </a></li>\n<li><a href=\"http://www.arao.me/2015/hexo-next-theme-optimize-seo/\" target=\"_blank\" rel=\"external\">动动手指，不限于NexT主题的Hexo优化（SEO篇）</a></li>\n<li><a href=\"http://www.arao.me/2015/hexo-next-theme-optimize-duoshuo/\" target=\"_blank\" rel=\"external\">动动手指，给你的Hexo站点添加最近访客（多说篇）</a></li>\n<li><a href=\"http://blog.csdn.net/qq_33699981/article/details/72716951\" target=\"_blank\" rel=\"external\">hexo的next主题个性化教程：打造炫酷网站 </a></li>\n<li><a href=\"http://www.jianshu.com/p/4ef35521fee9\" target=\"_blank\" rel=\"external\">Hexo-Next-主题优化(四)</a></li>\n</ol>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hexo-Next-Theme-Beautification\"><a href=\"#Hexo-Next-Theme-Beautification\" class=\"headerlink\" title=\"Hexo Next Theme Beautification\"></a>Hexo Next Theme Beautification</h1><blockquote>\n<p>There are a lot of other blog which demostrate how to beautify the Hexo-Next Theme.</p>\n</blockquote>\n<ol>\n<li><a href=\"http://www.zhihu.com/question/33324071/answer/58775540?group_id=654307162210365440#comment-106092511\" target=\"_blank\" rel=\"external\">hexo下新建页面下如何放多个文章？</a></li>\n<li><a href=\"http://www.jianshu.com/p/f869d1940985\" target=\"_blank\" rel=\"external\">玩转Hexo博客之Next</a></li>\n<li><a href=\"http://www.arao.me/2015/hexo-next-theme-optimize-base/#hexo_NexT%E4%B8%BB%E9%A2%98%E9%A6%96%E9%A1%B5title%E7%9A%84%E4%BC%98%E5%8C%96\" target=\"_blank\" rel=\"external\">动动手指，NexT主题与Hexo更搭哦（基础篇） </a></li>\n<li><a href=\"http://www.arao.me/2015/hexo-next-theme-optimize-seo/\" target=\"_blank\" rel=\"external\">动动手指，不限于NexT主题的Hexo优化（SEO篇）</a></li>\n<li><a href=\"http://www.arao.me/2015/hexo-next-theme-optimize-duoshuo/\" target=\"_blank\" rel=\"external\">动动手指，给你的Hexo站点添加最近访客（多说篇）</a></li>\n<li><a href=\"http://blog.csdn.net/qq_33699981/article/details/72716951\" target=\"_blank\" rel=\"external\">hexo的next主题个性化教程：打造炫酷网站 </a></li>\n<li><a href=\"http://www.jianshu.com/p/4ef35521fee9\" target=\"_blank\" rel=\"external\">Hexo-Next-主题优化(四)</a></li>\n</ol>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"What is a one-sided limits?","date":"2017-11-02T02:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# What is a one-sided limits?\n\n## Introduction\n### One-sided Limits and Two-sided Limits\nPeople are familiar with two sided limits, shown below.\n\n$$\n\\lim\\limits_{x->a} f(x) = L\n\\tag{$1$}\n$$\n\nBut here, we are going to introduce one-sided limits and shown the correlative between them.\nThe expression of one-sided limits are shown below.\n$$\n\\lim\\limits_{x->a^+} f(x) = L\n\\tag{$2$}\n$$\n$$\n\\lim\\limits_{x->a^-} f(x) = L\n\\tag{$3$}\n$$\n\nThe different is the superscript of a, which means the direction of approaching. E.g., the formula (2) means that $f(x)$ is close to $L$, as the provided $x$ is approach $a$ from the right hand side.\n\nAnd the same as formula (3), which means that $f(x)$ is close to $L$, as the provided $x$ is approach $a$ from the left hand side.\n\n\n<br>\n<br>\n\n## Graph\nSee the graph down there.\n\n![one-sided limits](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b285f3c9bf4387518c42780c3f3d615de0b78a13/__Blog/__Personal%20Understanding/Mathematics/Calculus/Introduction%20of%20Calculus/images/1.%20one-sided%20limit%20graph.jpg)\n<center> 1. one-sided limits </center>\n<br>\n\nCompare graph with formula (2) and (3). You can say that $f(x)$ is getting close to 1 if $x$ approach 2 from the left-hand side. And you can also say that $f(x)$ is getting close to 3 if $x$ approach 2 from the right-hand side.\n\n\n\n<br>\n<br>\n## Correlation between One-sided Limits and Two-sided Limits\nWhen one-sided limits from both sides are equal to the same value, we could simply use two-sided limits to express. Whereas, the one-sided limits is not exactly the same, we have use one-sided limits.\n\n\nE.g.,\n\n$$\nif\n\\left\\{\n\\begin{align}\n\\lim\\limits_{x->a^+} f(x) = L \\\\\n\\lim\\limits_{x->a^-} f(x) = L\n\\end{align}\n\\right.\n\\Longrightarrow\n\\lim\\limits_{x->a} f(x) = L\n$$\n\n$$\nif\n\\left\\{\n\\begin{align}\n\\lim\\limits_{x->a^+} f(x) = L_1 \\\\\n\\lim\\limits_{x->a^-} f(x) = L_2\n\\end{align}\n\\right.\n\\text{($L_1 \\neq L_2$)}\n\\nRightarrow\n\\lim\\limits_{x->a} f(x) = L\n$$\n\n# Summary\n1. One-sided limits is not equal to two-sided limits.\n2. If both sides of one-sided limits have the same value, we could make it two-sided limits.\n\n# Reference\n[1] Introduction of Calculus in Coursera by Jim Fowler\n\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/Calculus/2017-11-02-What is a one-sided limit.md","raw":"---\ntitle: What is a one-sided limits?\ndate: 2017-11-02 10:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics, Calculus]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# What is a one-sided limits?\n\n## Introduction\n### One-sided Limits and Two-sided Limits\nPeople are familiar with two sided limits, shown below.\n\n$$\n\\lim\\limits_{x->a} f(x) = L\n\\tag{$1$}\n$$\n\nBut here, we are going to introduce one-sided limits and shown the correlative between them.\nThe expression of one-sided limits are shown below.\n$$\n\\lim\\limits_{x->a^+} f(x) = L\n\\tag{$2$}\n$$\n$$\n\\lim\\limits_{x->a^-} f(x) = L\n\\tag{$3$}\n$$\n\nThe different is the superscript of a, which means the direction of approaching. E.g., the formula (2) means that $f(x)$ is close to $L$, as the provided $x$ is approach $a$ from the right hand side.\n\nAnd the same as formula (3), which means that $f(x)$ is close to $L$, as the provided $x$ is approach $a$ from the left hand side.\n\n\n<br>\n<br>\n\n## Graph\nSee the graph down there.\n\n![one-sided limits](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b285f3c9bf4387518c42780c3f3d615de0b78a13/__Blog/__Personal%20Understanding/Mathematics/Calculus/Introduction%20of%20Calculus/images/1.%20one-sided%20limit%20graph.jpg)\n<center> 1. one-sided limits </center>\n<br>\n\nCompare graph with formula (2) and (3). You can say that $f(x)$ is getting close to 1 if $x$ approach 2 from the left-hand side. And you can also say that $f(x)$ is getting close to 3 if $x$ approach 2 from the right-hand side.\n\n\n\n<br>\n<br>\n## Correlation between One-sided Limits and Two-sided Limits\nWhen one-sided limits from both sides are equal to the same value, we could simply use two-sided limits to express. Whereas, the one-sided limits is not exactly the same, we have use one-sided limits.\n\n\nE.g.,\n\n$$\nif\n\\left\\{\n\\begin{align}\n\\lim\\limits_{x->a^+} f(x) = L \\\\\n\\lim\\limits_{x->a^-} f(x) = L\n\\end{align}\n\\right.\n\\Longrightarrow\n\\lim\\limits_{x->a} f(x) = L\n$$\n\n$$\nif\n\\left\\{\n\\begin{align}\n\\lim\\limits_{x->a^+} f(x) = L_1 \\\\\n\\lim\\limits_{x->a^-} f(x) = L_2\n\\end{align}\n\\right.\n\\text{($L_1 \\neq L_2$)}\n\\nRightarrow\n\\lim\\limits_{x->a} f(x) = L\n$$\n\n# Summary\n1. One-sided limits is not equal to two-sided limits.\n2. If both sides of one-sided limits have the same value, we could make it two-sided limits.\n\n# Reference\n[1] Introduction of Calculus in Coursera by Jim Fowler\n\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/Calculus/2017-11-02-What is a one-sided limit","published":1,"updated":"2018-04-14T19:42:06.493Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hl003frwtj56exjaiq","content":"<h1 id=\"What-is-a-one-sided-limits\"><a href=\"#What-is-a-one-sided-limits\" class=\"headerlink\" title=\"What is a one-sided limits?\"></a>What is a one-sided limits?</h1><h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><h3 id=\"One-sided-Limits-and-Two-sided-Limits\"><a href=\"#One-sided-Limits-and-Two-sided-Limits\" class=\"headerlink\" title=\"One-sided Limits and Two-sided Limits\"></a>One-sided Limits and Two-sided Limits</h3><p>People are familiar with two sided limits, shown below.</p>\n<script type=\"math/tex; mode=display\">\n\\lim\\limits_{x->a} f(x) = L\n\\tag{$1$}</script><p>But here, we are going to introduce one-sided limits and shown the correlative between them.<br>The expression of one-sided limits are shown below.</p>\n<script type=\"math/tex; mode=display\">\n\\lim\\limits_{x->a^+} f(x) = L\n\\tag{$2$}</script><script type=\"math/tex; mode=display\">\n\\lim\\limits_{x->a^-} f(x) = L\n\\tag{$3$}</script><p>The different is the superscript of a, which means the direction of approaching. E.g., the formula (2) means that $f(x)$ is close to $L$, as the provided $x$ is approach $a$ from the right hand side.</p>\n<p>And the same as formula (3), which means that $f(x)$ is close to $L$, as the provided $x$ is approach $a$ from the left hand side.</p>\n<p><br><br><br></p>\n<h2 id=\"Graph\"><a href=\"#Graph\" class=\"headerlink\" title=\"Graph\"></a>Graph</h2><p>See the graph down there.</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b285f3c9bf4387518c42780c3f3d615de0b78a13/__Blog/__Personal%20Understanding/Mathematics/Calculus/Introduction%20of%20Calculus/images/1.%20one-sided%20limit%20graph.jpg\" alt=\"one-sided limits\"></p>\n<p><center> 1. one-sided limits </center><br><br></p>\n<p>Compare graph with formula (2) and (3). You can say that $f(x)$ is getting close to 1 if $x$ approach 2 from the left-hand side. And you can also say that $f(x)$ is getting close to 3 if $x$ approach 2 from the right-hand side.</p>\n<p><br><br><br></p>\n<h2 id=\"Correlation-between-One-sided-Limits-and-Two-sided-Limits\"><a href=\"#Correlation-between-One-sided-Limits-and-Two-sided-Limits\" class=\"headerlink\" title=\"Correlation between One-sided Limits and Two-sided Limits\"></a>Correlation between One-sided Limits and Two-sided Limits</h2><p>When one-sided limits from both sides are equal to the same value, we could simply use two-sided limits to express. Whereas, the one-sided limits is not exactly the same, we have use one-sided limits.</p>\n<p>E.g.,</p>\n<script type=\"math/tex; mode=display\">\nif\n\\left\\{\n\\begin{align}\n\\lim\\limits_{x->a^+} f(x) = L \\\\\n\\lim\\limits_{x->a^-} f(x) = L\n\\end{align}\n\\right.\n\\Longrightarrow\n\\lim\\limits_{x->a} f(x) = L</script><script type=\"math/tex; mode=display\">\nif\n\\left\\{\n\\begin{align}\n\\lim\\limits_{x->a^+} f(x) = L_1 \\\\\n\\lim\\limits_{x->a^-} f(x) = L_2\n\\end{align}\n\\right.\n\\text{($L_1 \\neq L_2$)}\n\\nRightarrow\n\\lim\\limits_{x->a} f(x) = L</script><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>One-sided limits is not equal to two-sided limits.</li>\n<li>If both sides of one-sided limits have the same value, we could make it two-sided limits.</li>\n</ol>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] Introduction of Calculus in Coursera by Jim Fowler</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"What-is-a-one-sided-limits\"><a href=\"#What-is-a-one-sided-limits\" class=\"headerlink\" title=\"What is a one-sided limits?\"></a>What is a one-sided limits?</h1><h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><h3 id=\"One-sided-Limits-and-Two-sided-Limits\"><a href=\"#One-sided-Limits-and-Two-sided-Limits\" class=\"headerlink\" title=\"One-sided Limits and Two-sided Limits\"></a>One-sided Limits and Two-sided Limits</h3><p>People are familiar with two sided limits, shown below.</p>\n<script type=\"math/tex; mode=display\">\n\\lim\\limits_{x->a} f(x) = L\n\\tag{$1$}</script><p>But here, we are going to introduce one-sided limits and shown the correlative between them.<br>The expression of one-sided limits are shown below.</p>\n<script type=\"math/tex; mode=display\">\n\\lim\\limits_{x->a^+} f(x) = L\n\\tag{$2$}</script><script type=\"math/tex; mode=display\">\n\\lim\\limits_{x->a^-} f(x) = L\n\\tag{$3$}</script><p>The different is the superscript of a, which means the direction of approaching. E.g., the formula (2) means that $f(x)$ is close to $L$, as the provided $x$ is approach $a$ from the right hand side.</p>\n<p>And the same as formula (3), which means that $f(x)$ is close to $L$, as the provided $x$ is approach $a$ from the left hand side.</p>\n<p><br><br><br></p>\n<h2 id=\"Graph\"><a href=\"#Graph\" class=\"headerlink\" title=\"Graph\"></a>Graph</h2><p>See the graph down there.</p>\n<p><img src=\"https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/b285f3c9bf4387518c42780c3f3d615de0b78a13/__Blog/__Personal%20Understanding/Mathematics/Calculus/Introduction%20of%20Calculus/images/1.%20one-sided%20limit%20graph.jpg\" alt=\"one-sided limits\"></p>\n<p><center> 1. one-sided limits </center><br><br></p>\n<p>Compare graph with formula (2) and (3). You can say that $f(x)$ is getting close to 1 if $x$ approach 2 from the left-hand side. And you can also say that $f(x)$ is getting close to 3 if $x$ approach 2 from the right-hand side.</p>\n<p><br><br><br></p>\n<h2 id=\"Correlation-between-One-sided-Limits-and-Two-sided-Limits\"><a href=\"#Correlation-between-One-sided-Limits-and-Two-sided-Limits\" class=\"headerlink\" title=\"Correlation between One-sided Limits and Two-sided Limits\"></a>Correlation between One-sided Limits and Two-sided Limits</h2><p>When one-sided limits from both sides are equal to the same value, we could simply use two-sided limits to express. Whereas, the one-sided limits is not exactly the same, we have use one-sided limits.</p>\n<p>E.g.,</p>\n<script type=\"math/tex; mode=display\">\nif\n\\left\\{\n\\begin{align}\n\\lim\\limits_{x->a^+} f(x) = L \\\\\n\\lim\\limits_{x->a^-} f(x) = L\n\\end{align}\n\\right.\n\\Longrightarrow\n\\lim\\limits_{x->a} f(x) = L</script><script type=\"math/tex; mode=display\">\nif\n\\left\\{\n\\begin{align}\n\\lim\\limits_{x->a^+} f(x) = L_1 \\\\\n\\lim\\limits_{x->a^-} f(x) = L_2\n\\end{align}\n\\right.\n\\text{($L_1 \\neq L_2$)}\n\\nRightarrow\n\\lim\\limits_{x->a} f(x) = L</script><h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><ol>\n<li>One-sided limits is not equal to two-sided limits.</li>\n<li>If both sides of one-sided limits have the same value, we could make it two-sided limits.</li>\n</ol>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] Introduction of Calculus in Coursera by Jim Fowler</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"Function and Limit","date":"2017-11-02T00:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Function and Limit\n\n## 1. Function\n### 1) What is a Function?\nIn mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output.[1]\n\nThink about a water dispenser, on the one side, we have a busket full of water, which inputs those water into the water dispenser fast. On the other side, the tag drains the water with much small speed. In this case, the water dispenser is somewhat a function of the speed of water. The input is fast, and the output is relatively slower.\n\nWe usually call the input as $x$, the function as $f$, and the output of $f$ corresponding to an input $x$ is denoted by $f(x)$ (read \"f of x\").[1]\n\n\n### 2) What does a Function do?\nA function map input (each number in its domain) to one and only output.\n\nFor example, $f(x) = x + 1 \\text( $x ∈ {\\rm I\\!R}$)$\n\n| x   | f(x) |\n| --- | ---- |\n| 1   | 2    |\n| 2   | 3    |\n| 3   | 4    |\n| 5   | 6    |\n| ... | ...  |\n| x   | x+1  |\n\n\n\n\n\n\n## 2. Limit\n### 1) What is a Limit?\nA limit is the function with a variable approach some value. Limits are essential to calculus (and mathematical analysis in general) and are used to define continuity, derivatives, and integrals.[2].\n\n### 2) What is one-sided limit?\n\nIn short, one-sided limit is the limition with one value approach from one side, and different will may have way different result of the formula because, the value is always bigger/small than the other value.\n\nWhen one-sided limits from both sides are equal to the same value, we could simply use two-sided limits to express. Whereas, the one-sided limits is not exactly the same, we have use one-sided limits.\n\nSee more at this blog [What is one-sided limit](https://zhichengmle.github.io/2017/11/02/Mathematics/Calculus/2017-11-02-What%20is%20a%20one-sided%20limit/)\n\n### 3) Some Rules of Limits\n\n> There are basically 4 types of limits\n\n1. $\\frac{0}{0}(\\frac{\\infty}{\\infty})$\n2. $\\infty \\times 0$\n3. Exponential  e.g., $1^\\infty$, $0^0$, $\\infty^0$\n4. $\\infty - \\infty$\n\n\nNote that, Tylor expansion could deal with all kinds of types.\n\n#### ① $\\frac{0}{0}(\\frac{\\infty}{\\infty})$\n\nUse l'Hôpital's rule to get the derivative of nominator and demoninator. That is $\\frac{0'}{0'}(\\frac{\\infty'}{\\infty'})$\n\n\n#### ② $\\infty \\times 0$\n\nTransform to type ①, and then use the method offered in type ①. For example, $\\infty \\times 0 \\Rightarrow \\frac{\\frac{0}{1}}{\\infty}$ or $\\frac{\\frac{\\infty}{1}}{0}$\n\n#### ③ Exponential\n\nTransform exponential to logarithm, then you get type ②。Using this rule $u^v = e^(vlnu)$ .For example, $1^\\infty \\Rightarrow e^{\\infty ln1}$, $0^\\infty \\Rightarrow e^{0ln0}$, $1^\\infty \\Rightarrow e^{0 ln\\infty}$\n\n#### ④ $\\infty - \\infty$\n\nThere are several detail for this type. But using Tylor expansion is the most powerful way for this type. Because Tylor expansion is an equation, so that you don't need to worry about the other complicate prerequisite.\n\n\n\n\n\n# Summary\n\n# Reference\n[1] Wikipedia - Function (mathematics) [https://en.wikipedia.org/wiki/Function_(mathematics)](https://en.wikipedia.org/wiki/Function_(mathematics))\n\n[2] Wikipedia - Limit (mathematics) [https://en.wikipedia.org/wiki/Limit_(mathematics)](https://en.wikipedia.org/wiki/Limit_(mathematics))\n\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/Calculus/2017-11-02-Function and limit.md","raw":"---\ntitle: Function and Limit\ndate: 2017-11-02 08:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics, Calculus]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Function and Limit\n\n## 1. Function\n### 1) What is a Function?\nIn mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output.[1]\n\nThink about a water dispenser, on the one side, we have a busket full of water, which inputs those water into the water dispenser fast. On the other side, the tag drains the water with much small speed. In this case, the water dispenser is somewhat a function of the speed of water. The input is fast, and the output is relatively slower.\n\nWe usually call the input as $x$, the function as $f$, and the output of $f$ corresponding to an input $x$ is denoted by $f(x)$ (read \"f of x\").[1]\n\n\n### 2) What does a Function do?\nA function map input (each number in its domain) to one and only output.\n\nFor example, $f(x) = x + 1 \\text( $x ∈ {\\rm I\\!R}$)$\n\n| x   | f(x) |\n| --- | ---- |\n| 1   | 2    |\n| 2   | 3    |\n| 3   | 4    |\n| 5   | 6    |\n| ... | ...  |\n| x   | x+1  |\n\n\n\n\n\n\n## 2. Limit\n### 1) What is a Limit?\nA limit is the function with a variable approach some value. Limits are essential to calculus (and mathematical analysis in general) and are used to define continuity, derivatives, and integrals.[2].\n\n### 2) What is one-sided limit?\n\nIn short, one-sided limit is the limition with one value approach from one side, and different will may have way different result of the formula because, the value is always bigger/small than the other value.\n\nWhen one-sided limits from both sides are equal to the same value, we could simply use two-sided limits to express. Whereas, the one-sided limits is not exactly the same, we have use one-sided limits.\n\nSee more at this blog [What is one-sided limit](https://zhichengmle.github.io/2017/11/02/Mathematics/Calculus/2017-11-02-What%20is%20a%20one-sided%20limit/)\n\n### 3) Some Rules of Limits\n\n> There are basically 4 types of limits\n\n1. $\\frac{0}{0}(\\frac{\\infty}{\\infty})$\n2. $\\infty \\times 0$\n3. Exponential  e.g., $1^\\infty$, $0^0$, $\\infty^0$\n4. $\\infty - \\infty$\n\n\nNote that, Tylor expansion could deal with all kinds of types.\n\n#### ① $\\frac{0}{0}(\\frac{\\infty}{\\infty})$\n\nUse l'Hôpital's rule to get the derivative of nominator and demoninator. That is $\\frac{0'}{0'}(\\frac{\\infty'}{\\infty'})$\n\n\n#### ② $\\infty \\times 0$\n\nTransform to type ①, and then use the method offered in type ①. For example, $\\infty \\times 0 \\Rightarrow \\frac{\\frac{0}{1}}{\\infty}$ or $\\frac{\\frac{\\infty}{1}}{0}$\n\n#### ③ Exponential\n\nTransform exponential to logarithm, then you get type ②。Using this rule $u^v = e^(vlnu)$ .For example, $1^\\infty \\Rightarrow e^{\\infty ln1}$, $0^\\infty \\Rightarrow e^{0ln0}$, $1^\\infty \\Rightarrow e^{0 ln\\infty}$\n\n#### ④ $\\infty - \\infty$\n\nThere are several detail for this type. But using Tylor expansion is the most powerful way for this type. Because Tylor expansion is an equation, so that you don't need to worry about the other complicate prerequisite.\n\n\n\n\n\n# Summary\n\n# Reference\n[1] Wikipedia - Function (mathematics) [https://en.wikipedia.org/wiki/Function_(mathematics)](https://en.wikipedia.org/wiki/Function_(mathematics))\n\n[2] Wikipedia - Limit (mathematics) [https://en.wikipedia.org/wiki/Limit_(mathematics)](https://en.wikipedia.org/wiki/Limit_(mathematics))\n\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/Calculus/2017-11-02-Function and limit","published":1,"updated":"2018-04-14T19:42:06.492Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hm003irwtjjwa2rku6","content":"<h1 id=\"Function-and-Limit\"><a href=\"#Function-and-Limit\" class=\"headerlink\" title=\"Function and Limit\"></a>Function and Limit</h1><h2 id=\"1-Function\"><a href=\"#1-Function\" class=\"headerlink\" title=\"1. Function\"></a>1. Function</h2><h3 id=\"1-What-is-a-Function\"><a href=\"#1-What-is-a-Function\" class=\"headerlink\" title=\"1) What is a Function?\"></a>1) What is a Function?</h3><p>In mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output.[1]</p>\n<p>Think about a water dispenser, on the one side, we have a busket full of water, which inputs those water into the water dispenser fast. On the other side, the tag drains the water with much small speed. In this case, the water dispenser is somewhat a function of the speed of water. The input is fast, and the output is relatively slower.</p>\n<p>We usually call the input as $x$, the function as $f$, and the output of $f$ corresponding to an input $x$ is denoted by $f(x)$ (read “f of x”).[1]</p>\n<h3 id=\"2-What-does-a-Function-do\"><a href=\"#2-What-does-a-Function-do\" class=\"headerlink\" title=\"2) What does a Function do?\"></a>2) What does a Function do?</h3><p>A function map input (each number in its domain) to one and only output.</p>\n<p>For example, $f(x) = x + 1 \\text( $x ∈ {\\rm I!R}$)$</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>x</th>\n<th>f(x)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>4</td>\n</tr>\n<tr>\n<td>5</td>\n<td>6</td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n</tr>\n<tr>\n<td>x</td>\n<td>x+1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"2-Limit\"><a href=\"#2-Limit\" class=\"headerlink\" title=\"2. Limit\"></a>2. Limit</h2><h3 id=\"1-What-is-a-Limit\"><a href=\"#1-What-is-a-Limit\" class=\"headerlink\" title=\"1) What is a Limit?\"></a>1) What is a Limit?</h3><p>A limit is the function with a variable approach some value. Limits are essential to calculus (and mathematical analysis in general) and are used to define continuity, derivatives, and integrals.[2].</p>\n<h3 id=\"2-What-is-one-sided-limit\"><a href=\"#2-What-is-one-sided-limit\" class=\"headerlink\" title=\"2) What is one-sided limit?\"></a>2) What is one-sided limit?</h3><p>In short, one-sided limit is the limition with one value approach from one side, and different will may have way different result of the formula because, the value is always bigger/small than the other value.</p>\n<p>When one-sided limits from both sides are equal to the same value, we could simply use two-sided limits to express. Whereas, the one-sided limits is not exactly the same, we have use one-sided limits.</p>\n<p>See more at this blog <a href=\"https://zhichengmle.github.io/2017/11/02/Mathematics/Calculus/2017-11-02-What%20is%20a%20one-sided%20limit/\">What is one-sided limit</a></p>\n<h3 id=\"3-Some-Rules-of-Limits\"><a href=\"#3-Some-Rules-of-Limits\" class=\"headerlink\" title=\"3) Some Rules of Limits\"></a>3) Some Rules of Limits</h3><blockquote>\n<p>There are basically 4 types of limits</p>\n</blockquote>\n<ol>\n<li>$\\frac{0}{0}(\\frac{\\infty}{\\infty})$</li>\n<li>$\\infty \\times 0$</li>\n<li>Exponential  e.g., $1^\\infty$, $0^0$, $\\infty^0$</li>\n<li>$\\infty - \\infty$</li>\n</ol>\n<p>Note that, Tylor expansion could deal with all kinds of types.</p>\n<h4 id=\"①-frac-0-0-frac-infty-infty\"><a href=\"#①-frac-0-0-frac-infty-infty\" class=\"headerlink\" title=\"① $\\frac{0}{0}(\\frac{\\infty}{\\infty})$\"></a>① $\\frac{0}{0}(\\frac{\\infty}{\\infty})$</h4><p>Use l’Hôpital’s rule to get the derivative of nominator and demoninator. That is $\\frac{0’}{0’}(\\frac{\\infty’}{\\infty’})$</p>\n<h4 id=\"②-infty-times-0\"><a href=\"#②-infty-times-0\" class=\"headerlink\" title=\"② $\\infty \\times 0$\"></a>② $\\infty \\times 0$</h4><p>Transform to type ①, and then use the method offered in type ①. For example, $\\infty \\times 0 \\Rightarrow \\frac{\\frac{0}{1}}{\\infty}$ or $\\frac{\\frac{\\infty}{1}}{0}$</p>\n<h4 id=\"③-Exponential\"><a href=\"#③-Exponential\" class=\"headerlink\" title=\"③ Exponential\"></a>③ Exponential</h4><p>Transform exponential to logarithm, then you get type ②。Using this rule $u^v = e^(vlnu)$ .For example, $1^\\infty \\Rightarrow e^{\\infty ln1}$, $0^\\infty \\Rightarrow e^{0ln0}$, $1^\\infty \\Rightarrow e^{0 ln\\infty}$</p>\n<h4 id=\"④-infty-infty\"><a href=\"#④-infty-infty\" class=\"headerlink\" title=\"④ $\\infty - \\infty$\"></a>④ $\\infty - \\infty$</h4><p>There are several detail for this type. But using Tylor expansion is the most powerful way for this type. Because Tylor expansion is an equation, so that you don’t need to worry about the other complicate prerequisite.</p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] Wikipedia - Function (mathematics) <a href=\"https://en.wikipedia.org/wiki/Function_(mathematics\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Function_(mathematics)</a>)</p>\n<p>[2] Wikipedia - Limit (mathematics) <a href=\"https://en.wikipedia.org/wiki/Limit_(mathematics\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Limit_(mathematics)</a>)</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Function-and-Limit\"><a href=\"#Function-and-Limit\" class=\"headerlink\" title=\"Function and Limit\"></a>Function and Limit</h1><h2 id=\"1-Function\"><a href=\"#1-Function\" class=\"headerlink\" title=\"1. Function\"></a>1. Function</h2><h3 id=\"1-What-is-a-Function\"><a href=\"#1-What-is-a-Function\" class=\"headerlink\" title=\"1) What is a Function?\"></a>1) What is a Function?</h3><p>In mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output.[1]</p>\n<p>Think about a water dispenser, on the one side, we have a busket full of water, which inputs those water into the water dispenser fast. On the other side, the tag drains the water with much small speed. In this case, the water dispenser is somewhat a function of the speed of water. The input is fast, and the output is relatively slower.</p>\n<p>We usually call the input as $x$, the function as $f$, and the output of $f$ corresponding to an input $x$ is denoted by $f(x)$ (read “f of x”).[1]</p>\n<h3 id=\"2-What-does-a-Function-do\"><a href=\"#2-What-does-a-Function-do\" class=\"headerlink\" title=\"2) What does a Function do?\"></a>2) What does a Function do?</h3><p>A function map input (each number in its domain) to one and only output.</p>\n<p>For example, $f(x) = x + 1 \\text( $x ∈ {\\rm I!R}$)$</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>x</th>\n<th>f(x)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>2</td>\n</tr>\n<tr>\n<td>2</td>\n<td>3</td>\n</tr>\n<tr>\n<td>3</td>\n<td>4</td>\n</tr>\n<tr>\n<td>5</td>\n<td>6</td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n</tr>\n<tr>\n<td>x</td>\n<td>x+1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"2-Limit\"><a href=\"#2-Limit\" class=\"headerlink\" title=\"2. Limit\"></a>2. Limit</h2><h3 id=\"1-What-is-a-Limit\"><a href=\"#1-What-is-a-Limit\" class=\"headerlink\" title=\"1) What is a Limit?\"></a>1) What is a Limit?</h3><p>A limit is the function with a variable approach some value. Limits are essential to calculus (and mathematical analysis in general) and are used to define continuity, derivatives, and integrals.[2].</p>\n<h3 id=\"2-What-is-one-sided-limit\"><a href=\"#2-What-is-one-sided-limit\" class=\"headerlink\" title=\"2) What is one-sided limit?\"></a>2) What is one-sided limit?</h3><p>In short, one-sided limit is the limition with one value approach from one side, and different will may have way different result of the formula because, the value is always bigger/small than the other value.</p>\n<p>When one-sided limits from both sides are equal to the same value, we could simply use two-sided limits to express. Whereas, the one-sided limits is not exactly the same, we have use one-sided limits.</p>\n<p>See more at this blog <a href=\"https://zhichengmle.github.io/2017/11/02/Mathematics/Calculus/2017-11-02-What%20is%20a%20one-sided%20limit/\">What is one-sided limit</a></p>\n<h3 id=\"3-Some-Rules-of-Limits\"><a href=\"#3-Some-Rules-of-Limits\" class=\"headerlink\" title=\"3) Some Rules of Limits\"></a>3) Some Rules of Limits</h3><blockquote>\n<p>There are basically 4 types of limits</p>\n</blockquote>\n<ol>\n<li>$\\frac{0}{0}(\\frac{\\infty}{\\infty})$</li>\n<li>$\\infty \\times 0$</li>\n<li>Exponential  e.g., $1^\\infty$, $0^0$, $\\infty^0$</li>\n<li>$\\infty - \\infty$</li>\n</ol>\n<p>Note that, Tylor expansion could deal with all kinds of types.</p>\n<h4 id=\"①-frac-0-0-frac-infty-infty\"><a href=\"#①-frac-0-0-frac-infty-infty\" class=\"headerlink\" title=\"① $\\frac{0}{0}(\\frac{\\infty}{\\infty})$\"></a>① $\\frac{0}{0}(\\frac{\\infty}{\\infty})$</h4><p>Use l’Hôpital’s rule to get the derivative of nominator and demoninator. That is $\\frac{0’}{0’}(\\frac{\\infty’}{\\infty’})$</p>\n<h4 id=\"②-infty-times-0\"><a href=\"#②-infty-times-0\" class=\"headerlink\" title=\"② $\\infty \\times 0$\"></a>② $\\infty \\times 0$</h4><p>Transform to type ①, and then use the method offered in type ①. For example, $\\infty \\times 0 \\Rightarrow \\frac{\\frac{0}{1}}{\\infty}$ or $\\frac{\\frac{\\infty}{1}}{0}$</p>\n<h4 id=\"③-Exponential\"><a href=\"#③-Exponential\" class=\"headerlink\" title=\"③ Exponential\"></a>③ Exponential</h4><p>Transform exponential to logarithm, then you get type ②。Using this rule $u^v = e^(vlnu)$ .For example, $1^\\infty \\Rightarrow e^{\\infty ln1}$, $0^\\infty \\Rightarrow e^{0ln0}$, $1^\\infty \\Rightarrow e^{0 ln\\infty}$</p>\n<h4 id=\"④-infty-infty\"><a href=\"#④-infty-infty\" class=\"headerlink\" title=\"④ $\\infty - \\infty$\"></a>④ $\\infty - \\infty$</h4><p>There are several detail for this type. But using Tylor expansion is the most powerful way for this type. Because Tylor expansion is an equation, so that you don’t need to worry about the other complicate prerequisite.</p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h1><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] Wikipedia - Function (mathematics) <a href=\"https://en.wikipedia.org/wiki/Function_(mathematics\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Function_(mathematics)</a>)</p>\n<p>[2] Wikipedia - Limit (mathematics) <a href=\"https://en.wikipedia.org/wiki/Limit_(mathematics\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Limit_(mathematics)</a>)</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"Deploy Blog With Hexo And Github Page","date":"2017-11-22T03:23:19.000Z","mathjax":false,"copyright":true,"top":100,"_content":"\n# Deploy Blog With Hexo And Github Page\n\nThis blog aims to write down the process of deploying a personal website, and the difficulties which I have met.\n\n## Platform:\n1. Hexo\n2. Github Page\n3. Windows 10\n\n## Prerequisite\n1. Regist Github and Create a repository for github page\n2. Install Node.js in you local PC.\n3. Add the ssh key to your Github\n> Test: ssh -T git@github.com\n> If you run this command without error, it means you are good to go.\n\n## Hexo Installation and Deployment\n\n### Install Hexo\n1. mkdir <Folder for Hexo>\n2. cd <Folder for Hexo>\n3. npm install -g hexo-cli\n4. hexo\n> you should have get this message.\n\n```\nD:\\Blog\\zhichengML> hexo\nUsage: hexo <command>\nCommands:\n  clean     Remove generated files and cache.\n  config    Get or set configurations.\n  deploy    Deploy your website.\n  generate  Generate static files.\n  help      Get help on a command.\n  init      Create a new Hexo folder.\n  list      List the information of the site\n  migrate   Migrate your site from other system to Hexo.\n  new       Create a new post.\n  publish   Moves a draft post from _drafts to _posts folder.\n  render    Render files with renderer plugins.\n  server    Start the server.\n  version   Display version information.\n\nGlobal Options:\n  --config  Specify config file instead of using _config.yml\n  --cwd     Specify the CWD\n  --debug   Display all verbose messages in the terminal\n  --draft   Display draft posts\n  --safe    Disable all plugins and scripts\n  --silent  Hide output on console\n\nFor more help, you can use 'hexo help [command]' for the detailed information\nor you can check the docs: http://hexo.io/docs/\n```\n\n### Deploy Website\n#### Init Hexo\n1. hexo init <blog name>\n2. cd <blog name>\n3. npm install\n\n#### Config Hexo\n1. Open \"_config.yml\" file and override this file with your information\n>The Origin file\n```\n# Hexo Configuration\n## Docs: https://hexo.io/docs/configuration.html\n## Source: https://github.com/hexojs/hexo/\n\n# Site\ntitle: # The title of your website\nsubtitle: # The subtitle of your website\ndescription: # The description of your website\nauthor: # Your name\nlanguage: # The language of your website\ntimezone:\n\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://yoursite.com/child\nroot: /\npermalink: :year/:month/:day/:title/\npermalink_defaults:\n\n# Directory\nsource_dir: source\npublic_dir: public\ntag_dir: tags\narchive_dir: archives\ncategory_dir: categories\ncode_dir: downloads/code\ni18n_dir: :lang\nskip_render:\n\n# Writing\nnew_post_name: :title.md # File name of new posts\ndefault_layout: post\ntitlecase: false # Transform title into titlecase\nexternal_link: true # Open external links in new tab\nfilename_case: 0\nrender_drafts: false\npost_asset_folder: false\nrelative_link: false\nfuture: true\nhighlight:\n  enable: true\n  line_number: true\n  auto_detect: false\n  tab_replace:\n\n# Category & Tag\ndefault_category: uncategorized\ncategory_map:\ntag_map:\n\n# Date / Time format\n## Hexo uses Moment.js to parse and display date\n## You can customize the date format as defined in\n## http://momentjs.com/docs/#/displaying/format/\ndate_format: YYYY-MM-DD\ntime_format: HH:mm:ss\n\n# Pagination\n## Set per_page to 0 to disable pagination\nper_page: 10\npagination_dir: page\n\n# Extensions\n## Plugins: https://hexo.io/plugins/\n## Themes: https://hexo.io/themes/\ntheme: landscape\n\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type:\n```\n\n> You should override at least \"# Site\" and \"# Deployment\" session\nE.g.\n```\n# Site\ntitle: zhichengML\nsubtitle:\ndescription: Personal Website\nauthor: zhichengML\nlanguage: en\ntimezone: Asia/Shanghai\n```\n```\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repo: git@github.com:zhichengML/zhichengML.github.io.git\n  branch: master\n```\nNote that, there must be a blank space \" \" between label and information, e.g., \"type: git\"\n\n#### Preview Your Website in Local PC\n1. hexo server\n> The default port is 4000. You may want to change that with command hexo server -p <port>.\n2. Open you local pc for test.\n\n#### Deploy Website\n1. hexo generate\n2. hexo deploy\n> You may met problem when deploying.\n> See the troubleshoot for more information\n\n## Troubleshoots\n1. There is no response when running \"hexo deploy\"\n> Open \"_config.yml\" file and check if you have a space between the label and information. E.g., \"type: git\".\n\n2. Deployer not found: git\n> Becaue the older hexo use github, while the latest one(3.0) use git. So make sure you had typed \"type: git\" instead of \"type: github\". Then, install the git delopyer with command \"npm install hexo-deployer-git --save\"\n\n3. fatal: Not a git repository (or any of the parent directories): .git\n> Make sure you could run \"ssh -T git@github.com\" correctly. Then, delte the \".deploy_git\" directory, and run \"hexo deploy\" again.\n\n\n## Reference\n[1] [20分钟教你使用hexo搭建github博客](http://www.jianshu.com/p/e99ed60390a8)\n[2] [针对github权限导致hexo部署失败的解决方案][http://www.cnblogs.com/xsilence/p/6001938.html]\n\n\n\n<br>\n<br>\n---------------------------------------","source":"_posts/Others/Hexos/2017-11-22-Deploy Blog With Hexo And Github Page.md","raw":"---\ntitle: Deploy Blog With Hexo And Github Page\ndate: 2017-11-22 11:23:19\ncategories: [Hexo]\ntags: [Hexo]\nmathjax: false\ncopyright: true\ntop: 100\n---\n\n# Deploy Blog With Hexo And Github Page\n\nThis blog aims to write down the process of deploying a personal website, and the difficulties which I have met.\n\n## Platform:\n1. Hexo\n2. Github Page\n3. Windows 10\n\n## Prerequisite\n1. Regist Github and Create a repository for github page\n2. Install Node.js in you local PC.\n3. Add the ssh key to your Github\n> Test: ssh -T git@github.com\n> If you run this command without error, it means you are good to go.\n\n## Hexo Installation and Deployment\n\n### Install Hexo\n1. mkdir <Folder for Hexo>\n2. cd <Folder for Hexo>\n3. npm install -g hexo-cli\n4. hexo\n> you should have get this message.\n\n```\nD:\\Blog\\zhichengML> hexo\nUsage: hexo <command>\nCommands:\n  clean     Remove generated files and cache.\n  config    Get or set configurations.\n  deploy    Deploy your website.\n  generate  Generate static files.\n  help      Get help on a command.\n  init      Create a new Hexo folder.\n  list      List the information of the site\n  migrate   Migrate your site from other system to Hexo.\n  new       Create a new post.\n  publish   Moves a draft post from _drafts to _posts folder.\n  render    Render files with renderer plugins.\n  server    Start the server.\n  version   Display version information.\n\nGlobal Options:\n  --config  Specify config file instead of using _config.yml\n  --cwd     Specify the CWD\n  --debug   Display all verbose messages in the terminal\n  --draft   Display draft posts\n  --safe    Disable all plugins and scripts\n  --silent  Hide output on console\n\nFor more help, you can use 'hexo help [command]' for the detailed information\nor you can check the docs: http://hexo.io/docs/\n```\n\n### Deploy Website\n#### Init Hexo\n1. hexo init <blog name>\n2. cd <blog name>\n3. npm install\n\n#### Config Hexo\n1. Open \"_config.yml\" file and override this file with your information\n>The Origin file\n```\n# Hexo Configuration\n## Docs: https://hexo.io/docs/configuration.html\n## Source: https://github.com/hexojs/hexo/\n\n# Site\ntitle: # The title of your website\nsubtitle: # The subtitle of your website\ndescription: # The description of your website\nauthor: # Your name\nlanguage: # The language of your website\ntimezone:\n\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://yoursite.com/child\nroot: /\npermalink: :year/:month/:day/:title/\npermalink_defaults:\n\n# Directory\nsource_dir: source\npublic_dir: public\ntag_dir: tags\narchive_dir: archives\ncategory_dir: categories\ncode_dir: downloads/code\ni18n_dir: :lang\nskip_render:\n\n# Writing\nnew_post_name: :title.md # File name of new posts\ndefault_layout: post\ntitlecase: false # Transform title into titlecase\nexternal_link: true # Open external links in new tab\nfilename_case: 0\nrender_drafts: false\npost_asset_folder: false\nrelative_link: false\nfuture: true\nhighlight:\n  enable: true\n  line_number: true\n  auto_detect: false\n  tab_replace:\n\n# Category & Tag\ndefault_category: uncategorized\ncategory_map:\ntag_map:\n\n# Date / Time format\n## Hexo uses Moment.js to parse and display date\n## You can customize the date format as defined in\n## http://momentjs.com/docs/#/displaying/format/\ndate_format: YYYY-MM-DD\ntime_format: HH:mm:ss\n\n# Pagination\n## Set per_page to 0 to disable pagination\nper_page: 10\npagination_dir: page\n\n# Extensions\n## Plugins: https://hexo.io/plugins/\n## Themes: https://hexo.io/themes/\ntheme: landscape\n\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type:\n```\n\n> You should override at least \"# Site\" and \"# Deployment\" session\nE.g.\n```\n# Site\ntitle: zhichengML\nsubtitle:\ndescription: Personal Website\nauthor: zhichengML\nlanguage: en\ntimezone: Asia/Shanghai\n```\n```\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repo: git@github.com:zhichengML/zhichengML.github.io.git\n  branch: master\n```\nNote that, there must be a blank space \" \" between label and information, e.g., \"type: git\"\n\n#### Preview Your Website in Local PC\n1. hexo server\n> The default port is 4000. You may want to change that with command hexo server -p <port>.\n2. Open you local pc for test.\n\n#### Deploy Website\n1. hexo generate\n2. hexo deploy\n> You may met problem when deploying.\n> See the troubleshoot for more information\n\n## Troubleshoots\n1. There is no response when running \"hexo deploy\"\n> Open \"_config.yml\" file and check if you have a space between the label and information. E.g., \"type: git\".\n\n2. Deployer not found: git\n> Becaue the older hexo use github, while the latest one(3.0) use git. So make sure you had typed \"type: git\" instead of \"type: github\". Then, install the git delopyer with command \"npm install hexo-deployer-git --save\"\n\n3. fatal: Not a git repository (or any of the parent directories): .git\n> Make sure you could run \"ssh -T git@github.com\" correctly. Then, delte the \".deploy_git\" directory, and run \"hexo deploy\" again.\n\n\n## Reference\n[1] [20分钟教你使用hexo搭建github博客](http://www.jianshu.com/p/e99ed60390a8)\n[2] [针对github权限导致hexo部署失败的解决方案][http://www.cnblogs.com/xsilence/p/6001938.html]\n\n\n\n<br>\n<br>\n---------------------------------------","slug":"Others/Hexos/2017-11-22-Deploy Blog With Hexo And Github Page","published":1,"updated":"2018-04-14T19:42:06.498Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hp003nrwtj2w6t44x6","content":"<h1 id=\"Deploy-Blog-With-Hexo-And-Github-Page\"><a href=\"#Deploy-Blog-With-Hexo-And-Github-Page\" class=\"headerlink\" title=\"Deploy Blog With Hexo And Github Page\"></a>Deploy Blog With Hexo And Github Page</h1><p>This blog aims to write down the process of deploying a personal website, and the difficulties which I have met.</p>\n<h2 id=\"Platform\"><a href=\"#Platform\" class=\"headerlink\" title=\"Platform:\"></a>Platform:</h2><ol>\n<li>Hexo</li>\n<li>Github Page</li>\n<li>Windows 10</li>\n</ol>\n<h2 id=\"Prerequisite\"><a href=\"#Prerequisite\" class=\"headerlink\" title=\"Prerequisite\"></a>Prerequisite</h2><ol>\n<li>Regist Github and Create a repository for github page</li>\n<li>Install Node.js in you local PC.</li>\n<li>Add the ssh key to your Github<blockquote>\n<p>Test: ssh -T git@github.com<br>If you run this command without error, it means you are good to go.</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"Hexo-Installation-and-Deployment\"><a href=\"#Hexo-Installation-and-Deployment\" class=\"headerlink\" title=\"Hexo Installation and Deployment\"></a>Hexo Installation and Deployment</h2><h3 id=\"Install-Hexo\"><a href=\"#Install-Hexo\" class=\"headerlink\" title=\"Install Hexo\"></a>Install Hexo</h3><ol>\n<li>mkdir <folder for=\"\" hexo=\"\"></folder></li>\n<li>cd <folder for=\"\" hexo=\"\"></folder></li>\n<li>npm install -g hexo-cli</li>\n<li>hexo<blockquote>\n<p>you should have get this message.</p>\n</blockquote>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">D:\\Blog\\zhichengML&gt; hexo</div><div class=\"line\">Usage: hexo &lt;command&gt;</div><div class=\"line\">Commands:</div><div class=\"line\">  clean     Remove generated files and cache.</div><div class=\"line\">  config    Get or set configurations.</div><div class=\"line\">  deploy    Deploy your website.</div><div class=\"line\">  generate  Generate static files.</div><div class=\"line\">  help      Get help on a command.</div><div class=\"line\">  init      Create a new Hexo folder.</div><div class=\"line\">  list      List the information of the site</div><div class=\"line\">  migrate   Migrate your site from other system to Hexo.</div><div class=\"line\">  new       Create a new post.</div><div class=\"line\">  publish   Moves a draft post from _drafts to _posts folder.</div><div class=\"line\">  render    Render files with renderer plugins.</div><div class=\"line\">  server    Start the server.</div><div class=\"line\">  version   Display version information.</div><div class=\"line\"></div><div class=\"line\">Global Options:</div><div class=\"line\">  --config  Specify config file instead of using _config.yml</div><div class=\"line\">  --cwd     Specify the CWD</div><div class=\"line\">  --debug   Display all verbose messages in the terminal</div><div class=\"line\">  --draft   Display draft posts</div><div class=\"line\">  --safe    Disable all plugins and scripts</div><div class=\"line\">  --silent  Hide output on console</div><div class=\"line\"></div><div class=\"line\">For more help, you can use &apos;hexo help [command]&apos; for the detailed information</div><div class=\"line\">or you can check the docs: http://hexo.io/docs/</div></pre></td></tr></table></figure>\n<h3 id=\"Deploy-Website\"><a href=\"#Deploy-Website\" class=\"headerlink\" title=\"Deploy Website\"></a>Deploy Website</h3><h4 id=\"Init-Hexo\"><a href=\"#Init-Hexo\" class=\"headerlink\" title=\"Init Hexo\"></a>Init Hexo</h4><ol>\n<li>hexo init <blog name=\"\"></blog></li>\n<li>cd <blog name=\"\"></blog></li>\n<li>npm install</li>\n</ol>\n<h4 id=\"Config-Hexo\"><a href=\"#Config-Hexo\" class=\"headerlink\" title=\"Config Hexo\"></a>Config Hexo</h4><ol>\n<li>Open “_config.yml” file and override this file with your information<blockquote>\n<p>The Origin file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Hexo Configuration</div><div class=\"line\">## Docs: https://hexo.io/docs/configuration.html</div><div class=\"line\">## Source: https://github.com/hexojs/hexo/</div><div class=\"line\"></div><div class=\"line\"># Site</div><div class=\"line\">title: # The title of your website</div><div class=\"line\">subtitle: # The subtitle of your website</div><div class=\"line\">description: # The description of your website</div><div class=\"line\">author: # Your name</div><div class=\"line\">language: # The language of your website</div><div class=\"line\">timezone:</div><div class=\"line\"></div><div class=\"line\"># URL</div><div class=\"line\">## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;</div><div class=\"line\">url: http://yoursite.com/child</div><div class=\"line\">root: /</div><div class=\"line\">permalink: :year/:month/:day/:title/</div><div class=\"line\">permalink_defaults:</div><div class=\"line\"></div><div class=\"line\"># Directory</div><div class=\"line\">source_dir: source</div><div class=\"line\">public_dir: public</div><div class=\"line\">tag_dir: tags</div><div class=\"line\">archive_dir: archives</div><div class=\"line\">category_dir: categories</div><div class=\"line\">code_dir: downloads/code</div><div class=\"line\">i18n_dir: :lang</div><div class=\"line\">skip_render:</div><div class=\"line\"></div><div class=\"line\"># Writing</div><div class=\"line\">new_post_name: :title.md # File name of new posts</div><div class=\"line\">default_layout: post</div><div class=\"line\">titlecase: false # Transform title into titlecase</div><div class=\"line\">external_link: true # Open external links in new tab</div><div class=\"line\">filename_case: 0</div><div class=\"line\">render_drafts: false</div><div class=\"line\">post_asset_folder: false</div><div class=\"line\">relative_link: false</div><div class=\"line\">future: true</div><div class=\"line\">highlight:</div><div class=\"line\">  enable: true</div><div class=\"line\">  line_number: true</div><div class=\"line\">  auto_detect: false</div><div class=\"line\">  tab_replace:</div><div class=\"line\"></div><div class=\"line\"># Category &amp; Tag</div><div class=\"line\">default_category: uncategorized</div><div class=\"line\">category_map:</div><div class=\"line\">tag_map:</div><div class=\"line\"></div><div class=\"line\"># Date / Time format</div><div class=\"line\">## Hexo uses Moment.js to parse and display date</div><div class=\"line\">## You can customize the date format as defined in</div><div class=\"line\">## http://momentjs.com/docs/#/displaying/format/</div><div class=\"line\">date_format: YYYY-MM-DD</div><div class=\"line\">time_format: HH:mm:ss</div><div class=\"line\"></div><div class=\"line\"># Pagination</div><div class=\"line\">## Set per_page to 0 to disable pagination</div><div class=\"line\">per_page: 10</div><div class=\"line\">pagination_dir: page</div><div class=\"line\"></div><div class=\"line\"># Extensions</div><div class=\"line\">## Plugins: https://hexo.io/plugins/</div><div class=\"line\">## Themes: https://hexo.io/themes/</div><div class=\"line\">theme: landscape</div><div class=\"line\"></div><div class=\"line\"># Deployment</div><div class=\"line\">## Docs: https://hexo.io/docs/deployment.html</div><div class=\"line\">deploy:</div><div class=\"line\">  type:</div></pre></td></tr></table></figure>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>You should override at least “# Site” and “# Deployment” session<br>E.g.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Site</div><div class=\"line\">title: zhichengML</div><div class=\"line\">subtitle:</div><div class=\"line\">description: Personal Website</div><div class=\"line\">author: zhichengML</div><div class=\"line\">language: en</div><div class=\"line\">timezone: Asia/Shanghai</div></pre></td></tr></table></figure></p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Deployment</div><div class=\"line\">## Docs: https://hexo.io/docs/deployment.html</div><div class=\"line\">deploy:</div><div class=\"line\">  type: git</div><div class=\"line\">  repo: git@github.com:zhichengML/zhichengML.github.io.git</div><div class=\"line\">  branch: master</div></pre></td></tr></table></figure>\n<p>Note that, there must be a blank space “ “ between label and information, e.g., “type: git”</p>\n<h4 id=\"Preview-Your-Website-in-Local-PC\"><a href=\"#Preview-Your-Website-in-Local-PC\" class=\"headerlink\" title=\"Preview Your Website in Local PC\"></a>Preview Your Website in Local PC</h4><ol>\n<li>hexo server<blockquote>\n<p>The default port is 4000. You may want to change that with command hexo server -p <port>.</port></p>\n</blockquote>\n</li>\n<li>Open you local pc for test.</li>\n</ol>\n<h4 id=\"Deploy-Website-1\"><a href=\"#Deploy-Website-1\" class=\"headerlink\" title=\"Deploy Website\"></a>Deploy Website</h4><ol>\n<li>hexo generate</li>\n<li>hexo deploy<blockquote>\n<p>You may met problem when deploying.<br>See the troubleshoot for more information</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"Troubleshoots\"><a href=\"#Troubleshoots\" class=\"headerlink\" title=\"Troubleshoots\"></a>Troubleshoots</h2><ol>\n<li><p>There is no response when running “hexo deploy”</p>\n<blockquote>\n<p>Open “_config.yml” file and check if you have a space between the label and information. E.g., “type: git”.</p>\n</blockquote>\n</li>\n<li><p>Deployer not found: git</p>\n<blockquote>\n<p>Becaue the older hexo use github, while the latest one(3.0) use git. So make sure you had typed “type: git” instead of “type: github”. Then, install the git delopyer with command “npm install hexo-deployer-git —save”</p>\n</blockquote>\n</li>\n<li><p>fatal: Not a git repository (or any of the parent directories): .git</p>\n<blockquote>\n<p>Make sure you could run “ssh -T git@github.com” correctly. Then, delte the “.deploy_git” directory, and run “hexo deploy” again.</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] <a href=\"http://www.jianshu.com/p/e99ed60390a8\" target=\"_blank\" rel=\"external\">20分钟教你使用hexo搭建github博客</a><br>[2] [针对github权限导致hexo部署失败的解决方案][<a href=\"http://www.cnblogs.com/xsilence/p/6001938.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/xsilence/p/6001938.html</a>]</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Deploy-Blog-With-Hexo-And-Github-Page\"><a href=\"#Deploy-Blog-With-Hexo-And-Github-Page\" class=\"headerlink\" title=\"Deploy Blog With Hexo And Github Page\"></a>Deploy Blog With Hexo And Github Page</h1><p>This blog aims to write down the process of deploying a personal website, and the difficulties which I have met.</p>\n<h2 id=\"Platform\"><a href=\"#Platform\" class=\"headerlink\" title=\"Platform:\"></a>Platform:</h2><ol>\n<li>Hexo</li>\n<li>Github Page</li>\n<li>Windows 10</li>\n</ol>\n<h2 id=\"Prerequisite\"><a href=\"#Prerequisite\" class=\"headerlink\" title=\"Prerequisite\"></a>Prerequisite</h2><ol>\n<li>Regist Github and Create a repository for github page</li>\n<li>Install Node.js in you local PC.</li>\n<li>Add the ssh key to your Github<blockquote>\n<p>Test: ssh -T git@github.com<br>If you run this command without error, it means you are good to go.</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"Hexo-Installation-and-Deployment\"><a href=\"#Hexo-Installation-and-Deployment\" class=\"headerlink\" title=\"Hexo Installation and Deployment\"></a>Hexo Installation and Deployment</h2><h3 id=\"Install-Hexo\"><a href=\"#Install-Hexo\" class=\"headerlink\" title=\"Install Hexo\"></a>Install Hexo</h3><ol>\n<li>mkdir <folder for=\"\" hexo=\"\"></folder></li>\n<li>cd <folder for=\"\" hexo=\"\"></folder></li>\n<li>npm install -g hexo-cli</li>\n<li>hexo<blockquote>\n<p>you should have get this message.</p>\n</blockquote>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">D:\\Blog\\zhichengML&gt; hexo</div><div class=\"line\">Usage: hexo &lt;command&gt;</div><div class=\"line\">Commands:</div><div class=\"line\">  clean     Remove generated files and cache.</div><div class=\"line\">  config    Get or set configurations.</div><div class=\"line\">  deploy    Deploy your website.</div><div class=\"line\">  generate  Generate static files.</div><div class=\"line\">  help      Get help on a command.</div><div class=\"line\">  init      Create a new Hexo folder.</div><div class=\"line\">  list      List the information of the site</div><div class=\"line\">  migrate   Migrate your site from other system to Hexo.</div><div class=\"line\">  new       Create a new post.</div><div class=\"line\">  publish   Moves a draft post from _drafts to _posts folder.</div><div class=\"line\">  render    Render files with renderer plugins.</div><div class=\"line\">  server    Start the server.</div><div class=\"line\">  version   Display version information.</div><div class=\"line\"></div><div class=\"line\">Global Options:</div><div class=\"line\">  --config  Specify config file instead of using _config.yml</div><div class=\"line\">  --cwd     Specify the CWD</div><div class=\"line\">  --debug   Display all verbose messages in the terminal</div><div class=\"line\">  --draft   Display draft posts</div><div class=\"line\">  --safe    Disable all plugins and scripts</div><div class=\"line\">  --silent  Hide output on console</div><div class=\"line\"></div><div class=\"line\">For more help, you can use &apos;hexo help [command]&apos; for the detailed information</div><div class=\"line\">or you can check the docs: http://hexo.io/docs/</div></pre></td></tr></table></figure>\n<h3 id=\"Deploy-Website\"><a href=\"#Deploy-Website\" class=\"headerlink\" title=\"Deploy Website\"></a>Deploy Website</h3><h4 id=\"Init-Hexo\"><a href=\"#Init-Hexo\" class=\"headerlink\" title=\"Init Hexo\"></a>Init Hexo</h4><ol>\n<li>hexo init <blog name=\"\"></blog></li>\n<li>cd <blog name=\"\"></blog></li>\n<li>npm install</li>\n</ol>\n<h4 id=\"Config-Hexo\"><a href=\"#Config-Hexo\" class=\"headerlink\" title=\"Config Hexo\"></a>Config Hexo</h4><ol>\n<li>Open “_config.yml” file and override this file with your information<blockquote>\n<p>The Origin file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Hexo Configuration</div><div class=\"line\">## Docs: https://hexo.io/docs/configuration.html</div><div class=\"line\">## Source: https://github.com/hexojs/hexo/</div><div class=\"line\"></div><div class=\"line\"># Site</div><div class=\"line\">title: # The title of your website</div><div class=\"line\">subtitle: # The subtitle of your website</div><div class=\"line\">description: # The description of your website</div><div class=\"line\">author: # Your name</div><div class=\"line\">language: # The language of your website</div><div class=\"line\">timezone:</div><div class=\"line\"></div><div class=\"line\"># URL</div><div class=\"line\">## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;</div><div class=\"line\">url: http://yoursite.com/child</div><div class=\"line\">root: /</div><div class=\"line\">permalink: :year/:month/:day/:title/</div><div class=\"line\">permalink_defaults:</div><div class=\"line\"></div><div class=\"line\"># Directory</div><div class=\"line\">source_dir: source</div><div class=\"line\">public_dir: public</div><div class=\"line\">tag_dir: tags</div><div class=\"line\">archive_dir: archives</div><div class=\"line\">category_dir: categories</div><div class=\"line\">code_dir: downloads/code</div><div class=\"line\">i18n_dir: :lang</div><div class=\"line\">skip_render:</div><div class=\"line\"></div><div class=\"line\"># Writing</div><div class=\"line\">new_post_name: :title.md # File name of new posts</div><div class=\"line\">default_layout: post</div><div class=\"line\">titlecase: false # Transform title into titlecase</div><div class=\"line\">external_link: true # Open external links in new tab</div><div class=\"line\">filename_case: 0</div><div class=\"line\">render_drafts: false</div><div class=\"line\">post_asset_folder: false</div><div class=\"line\">relative_link: false</div><div class=\"line\">future: true</div><div class=\"line\">highlight:</div><div class=\"line\">  enable: true</div><div class=\"line\">  line_number: true</div><div class=\"line\">  auto_detect: false</div><div class=\"line\">  tab_replace:</div><div class=\"line\"></div><div class=\"line\"># Category &amp; Tag</div><div class=\"line\">default_category: uncategorized</div><div class=\"line\">category_map:</div><div class=\"line\">tag_map:</div><div class=\"line\"></div><div class=\"line\"># Date / Time format</div><div class=\"line\">## Hexo uses Moment.js to parse and display date</div><div class=\"line\">## You can customize the date format as defined in</div><div class=\"line\">## http://momentjs.com/docs/#/displaying/format/</div><div class=\"line\">date_format: YYYY-MM-DD</div><div class=\"line\">time_format: HH:mm:ss</div><div class=\"line\"></div><div class=\"line\"># Pagination</div><div class=\"line\">## Set per_page to 0 to disable pagination</div><div class=\"line\">per_page: 10</div><div class=\"line\">pagination_dir: page</div><div class=\"line\"></div><div class=\"line\"># Extensions</div><div class=\"line\">## Plugins: https://hexo.io/plugins/</div><div class=\"line\">## Themes: https://hexo.io/themes/</div><div class=\"line\">theme: landscape</div><div class=\"line\"></div><div class=\"line\"># Deployment</div><div class=\"line\">## Docs: https://hexo.io/docs/deployment.html</div><div class=\"line\">deploy:</div><div class=\"line\">  type:</div></pre></td></tr></table></figure>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>You should override at least “# Site” and “# Deployment” session<br>E.g.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Site</div><div class=\"line\">title: zhichengML</div><div class=\"line\">subtitle:</div><div class=\"line\">description: Personal Website</div><div class=\"line\">author: zhichengML</div><div class=\"line\">language: en</div><div class=\"line\">timezone: Asia/Shanghai</div></pre></td></tr></table></figure></p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Deployment</div><div class=\"line\">## Docs: https://hexo.io/docs/deployment.html</div><div class=\"line\">deploy:</div><div class=\"line\">  type: git</div><div class=\"line\">  repo: git@github.com:zhichengML/zhichengML.github.io.git</div><div class=\"line\">  branch: master</div></pre></td></tr></table></figure>\n<p>Note that, there must be a blank space “ “ between label and information, e.g., “type: git”</p>\n<h4 id=\"Preview-Your-Website-in-Local-PC\"><a href=\"#Preview-Your-Website-in-Local-PC\" class=\"headerlink\" title=\"Preview Your Website in Local PC\"></a>Preview Your Website in Local PC</h4><ol>\n<li>hexo server<blockquote>\n<p>The default port is 4000. You may want to change that with command hexo server -p <port>.</port></p>\n</blockquote>\n</li>\n<li>Open you local pc for test.</li>\n</ol>\n<h4 id=\"Deploy-Website-1\"><a href=\"#Deploy-Website-1\" class=\"headerlink\" title=\"Deploy Website\"></a>Deploy Website</h4><ol>\n<li>hexo generate</li>\n<li>hexo deploy<blockquote>\n<p>You may met problem when deploying.<br>See the troubleshoot for more information</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"Troubleshoots\"><a href=\"#Troubleshoots\" class=\"headerlink\" title=\"Troubleshoots\"></a>Troubleshoots</h2><ol>\n<li><p>There is no response when running “hexo deploy”</p>\n<blockquote>\n<p>Open “_config.yml” file and check if you have a space between the label and information. E.g., “type: git”.</p>\n</blockquote>\n</li>\n<li><p>Deployer not found: git</p>\n<blockquote>\n<p>Becaue the older hexo use github, while the latest one(3.0) use git. So make sure you had typed “type: git” instead of “type: github”. Then, install the git delopyer with command “npm install hexo-deployer-git —save”</p>\n</blockquote>\n</li>\n<li><p>fatal: Not a git repository (or any of the parent directories): .git</p>\n<blockquote>\n<p>Make sure you could run “ssh -T git@github.com” correctly. Then, delte the “.deploy_git” directory, and run “hexo deploy” again.</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] <a href=\"http://www.jianshu.com/p/e99ed60390a8\" target=\"_blank\" rel=\"external\">20分钟教你使用hexo搭建github博客</a><br>[2] [针对github权限导致hexo部署失败的解决方案][<a href=\"http://www.cnblogs.com/xsilence/p/6001938.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/xsilence/p/6001938.html</a>]</p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"Practical Derivatives","date":"2017-11-03T09:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Practical Derivatives\n\n\n## 1. Power Function\n\nGiven : $f \\left( x \\right) = x^a \\text(a \\in Q)$\n\nProofs : $f'\\left( x \\right) = a \\cdot x^{a-1}$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(x + \\Delta x\\right)^a - x^a}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(C_a^a x^a+C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) - x^a}{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(C_a^a x^a- x^a \\right) + \\left(C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) }{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 0 + \\left(C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) }{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} { C_a^1x^{a-1} + \\cdots \\Delta x^{a-1} } \\\\\n& = C_a^1x^{a-1} \\\\\n& = a x^{a-1}\n\\end{align}\n$$\n\n\n## 2. Sinus Function\nGiven : $f \\left( x \\right) = sin\\left(x\\right)$\n\nProofs : $f'\\left( x \\right) = cos\\left(x\\right)$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin \\left(x + \\Delta x\\right) - \\sin x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ (\\sin x \\cos {\\Delta x} +  \\cos x \\sin {\\Delta x}) - \\sin x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ (\\sin x \\cos {\\Delta x} - \\sin x) +  \\cos x \\sin {\\Delta x}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin x( \\cos {\\Delta x} - 1) +  \\cos x \\sin {\\Delta x}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin x \\left[\\left( 1-2 \\sin^2{\\frac{\\Delta x}{2}} \\right) -1 \\right] + \\cos x \\left( 2 \\sin{\\frac{\\Delta x}{2} \\cos{\\frac{\\Delta x}{2}}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ -2 \\sin x  \\sin^2{\\frac{\\Delta x}{2}} + 2 \\sin{\\frac{\\Delta x}{2}} \\left( \\cos x  \\cos{\\frac{\\Delta x}{2}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{  2 \\sin{\\frac{\\Delta x}{2}} \\left( \\cos x  \\cos{\\frac{\\Delta x}{2}} - \\sin x  \\sin^2{\\frac{\\Delta x}{2}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{  2 \\sin{\\frac{\\Delta x}{2}} \\cos\\left( x + \\frac{\\Delta x}{2} \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin{\\frac{\\Delta x}{2}} \\cos\\left( x + \\frac{\\Delta x}{2} \\right)}{\\frac{\\Delta x}{2}} \\\\\n&\\because \\lim\\limits_{\\Delta x \\rightarrow0} \\sin \\frac{\\Delta x}{2} =  \\frac{\\Delta x}{2} \\\\\n&\\therefore  \\frac{  \\sin{\\frac{\\Delta x}{2}} }{\\frac{\\Delta x}{2}} = 1 \\\\\n&\\because  \\lim\\limits_{\\Delta x \\rightarrow0} \\cos\\left( x + \\frac{\\Delta x}{2} \\right) = \\cos \\left( x  + 0\\right) = \\cos x \\\\\n&\\therefore f'(\\sin x) = \\cos x\n\\end{align}\n$$\n\n\n\n\n\n## 3. Cosinus Function\nGiven : $f \\left( x \\right) = \\cos(x)$\n\nProofs : $f'\\left( x \\right) = - \\sin(x)$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos\\left(x + \\Delta x\\right) - \\cos x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left( \\cos x \\cos \\Delta x - \\sin x \\sin \\Delta x\\right) - \\cos x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left( \\cos x \\cos \\Delta x  - \\cos x \\right) - \\sin x \\sin \\Delta x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos x \\left( \\cos \\Delta x  - 1 \\right) - \\sin x \\sin \\Delta x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos x \\left[\\left( 1 - 2 \\sin^2\\frac{\\Delta x}{2} \\right) -1 \\right] - \\sin x \\left( 2 \\sin \\frac{\\Delta x}{2} \\cos\\frac{\\Delta x}{2} \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{2 \\sin^2 \\frac{\\Delta x}{2} \\cos x - 2 \\sin \\frac{\\Delta x}{2} \\left( \\sin x \\cos \\frac{\\Delta x}{2} \\right)}{\\Delta x}\\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 2 \\sin \\frac{\\Delta x}{2} \\left( \\sin \\frac{\\Delta x}{2} \\cos x - \\cos \\frac{\\Delta x}{2} \\sin x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 2 \\sin \\frac{\\Delta x}{2} \\sin \\left( \\frac{\\Delta x}{2} -x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\left[ \\sin \\left( \\frac{\\Delta x}{2} - x  \\right) \\frac{\\sin\\frac{\\Delta x}{2}}{\\frac{\\Delta x}{2}} \\right] \\\\\n&\\because \\lim\\limits_{\\Delta x \\rightarrow0} \\sin \\frac{\\Delta x}{2} =  \\frac{\\Delta x}{2} \\\\\n&\\therefore  \\frac{  \\sin{\\frac{\\Delta x}{2}} }{\\frac{\\Delta x}{2}} = 1 \\\\\n&\\because  \\lim\\limits_{\\Delta x \\rightarrow0} \\sin\\left( \\frac{\\Delta x}{2} - x \\right) = \\sin \\left( 0 - x \\right) = \\sin (-x)  = - \\sin x\\\\\n&\\therefore f'(\\cos x) = - \\sin x\n\\end{align}\n$$\n\n\n\n\n## 4. Exponential Function\nGiven : $f \\left( x \\right) = a ^ x \\text( a>0 \\&  a \\neq 1 )$\n\nProofs : $f'\\left( x \\right) = a^x \\ln a$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left( x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ a^{x + \\Delta x} - a^x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ a^x \\left( a^{\\Delta x} -1 \\right)}{\\Delta x} \\\\\nintroduce \\quad\n& t = a^{\\Delta x} - 1, then \\quad a^{\\Delta x} = t + 1, \\quad \\Delta x = \\log_a\\left( t+1 \\right)\\\\\nequation  \\quad\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x t}{\\log_a(t+1)}\\\\\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\frac{1}{t} \\log_a(t+1)}\\\\\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\log_a(t+1)^{\\frac{1}{t} }}\\\\\n&\\because  \\lim\\limits_{t \\rightarrow0} (t+1)^{\\frac{1}{t}} = e \\\\\n&\\therefore  \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\log_a\\left( t+1 \\right)^{\\frac{1}{t} }} = \\frac{ a^x}{\\log_ae}  = a^x \\cdot \\frac{ \\ln a }{\\ln e} = a^x \\ln a\\\\\n&\\therefore f'(a ^ x ) = - a^x \\ln a\n\\end{align}\n$$\n\n\n## 5. Logarithmic function\nGiven : $f \\left( x \\right) = \\log_a x \\text( x>0 \\&  a \\neq 1 )$\n\nProofs : $f'\\left( x \\right) = \\frac{1}{x\\ln a}$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f \\left(x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\log_a \\left(x + \\Delta x \\right) - \\log_a x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\log_a(\\frac{x + \\Delta x}{x})}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x}\\frac{x }{\\Delta x} \\log_a \\left(\\frac{x + \\Delta x}{x}\\right) \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ \\frac{  \\Delta x}{x} \\right)^{\\frac{x }{\\Delta x}} \\\\\nintroduce \\quad\n& t = \\frac{  \\Delta x}{x}\\\\\nequation  \\quad\n&=\\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ t \\right)^{\\frac{1 }{t}} \\\\\n&\\because  \\lim\\limits_{t \\rightarrow0} \\left( t+1 \\right)^{\\frac{1}{t}} = e \\\\\n&\\therefore  \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ t \\right)^{\\frac{1 }{t}} = \\frac{1}{x} \\frac{\\ln e}{\\ln a} = \\frac{1}{ x \\ln a} \\\\\n&\\therefore f'\\left(\\log_a x\\right) = \\frac{1}{x\\ln a}\n\\end{align}\n$$\n\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/Calculus/2017-11-03-Practical Derivatives.md","raw":"---\ntitle: Practical Derivatives\ndate: 2017-11-03 17:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics, Calculus]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Practical Derivatives\n\n\n## 1. Power Function\n\nGiven : $f \\left( x \\right) = x^a \\text(a \\in Q)$\n\nProofs : $f'\\left( x \\right) = a \\cdot x^{a-1}$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(x + \\Delta x\\right)^a - x^a}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(C_a^a x^a+C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) - x^a}{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(C_a^a x^a- x^a \\right) + \\left(C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) }{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 0 + \\left(C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) }{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} { C_a^1x^{a-1} + \\cdots \\Delta x^{a-1} } \\\\\n& = C_a^1x^{a-1} \\\\\n& = a x^{a-1}\n\\end{align}\n$$\n\n\n## 2. Sinus Function\nGiven : $f \\left( x \\right) = sin\\left(x\\right)$\n\nProofs : $f'\\left( x \\right) = cos\\left(x\\right)$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin \\left(x + \\Delta x\\right) - \\sin x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ (\\sin x \\cos {\\Delta x} +  \\cos x \\sin {\\Delta x}) - \\sin x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ (\\sin x \\cos {\\Delta x} - \\sin x) +  \\cos x \\sin {\\Delta x}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin x( \\cos {\\Delta x} - 1) +  \\cos x \\sin {\\Delta x}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin x \\left[\\left( 1-2 \\sin^2{\\frac{\\Delta x}{2}} \\right) -1 \\right] + \\cos x \\left( 2 \\sin{\\frac{\\Delta x}{2} \\cos{\\frac{\\Delta x}{2}}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ -2 \\sin x  \\sin^2{\\frac{\\Delta x}{2}} + 2 \\sin{\\frac{\\Delta x}{2}} \\left( \\cos x  \\cos{\\frac{\\Delta x}{2}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{  2 \\sin{\\frac{\\Delta x}{2}} \\left( \\cos x  \\cos{\\frac{\\Delta x}{2}} - \\sin x  \\sin^2{\\frac{\\Delta x}{2}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{  2 \\sin{\\frac{\\Delta x}{2}} \\cos\\left( x + \\frac{\\Delta x}{2} \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin{\\frac{\\Delta x}{2}} \\cos\\left( x + \\frac{\\Delta x}{2} \\right)}{\\frac{\\Delta x}{2}} \\\\\n&\\because \\lim\\limits_{\\Delta x \\rightarrow0} \\sin \\frac{\\Delta x}{2} =  \\frac{\\Delta x}{2} \\\\\n&\\therefore  \\frac{  \\sin{\\frac{\\Delta x}{2}} }{\\frac{\\Delta x}{2}} = 1 \\\\\n&\\because  \\lim\\limits_{\\Delta x \\rightarrow0} \\cos\\left( x + \\frac{\\Delta x}{2} \\right) = \\cos \\left( x  + 0\\right) = \\cos x \\\\\n&\\therefore f'(\\sin x) = \\cos x\n\\end{align}\n$$\n\n\n\n\n\n## 3. Cosinus Function\nGiven : $f \\left( x \\right) = \\cos(x)$\n\nProofs : $f'\\left( x \\right) = - \\sin(x)$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos\\left(x + \\Delta x\\right) - \\cos x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left( \\cos x \\cos \\Delta x - \\sin x \\sin \\Delta x\\right) - \\cos x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left( \\cos x \\cos \\Delta x  - \\cos x \\right) - \\sin x \\sin \\Delta x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos x \\left( \\cos \\Delta x  - 1 \\right) - \\sin x \\sin \\Delta x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos x \\left[\\left( 1 - 2 \\sin^2\\frac{\\Delta x}{2} \\right) -1 \\right] - \\sin x \\left( 2 \\sin \\frac{\\Delta x}{2} \\cos\\frac{\\Delta x}{2} \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{2 \\sin^2 \\frac{\\Delta x}{2} \\cos x - 2 \\sin \\frac{\\Delta x}{2} \\left( \\sin x \\cos \\frac{\\Delta x}{2} \\right)}{\\Delta x}\\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 2 \\sin \\frac{\\Delta x}{2} \\left( \\sin \\frac{\\Delta x}{2} \\cos x - \\cos \\frac{\\Delta x}{2} \\sin x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 2 \\sin \\frac{\\Delta x}{2} \\sin \\left( \\frac{\\Delta x}{2} -x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\left[ \\sin \\left( \\frac{\\Delta x}{2} - x  \\right) \\frac{\\sin\\frac{\\Delta x}{2}}{\\frac{\\Delta x}{2}} \\right] \\\\\n&\\because \\lim\\limits_{\\Delta x \\rightarrow0} \\sin \\frac{\\Delta x}{2} =  \\frac{\\Delta x}{2} \\\\\n&\\therefore  \\frac{  \\sin{\\frac{\\Delta x}{2}} }{\\frac{\\Delta x}{2}} = 1 \\\\\n&\\because  \\lim\\limits_{\\Delta x \\rightarrow0} \\sin\\left( \\frac{\\Delta x}{2} - x \\right) = \\sin \\left( 0 - x \\right) = \\sin (-x)  = - \\sin x\\\\\n&\\therefore f'(\\cos x) = - \\sin x\n\\end{align}\n$$\n\n\n\n\n## 4. Exponential Function\nGiven : $f \\left( x \\right) = a ^ x \\text( a>0 \\&  a \\neq 1 )$\n\nProofs : $f'\\left( x \\right) = a^x \\ln a$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left( x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ a^{x + \\Delta x} - a^x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ a^x \\left( a^{\\Delta x} -1 \\right)}{\\Delta x} \\\\\nintroduce \\quad\n& t = a^{\\Delta x} - 1, then \\quad a^{\\Delta x} = t + 1, \\quad \\Delta x = \\log_a\\left( t+1 \\right)\\\\\nequation  \\quad\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x t}{\\log_a(t+1)}\\\\\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\frac{1}{t} \\log_a(t+1)}\\\\\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\log_a(t+1)^{\\frac{1}{t} }}\\\\\n&\\because  \\lim\\limits_{t \\rightarrow0} (t+1)^{\\frac{1}{t}} = e \\\\\n&\\therefore  \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\log_a\\left( t+1 \\right)^{\\frac{1}{t} }} = \\frac{ a^x}{\\log_ae}  = a^x \\cdot \\frac{ \\ln a }{\\ln e} = a^x \\ln a\\\\\n&\\therefore f'(a ^ x ) = - a^x \\ln a\n\\end{align}\n$$\n\n\n## 5. Logarithmic function\nGiven : $f \\left( x \\right) = \\log_a x \\text( x>0 \\&  a \\neq 1 )$\n\nProofs : $f'\\left( x \\right) = \\frac{1}{x\\ln a}$\n\n> Deduction\n\n$$\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f \\left(x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\log_a \\left(x + \\Delta x \\right) - \\log_a x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\log_a(\\frac{x + \\Delta x}{x})}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x}\\frac{x }{\\Delta x} \\log_a \\left(\\frac{x + \\Delta x}{x}\\right) \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ \\frac{  \\Delta x}{x} \\right)^{\\frac{x }{\\Delta x}} \\\\\nintroduce \\quad\n& t = \\frac{  \\Delta x}{x}\\\\\nequation  \\quad\n&=\\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ t \\right)^{\\frac{1 }{t}} \\\\\n&\\because  \\lim\\limits_{t \\rightarrow0} \\left( t+1 \\right)^{\\frac{1}{t}} = e \\\\\n&\\therefore  \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ t \\right)^{\\frac{1 }{t}} = \\frac{1}{x} \\frac{\\ln e}{\\ln a} = \\frac{1}{ x \\ln a} \\\\\n&\\therefore f'\\left(\\log_a x\\right) = \\frac{1}{x\\ln a}\n\\end{align}\n$$\n\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/Calculus/2017-11-03-Practical Derivatives","published":1,"updated":"2018-04-14T19:42:06.493Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hq003prwtj98zbd0x9","content":"<h1 id=\"Practical-Derivatives\"><a href=\"#Practical-Derivatives\" class=\"headerlink\" title=\"Practical Derivatives\"></a>Practical Derivatives</h1><h2 id=\"1-Power-Function\"><a href=\"#1-Power-Function\" class=\"headerlink\" title=\"1. Power Function\"></a>1. Power Function</h2><p>Given : $f \\left( x \\right) = x^a \\text(a \\in Q)$</p>\n<p>Proofs : $f’\\left( x \\right) = a \\cdot x^{a-1}$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(x + \\Delta x\\right)^a - x^a}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(C_a^a x^a+C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) - x^a}{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(C_a^a x^a- x^a \\right) + \\left(C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) }{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 0 + \\left(C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) }{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} { C_a^1x^{a-1} + \\cdots \\Delta x^{a-1} } \\\\\n& = C_a^1x^{a-1} \\\\\n& = a x^{a-1}\n\\end{align}</script><h2 id=\"2-Sinus-Function\"><a href=\"#2-Sinus-Function\" class=\"headerlink\" title=\"2. Sinus Function\"></a>2. Sinus Function</h2><p>Given : $f \\left( x \\right) = sin\\left(x\\right)$</p>\n<p>Proofs : $f’\\left( x \\right) = cos\\left(x\\right)$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin \\left(x + \\Delta x\\right) - \\sin x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ (\\sin x \\cos {\\Delta x} +  \\cos x \\sin {\\Delta x}) - \\sin x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ (\\sin x \\cos {\\Delta x} - \\sin x) +  \\cos x \\sin {\\Delta x}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin x( \\cos {\\Delta x} - 1) +  \\cos x \\sin {\\Delta x}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin x \\left[\\left( 1-2 \\sin^2{\\frac{\\Delta x}{2}} \\right) -1 \\right] + \\cos x \\left( 2 \\sin{\\frac{\\Delta x}{2} \\cos{\\frac{\\Delta x}{2}}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ -2 \\sin x  \\sin^2{\\frac{\\Delta x}{2}} + 2 \\sin{\\frac{\\Delta x}{2}} \\left( \\cos x  \\cos{\\frac{\\Delta x}{2}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{  2 \\sin{\\frac{\\Delta x}{2}} \\left( \\cos x  \\cos{\\frac{\\Delta x}{2}} - \\sin x  \\sin^2{\\frac{\\Delta x}{2}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{  2 \\sin{\\frac{\\Delta x}{2}} \\cos\\left( x + \\frac{\\Delta x}{2} \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin{\\frac{\\Delta x}{2}} \\cos\\left( x + \\frac{\\Delta x}{2} \\right)}{\\frac{\\Delta x}{2}} \\\\\n&\\because \\lim\\limits_{\\Delta x \\rightarrow0} \\sin \\frac{\\Delta x}{2} =  \\frac{\\Delta x}{2} \\\\\n&\\therefore  \\frac{  \\sin{\\frac{\\Delta x}{2}} }{\\frac{\\Delta x}{2}} = 1 \\\\\n&\\because  \\lim\\limits_{\\Delta x \\rightarrow0} \\cos\\left( x + \\frac{\\Delta x}{2} \\right) = \\cos \\left( x  + 0\\right) = \\cos x \\\\\n&\\therefore f'(\\sin x) = \\cos x\n\\end{align}</script><h2 id=\"3-Cosinus-Function\"><a href=\"#3-Cosinus-Function\" class=\"headerlink\" title=\"3. Cosinus Function\"></a>3. Cosinus Function</h2><p>Given : $f \\left( x \\right) = \\cos(x)$</p>\n<p>Proofs : $f’\\left( x \\right) = - \\sin(x)$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos\\left(x + \\Delta x\\right) - \\cos x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left( \\cos x \\cos \\Delta x - \\sin x \\sin \\Delta x\\right) - \\cos x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left( \\cos x \\cos \\Delta x  - \\cos x \\right) - \\sin x \\sin \\Delta x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos x \\left( \\cos \\Delta x  - 1 \\right) - \\sin x \\sin \\Delta x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos x \\left[\\left( 1 - 2 \\sin^2\\frac{\\Delta x}{2} \\right) -1 \\right] - \\sin x \\left( 2 \\sin \\frac{\\Delta x}{2} \\cos\\frac{\\Delta x}{2} \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{2 \\sin^2 \\frac{\\Delta x}{2} \\cos x - 2 \\sin \\frac{\\Delta x}{2} \\left( \\sin x \\cos \\frac{\\Delta x}{2} \\right)}{\\Delta x}\\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 2 \\sin \\frac{\\Delta x}{2} \\left( \\sin \\frac{\\Delta x}{2} \\cos x - \\cos \\frac{\\Delta x}{2} \\sin x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 2 \\sin \\frac{\\Delta x}{2} \\sin \\left( \\frac{\\Delta x}{2} -x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\left[ \\sin \\left( \\frac{\\Delta x}{2} - x  \\right) \\frac{\\sin\\frac{\\Delta x}{2}}{\\frac{\\Delta x}{2}} \\right] \\\\\n&\\because \\lim\\limits_{\\Delta x \\rightarrow0} \\sin \\frac{\\Delta x}{2} =  \\frac{\\Delta x}{2} \\\\\n&\\therefore  \\frac{  \\sin{\\frac{\\Delta x}{2}} }{\\frac{\\Delta x}{2}} = 1 \\\\\n&\\because  \\lim\\limits_{\\Delta x \\rightarrow0} \\sin\\left( \\frac{\\Delta x}{2} - x \\right) = \\sin \\left( 0 - x \\right) = \\sin (-x)  = - \\sin x\\\\\n&\\therefore f'(\\cos x) = - \\sin x\n\\end{align}</script><h2 id=\"4-Exponential-Function\"><a href=\"#4-Exponential-Function\" class=\"headerlink\" title=\"4. Exponential Function\"></a>4. Exponential Function</h2><p>Given : $f \\left( x \\right) = a ^ x \\text( a&gt;0 \\&amp;  a \\neq 1 )$</p>\n<p>Proofs : $f’\\left( x \\right) = a^x \\ln a$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left( x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ a^{x + \\Delta x} - a^x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ a^x \\left( a^{\\Delta x} -1 \\right)}{\\Delta x} \\\\\nintroduce \\quad\n& t = a^{\\Delta x} - 1, then \\quad a^{\\Delta x} = t + 1, \\quad \\Delta x = \\log_a\\left( t+1 \\right)\\\\\nequation  \\quad\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x t}{\\log_a(t+1)}\\\\\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\frac{1}{t} \\log_a(t+1)}\\\\\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\log_a(t+1)^{\\frac{1}{t} }}\\\\\n&\\because  \\lim\\limits_{t \\rightarrow0} (t+1)^{\\frac{1}{t}} = e \\\\\n&\\therefore  \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\log_a\\left( t+1 \\right)^{\\frac{1}{t} }} = \\frac{ a^x}{\\log_ae}  = a^x \\cdot \\frac{ \\ln a }{\\ln e} = a^x \\ln a\\\\\n&\\therefore f'(a ^ x ) = - a^x \\ln a\n\\end{align}</script><h2 id=\"5-Logarithmic-function\"><a href=\"#5-Logarithmic-function\" class=\"headerlink\" title=\"5. Logarithmic function\"></a>5. Logarithmic function</h2><p>Given : $f \\left( x \\right) = \\log_a x \\text( x&gt;0 \\&amp;  a \\neq 1 )$</p>\n<p>Proofs : $f’\\left( x \\right) = \\frac{1}{x\\ln a}$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f \\left(x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\log_a \\left(x + \\Delta x \\right) - \\log_a x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\log_a(\\frac{x + \\Delta x}{x})}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x}\\frac{x }{\\Delta x} \\log_a \\left(\\frac{x + \\Delta x}{x}\\right) \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ \\frac{  \\Delta x}{x} \\right)^{\\frac{x }{\\Delta x}} \\\\\nintroduce \\quad\n& t = \\frac{  \\Delta x}{x}\\\\\nequation  \\quad\n&=\\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ t \\right)^{\\frac{1 }{t}} \\\\\n&\\because  \\lim\\limits_{t \\rightarrow0} \\left( t+1 \\right)^{\\frac{1}{t}} = e \\\\\n&\\therefore  \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ t \\right)^{\\frac{1 }{t}} = \\frac{1}{x} \\frac{\\ln e}{\\ln a} = \\frac{1}{ x \\ln a} \\\\\n&\\therefore f'\\left(\\log_a x\\right) = \\frac{1}{x\\ln a}\n\\end{align}</script><p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Practical-Derivatives\"><a href=\"#Practical-Derivatives\" class=\"headerlink\" title=\"Practical Derivatives\"></a>Practical Derivatives</h1><h2 id=\"1-Power-Function\"><a href=\"#1-Power-Function\" class=\"headerlink\" title=\"1. Power Function\"></a>1. Power Function</h2><p>Given : $f \\left( x \\right) = x^a \\text(a \\in Q)$</p>\n<p>Proofs : $f’\\left( x \\right) = a \\cdot x^{a-1}$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(x + \\Delta x\\right)^a - x^a}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(C_a^a x^a+C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) - x^a}{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left(C_a^a x^a- x^a \\right) + \\left(C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) }{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 0 + \\left(C_a^1x^{a-1}\\Delta x + \\cdots \\Delta x^a\\right) }{\\Delta x} \\\\\n& = \\lim\\limits_{\\Delta x \\rightarrow0} { C_a^1x^{a-1} + \\cdots \\Delta x^{a-1} } \\\\\n& = C_a^1x^{a-1} \\\\\n& = a x^{a-1}\n\\end{align}</script><h2 id=\"2-Sinus-Function\"><a href=\"#2-Sinus-Function\" class=\"headerlink\" title=\"2. Sinus Function\"></a>2. Sinus Function</h2><p>Given : $f \\left( x \\right) = sin\\left(x\\right)$</p>\n<p>Proofs : $f’\\left( x \\right) = cos\\left(x\\right)$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x\\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin \\left(x + \\Delta x\\right) - \\sin x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ (\\sin x \\cos {\\Delta x} +  \\cos x \\sin {\\Delta x}) - \\sin x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ (\\sin x \\cos {\\Delta x} - \\sin x) +  \\cos x \\sin {\\Delta x}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin x( \\cos {\\Delta x} - 1) +  \\cos x \\sin {\\Delta x}}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin x \\left[\\left( 1-2 \\sin^2{\\frac{\\Delta x}{2}} \\right) -1 \\right] + \\cos x \\left( 2 \\sin{\\frac{\\Delta x}{2} \\cos{\\frac{\\Delta x}{2}}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ -2 \\sin x  \\sin^2{\\frac{\\Delta x}{2}} + 2 \\sin{\\frac{\\Delta x}{2}} \\left( \\cos x  \\cos{\\frac{\\Delta x}{2}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{  2 \\sin{\\frac{\\Delta x}{2}} \\left( \\cos x  \\cos{\\frac{\\Delta x}{2}} - \\sin x  \\sin^2{\\frac{\\Delta x}{2}}\\right)\n}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{  2 \\sin{\\frac{\\Delta x}{2}} \\cos\\left( x + \\frac{\\Delta x}{2} \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\sin{\\frac{\\Delta x}{2}} \\cos\\left( x + \\frac{\\Delta x}{2} \\right)}{\\frac{\\Delta x}{2}} \\\\\n&\\because \\lim\\limits_{\\Delta x \\rightarrow0} \\sin \\frac{\\Delta x}{2} =  \\frac{\\Delta x}{2} \\\\\n&\\therefore  \\frac{  \\sin{\\frac{\\Delta x}{2}} }{\\frac{\\Delta x}{2}} = 1 \\\\\n&\\because  \\lim\\limits_{\\Delta x \\rightarrow0} \\cos\\left( x + \\frac{\\Delta x}{2} \\right) = \\cos \\left( x  + 0\\right) = \\cos x \\\\\n&\\therefore f'(\\sin x) = \\cos x\n\\end{align}</script><h2 id=\"3-Cosinus-Function\"><a href=\"#3-Cosinus-Function\" class=\"headerlink\" title=\"3. Cosinus Function\"></a>3. Cosinus Function</h2><p>Given : $f \\left( x \\right) = \\cos(x)$</p>\n<p>Proofs : $f’\\left( x \\right) = - \\sin(x)$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left(x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos\\left(x + \\Delta x\\right) - \\cos x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left( \\cos x \\cos \\Delta x - \\sin x \\sin \\Delta x\\right) - \\cos x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\left( \\cos x \\cos \\Delta x  - \\cos x \\right) - \\sin x \\sin \\Delta x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos x \\left( \\cos \\Delta x  - 1 \\right) - \\sin x \\sin \\Delta x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\cos x \\left[\\left( 1 - 2 \\sin^2\\frac{\\Delta x}{2} \\right) -1 \\right] - \\sin x \\left( 2 \\sin \\frac{\\Delta x}{2} \\cos\\frac{\\Delta x}{2} \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{2 \\sin^2 \\frac{\\Delta x}{2} \\cos x - 2 \\sin \\frac{\\Delta x}{2} \\left( \\sin x \\cos \\frac{\\Delta x}{2} \\right)}{\\Delta x}\\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 2 \\sin \\frac{\\Delta x}{2} \\left( \\sin \\frac{\\Delta x}{2} \\cos x - \\cos \\frac{\\Delta x}{2} \\sin x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ 2 \\sin \\frac{\\Delta x}{2} \\sin \\left( \\frac{\\Delta x}{2} -x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\left[ \\sin \\left( \\frac{\\Delta x}{2} - x  \\right) \\frac{\\sin\\frac{\\Delta x}{2}}{\\frac{\\Delta x}{2}} \\right] \\\\\n&\\because \\lim\\limits_{\\Delta x \\rightarrow0} \\sin \\frac{\\Delta x}{2} =  \\frac{\\Delta x}{2} \\\\\n&\\therefore  \\frac{  \\sin{\\frac{\\Delta x}{2}} }{\\frac{\\Delta x}{2}} = 1 \\\\\n&\\because  \\lim\\limits_{\\Delta x \\rightarrow0} \\sin\\left( \\frac{\\Delta x}{2} - x \\right) = \\sin \\left( 0 - x \\right) = \\sin (-x)  = - \\sin x\\\\\n&\\therefore f'(\\cos x) = - \\sin x\n\\end{align}</script><h2 id=\"4-Exponential-Function\"><a href=\"#4-Exponential-Function\" class=\"headerlink\" title=\"4. Exponential Function\"></a>4. Exponential Function</h2><p>Given : $f \\left( x \\right) = a ^ x \\text( a&gt;0 \\&amp;  a \\neq 1 )$</p>\n<p>Proofs : $f’\\left( x \\right) = a^x \\ln a$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f\\left( x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ a^{x + \\Delta x} - a^x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ a^x \\left( a^{\\Delta x} -1 \\right)}{\\Delta x} \\\\\nintroduce \\quad\n& t = a^{\\Delta x} - 1, then \\quad a^{\\Delta x} = t + 1, \\quad \\Delta x = \\log_a\\left( t+1 \\right)\\\\\nequation  \\quad\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x t}{\\log_a(t+1)}\\\\\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\frac{1}{t} \\log_a(t+1)}\\\\\n&= \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\log_a(t+1)^{\\frac{1}{t} }}\\\\\n&\\because  \\lim\\limits_{t \\rightarrow0} (t+1)^{\\frac{1}{t}} = e \\\\\n&\\therefore  \\lim\\limits_{t \\rightarrow0} \\frac{ a^x}{\\log_a\\left( t+1 \\right)^{\\frac{1}{t} }} = \\frac{ a^x}{\\log_ae}  = a^x \\cdot \\frac{ \\ln a }{\\ln e} = a^x \\ln a\\\\\n&\\therefore f'(a ^ x ) = - a^x \\ln a\n\\end{align}</script><h2 id=\"5-Logarithmic-function\"><a href=\"#5-Logarithmic-function\" class=\"headerlink\" title=\"5. Logarithmic function\"></a>5. Logarithmic function</h2><p>Given : $f \\left( x \\right) = \\log_a x \\text( x&gt;0 \\&amp;  a \\neq 1 )$</p>\n<p>Proofs : $f’\\left( x \\right) = \\frac{1}{x\\ln a}$</p>\n<blockquote>\n<p>Deduction</p>\n</blockquote>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\nf'\\left( x \\right)\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ f \\left(x + \\Delta x \\right) - f \\left( x \\right)}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\log_a \\left(x + \\Delta x \\right) - \\log_a x}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0} \\frac{ \\log_a(\\frac{x + \\Delta x}{x})}{\\Delta x} \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x}\\frac{x }{\\Delta x} \\log_a \\left(\\frac{x + \\Delta x}{x}\\right) \\\\\n&= \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ \\frac{  \\Delta x}{x} \\right)^{\\frac{x }{\\Delta x}} \\\\\nintroduce \\quad\n& t = \\frac{  \\Delta x}{x}\\\\\nequation  \\quad\n&=\\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ t \\right)^{\\frac{1 }{t}} \\\\\n&\\because  \\lim\\limits_{t \\rightarrow0} \\left( t+1 \\right)^{\\frac{1}{t}} = e \\\\\n&\\therefore  \\lim\\limits_{\\Delta x \\rightarrow0}  \\frac{1}{x} \\log_a \\left( 1+ t \\right)^{\\frac{1 }{t}} = \\frac{1}{x} \\frac{\\ln e}{\\ln a} = \\frac{1}{ x \\ln a} \\\\\n&\\therefore f'\\left(\\log_a x\\right) = \\frac{1}{x\\ln a}\n\\end{align}</script><p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"How to use MathJax in Markdown","date":"2017-11-30T03:23:19.000Z","mathjax":false,"copyright":true,"top":100,"_content":"\n# How to use MathJax in Markdown\n\n\n## When to Use MathJax?\n When using markdown to write blog, especially using Github Page to do it. You may have trouble to display formula. There are several ways[1] to do that. But the simplest way is to use MathJax.\n\n## How to Use MathJax?\nAdd the code below to your markdown file, and that's it!\n\n```\n<script type=\"text/javascript\" async src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\"> </script>\n```\n\n\nFor example, add the test code to your blog.md file\n```\n<script type=\"text/javascript\" async src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\"> </script>\n\ntest: $$n==x$$\n```\n\n\n# Reference\n[1] [Markdown中插入数学公式的方法](http://blog.csdn.net/xiahouzuoxin/article/details/26478179)\n\n[2] [MathJax Chinese Doc 2.0 documentation](http://mathjax-chinese-doc.readthedocs.io/en/latest/start.html)\n\n[3] [MathJax basic tutorial and quick reference](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)\n\n[4] [docs.mathjax.org](http://docs.mathjax.org/en/latest/configuration.html#loading)\n\n<br>\n<br>\n---------------------------------------","source":"_posts/Others/Markdown/2017-11-30-How to use MathJax in Markdown.md","raw":"---\ntitle: How to use MathJax in Markdown\ndate: 2017-11-30 11:23:19\ncategories: [Markdown]\ntags: [Markdown, Mathjax]\nmathjax: false\ncopyright: true\ntop: 100\n---\n\n# How to use MathJax in Markdown\n\n\n## When to Use MathJax?\n When using markdown to write blog, especially using Github Page to do it. You may have trouble to display formula. There are several ways[1] to do that. But the simplest way is to use MathJax.\n\n## How to Use MathJax?\nAdd the code below to your markdown file, and that's it!\n\n```\n<script type=\"text/javascript\" async src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\"> </script>\n```\n\n\nFor example, add the test code to your blog.md file\n```\n<script type=\"text/javascript\" async src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\"> </script>\n\ntest: $$n==x$$\n```\n\n\n# Reference\n[1] [Markdown中插入数学公式的方法](http://blog.csdn.net/xiahouzuoxin/article/details/26478179)\n\n[2] [MathJax Chinese Doc 2.0 documentation](http://mathjax-chinese-doc.readthedocs.io/en/latest/start.html)\n\n[3] [MathJax basic tutorial and quick reference](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)\n\n[4] [docs.mathjax.org](http://docs.mathjax.org/en/latest/configuration.html#loading)\n\n<br>\n<br>\n---------------------------------------","slug":"Others/Markdown/2017-11-30-How to use MathJax in Markdown","published":1,"updated":"2018-04-14T19:42:06.500Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hs003srwtj9skq1y5z","content":"<h1 id=\"How-to-use-MathJax-in-Markdown\"><a href=\"#How-to-use-MathJax-in-Markdown\" class=\"headerlink\" title=\"How to use MathJax in Markdown\"></a>How to use MathJax in Markdown</h1><h2 id=\"When-to-Use-MathJax\"><a href=\"#When-to-Use-MathJax\" class=\"headerlink\" title=\"When to Use MathJax?\"></a>When to Use MathJax?</h2><p> When using markdown to write blog, especially using Github Page to do it. You may have trouble to display formula. There are several ways[1] to do that. But the simplest way is to use MathJax.</p>\n<h2 id=\"How-to-Use-MathJax\"><a href=\"#How-to-Use-MathJax\" class=\"headerlink\" title=\"How to Use MathJax?\"></a>How to Use MathJax?</h2><p>Add the code below to your markdown file, and that’s it!</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt; &lt;/script&gt;</div></pre></td></tr></table></figure>\n<p>For example, add the test code to your blog.md file<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt; &lt;/script&gt;</div><div class=\"line\"></div><div class=\"line\">test: $$n==x$$</div></pre></td></tr></table></figure></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] <a href=\"http://blog.csdn.net/xiahouzuoxin/article/details/26478179\" target=\"_blank\" rel=\"external\">Markdown中插入数学公式的方法</a></p>\n<p>[2] <a href=\"http://mathjax-chinese-doc.readthedocs.io/en/latest/start.html\" target=\"_blank\" rel=\"external\">MathJax Chinese Doc 2.0 documentation</a></p>\n<p>[3] <a href=\"https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference\" target=\"_blank\" rel=\"external\">MathJax basic tutorial and quick reference</a></p>\n<p>[4] <a href=\"http://docs.mathjax.org/en/latest/configuration.html#loading\" target=\"_blank\" rel=\"external\">docs.mathjax.org</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-to-use-MathJax-in-Markdown\"><a href=\"#How-to-use-MathJax-in-Markdown\" class=\"headerlink\" title=\"How to use MathJax in Markdown\"></a>How to use MathJax in Markdown</h1><h2 id=\"When-to-Use-MathJax\"><a href=\"#When-to-Use-MathJax\" class=\"headerlink\" title=\"When to Use MathJax?\"></a>When to Use MathJax?</h2><p> When using markdown to write blog, especially using Github Page to do it. You may have trouble to display formula. There are several ways[1] to do that. But the simplest way is to use MathJax.</p>\n<h2 id=\"How-to-Use-MathJax\"><a href=\"#How-to-Use-MathJax\" class=\"headerlink\" title=\"How to Use MathJax?\"></a>How to Use MathJax?</h2><p>Add the code below to your markdown file, and that’s it!</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt; &lt;/script&gt;</div></pre></td></tr></table></figure>\n<p>For example, add the test code to your blog.md file<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt; &lt;/script&gt;</div><div class=\"line\"></div><div class=\"line\">test: $$n==x$$</div></pre></td></tr></table></figure></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><p>[1] <a href=\"http://blog.csdn.net/xiahouzuoxin/article/details/26478179\" target=\"_blank\" rel=\"external\">Markdown中插入数学公式的方法</a></p>\n<p>[2] <a href=\"http://mathjax-chinese-doc.readthedocs.io/en/latest/start.html\" target=\"_blank\" rel=\"external\">MathJax Chinese Doc 2.0 documentation</a></p>\n<p>[3] <a href=\"https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference\" target=\"_blank\" rel=\"external\">MathJax basic tutorial and quick reference</a></p>\n<p>[4] <a href=\"http://docs.mathjax.org/en/latest/configuration.html#loading\" target=\"_blank\" rel=\"external\">docs.mathjax.org</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"Tylor Expansion Example","date":"2017-11-04T07:23:19.000Z","mathjax":true,"copyright":true,"top":100,"_content":"\n# Tylor Expansion Example\n\n> Tylor Expansion is a powerful tool to deal with limits. Some examples are showed below.\n\n## Prerequisite\n\nSee more about how to calculate derivative at this link and differential rules at this link.\n\n## Definition[1]\n\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}\n$$\n\n\nWhen a = 0, the formula is showed below.\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n\n\\end{align}\n$$\n\n\n## Examples\n### 1. Example 1\n\n$f(x) = e^x$\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow 0} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\frac{e^0}{ 0 !}x^0 + \\frac{e^0}{ 1 !}x^1 + \\frac{e^0}{ 2 !}x^2  + \\cdots + \\frac{e^0}{ n !}x^n \\\\\n&= \\frac{x^0}{ 0 !} + \\frac{x^1}{ 1 !} + \\frac{x^2}{ 2 !}  + \\cdots + \\frac{x^n}{ n !} \\\\\n\\end{align}\n$$\n\n\n### 1. Example 2\n\n$f(x) = \\sin x$\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow 0} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\frac{\\sin 0}{ 0 !}x^0 + \\frac{\\cos 0}{ 1 !}x^1 + \\frac{-\\sin 0}{ 2 !}x^2  + \\cdots\\\\\n&= \\frac{0}{ 0 !} + \\frac{1}{ 1 !}x^1 + \\frac{0}{ 2 !}  + \\cdots + \\frac{x^n}{ n !}  + \\cdots\\\\\n&= \\frac{x^1}{ 1 !} - \\frac{x^3}{ 3 !} + \\frac{x^5}{ 5 !} - \\frac{x^7}{ 7 !} + \\cdots \\\\\n\\end{align}\n$$\n\n\n\n## Reference\n[1] [wikipedia-Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n\n\n<br>\n<br>\n------------------------------------------","source":"_posts/Mathematics/Calculus/2017-11-04-Typlor Expansion Example.md","raw":"---\ntitle: Tylor Expansion Example\ndate: 2017-11-04 15:23:19\ncategories: [Machine-Learning-Mathematics]\ntags: [Machine-Learning, Mathematics, Calculus]\nmathjax: true\ncopyright: true\ntop: 100\n---\n\n# Tylor Expansion Example\n\n> Tylor Expansion is a powerful tool to deal with limits. Some examples are showed below.\n\n## Prerequisite\n\nSee more about how to calculate derivative at this link and differential rules at this link.\n\n## Definition[1]\n\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}\n$$\n\n\nWhen a = 0, the formula is showed below.\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n\n\\end{align}\n$$\n\n\n## Examples\n### 1. Example 1\n\n$f(x) = e^x$\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow 0} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\frac{e^0}{ 0 !}x^0 + \\frac{e^0}{ 1 !}x^1 + \\frac{e^0}{ 2 !}x^2  + \\cdots + \\frac{e^0}{ n !}x^n \\\\\n&= \\frac{x^0}{ 0 !} + \\frac{x^1}{ 1 !} + \\frac{x^2}{ 2 !}  + \\cdots + \\frac{x^n}{ n !} \\\\\n\\end{align}\n$$\n\n\n### 1. Example 2\n\n$f(x) = \\sin x$\n$$\n\\begin{align}\n\\lim\\limits_{x \\rightarrow 0} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\frac{\\sin 0}{ 0 !}x^0 + \\frac{\\cos 0}{ 1 !}x^1 + \\frac{-\\sin 0}{ 2 !}x^2  + \\cdots\\\\\n&= \\frac{0}{ 0 !} + \\frac{1}{ 1 !}x^1 + \\frac{0}{ 2 !}  + \\cdots + \\frac{x^n}{ n !}  + \\cdots\\\\\n&= \\frac{x^1}{ 1 !} - \\frac{x^3}{ 3 !} + \\frac{x^5}{ 5 !} - \\frac{x^7}{ 7 !} + \\cdots \\\\\n\\end{align}\n$$\n\n\n\n## Reference\n[1] [wikipedia-Taylor series](https://en.wikipedia.org/wiki/Taylor_series)\n\n\n<br>\n<br>\n------------------------------------------","slug":"Mathematics/Calculus/2017-11-04-Typlor Expansion Example","published":1,"updated":"2018-04-14T19:42:06.494Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2ht003vrwtjvgqz96p3","content":"<h1 id=\"Tylor-Expansion-Example\"><a href=\"#Tylor-Expansion-Example\" class=\"headerlink\" title=\"Tylor Expansion Example\"></a>Tylor Expansion Example</h1><blockquote>\n<p>Tylor Expansion is a powerful tool to deal with limits. Some examples are showed below.</p>\n</blockquote>\n<h2 id=\"Prerequisite\"><a href=\"#Prerequisite\" class=\"headerlink\" title=\"Prerequisite\"></a>Prerequisite</h2><p>See more about how to calculate derivative at this link and differential rules at this link.</p>\n<h2 id=\"Definition-1\"><a href=\"#Definition-1\" class=\"headerlink\" title=\"Definition[1]\"></a>Definition[1]</h2><script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}</script><p>When a = 0, the formula is showed below.</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n\n\\end{align}</script><h2 id=\"Examples\"><a href=\"#Examples\" class=\"headerlink\" title=\"Examples\"></a>Examples</h2><h3 id=\"1-Example-1\"><a href=\"#1-Example-1\" class=\"headerlink\" title=\"1. Example 1\"></a>1. Example 1</h3><p>$f(x) = e^x$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow 0} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\frac{e^0}{ 0 !}x^0 + \\frac{e^0}{ 1 !}x^1 + \\frac{e^0}{ 2 !}x^2  + \\cdots + \\frac{e^0}{ n !}x^n \\\\\n&= \\frac{x^0}{ 0 !} + \\frac{x^1}{ 1 !} + \\frac{x^2}{ 2 !}  + \\cdots + \\frac{x^n}{ n !} \\\\\n\\end{align}</script><h3 id=\"1-Example-2\"><a href=\"#1-Example-2\" class=\"headerlink\" title=\"1. Example 2\"></a>1. Example 2</h3><p>$f(x) = \\sin x$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow 0} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\frac{\\sin 0}{ 0 !}x^0 + \\frac{\\cos 0}{ 1 !}x^1 + \\frac{-\\sin 0}{ 2 !}x^2  + \\cdots\\\\\n&= \\frac{0}{ 0 !} + \\frac{1}{ 1 !}x^1 + \\frac{0}{ 2 !}  + \\cdots + \\frac{x^n}{ n !}  + \\cdots\\\\\n&= \\frac{x^1}{ 1 !} - \\frac{x^3}{ 3 !} + \\frac{x^5}{ 5 !} - \\frac{x^7}{ 7 !} + \\cdots \\\\\n\\end{align}</script><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] <a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"external\">wikipedia-Taylor series</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Tylor-Expansion-Example\"><a href=\"#Tylor-Expansion-Example\" class=\"headerlink\" title=\"Tylor Expansion Example\"></a>Tylor Expansion Example</h1><blockquote>\n<p>Tylor Expansion is a powerful tool to deal with limits. Some examples are showed below.</p>\n</blockquote>\n<h2 id=\"Prerequisite\"><a href=\"#Prerequisite\" class=\"headerlink\" title=\"Prerequisite\"></a>Prerequisite</h2><p>See more about how to calculate derivative at this link and differential rules at this link.</p>\n<h2 id=\"Definition-1\"><a href=\"#Definition-1\" class=\"headerlink\" title=\"Definition[1]\"></a>Definition[1]</h2><script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}\\left( x - a \\right) ^ 0 + \\frac{f'\\left( a \\right)}{ 1 !}\\left( x - a \\right) ^ 1 + \\frac{f''\\left( a \\right)}{ 2 !}\\left( x - a \\right) ^ 2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}\\left( x - a \\right) ^ n\n\\end{align}</script><p>When a = 0, the formula is showed below.</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow a} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\sum\\limits_{n=0}^{\\infty} \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n\n\\end{align}</script><h2 id=\"Examples\"><a href=\"#Examples\" class=\"headerlink\" title=\"Examples\"></a>Examples</h2><h3 id=\"1-Example-1\"><a href=\"#1-Example-1\" class=\"headerlink\" title=\"1. Example 1\"></a>1. Example 1</h3><p>$f(x) = e^x$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow 0} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\frac{e^0}{ 0 !}x^0 + \\frac{e^0}{ 1 !}x^1 + \\frac{e^0}{ 2 !}x^2  + \\cdots + \\frac{e^0}{ n !}x^n \\\\\n&= \\frac{x^0}{ 0 !} + \\frac{x^1}{ 1 !} + \\frac{x^2}{ 2 !}  + \\cdots + \\frac{x^n}{ n !} \\\\\n\\end{align}</script><h3 id=\"1-Example-2\"><a href=\"#1-Example-2\" class=\"headerlink\" title=\"1. Example 2\"></a>1. Example 2</h3><p>$f(x) = \\sin x$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n\\lim\\limits_{x \\rightarrow 0} f(x)\n&= \\frac{f\\left( a \\right)}{ 0 !}x^0 + \\frac{f'\\left( a \\right)}{ 1 !}x^1 + \\frac{f''\\left( a \\right)}{ 2 !}x^2  + \\cdots + \\frac{f^{(n)}\\left( a \\right)}{ n !}x^n \\\\\n&= \\frac{\\sin 0}{ 0 !}x^0 + \\frac{\\cos 0}{ 1 !}x^1 + \\frac{-\\sin 0}{ 2 !}x^2  + \\cdots\\\\\n&= \\frac{0}{ 0 !} + \\frac{1}{ 1 !}x^1 + \\frac{0}{ 2 !}  + \\cdots + \\frac{x^n}{ n !}  + \\cdots\\\\\n&= \\frac{x^1}{ 1 !} - \\frac{x^3}{ 3 !} + \\frac{x^5}{ 5 !} - \\frac{x^7}{ 7 !} + \\cdots \\\\\n\\end{align}</script><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] <a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\" rel=\"external\">wikipedia-Taylor series</a></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"},{"title":"How to Render the Hyperlink with Braces","date":"2017-12-03T03:23:19.000Z","mathjax":false,"copyright":true,"top":100,"_content":"\n# How to Render the Hyperlink with Braces\n\n## Here is the code\n\n```\n![Reorganized Dichotomies of B(4,3) - 1][11]\n\n[11]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\n```\n\n\n## Here is the result\n\n![Reorganized Dichotomies of B(4,3) - 1][11]\n\n[11]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\n\n\n<br>\n<br>\n---------------------------------------","source":"_posts/Others/Markdown/2017-12-03-How to Render the Hyperlink with Braces.md","raw":"---\ntitle: How to Render the Hyperlink with Braces\ndate: 2017-12-03 11:23:19\ncategories: [Markdown]\ntags: [Markdown]\nmathjax: false\ncopyright: true\ntop: 100\n---\n\n# How to Render the Hyperlink with Braces\n\n## Here is the code\n\n```\n![Reorganized Dichotomies of B(4,3) - 1][11]\n\n[11]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\n```\n\n\n## Here is the result\n\n![Reorganized Dichotomies of B(4,3) - 1][11]\n\n[11]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\n\n\n<br>\n<br>\n---------------------------------------","slug":"Others/Markdown/2017-12-03-How to Render the Hyperlink with Braces","published":1,"updated":"2018-04-14T19:42:06.500Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjfzzt2hw0040rwtjk5lfaoor","content":"<h1 id=\"How-to-Render-the-Hyperlink-with-Braces\"><a href=\"#How-to-Render-the-Hyperlink-with-Braces\" class=\"headerlink\" title=\"How to Render the Hyperlink with Braces\"></a>How to Render the Hyperlink with Braces</h1><h2 id=\"Here-is-the-code\"><a href=\"#Here-is-the-code\" class=\"headerlink\" title=\"Here is the code\"></a>Here is the code</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">![Reorganized Dichotomies of B(4,3) - 1][11]</div><div class=\"line\"></div><div class=\"line\">[11]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png</div></pre></td></tr></table></figure>\n<h2 id=\"Here-is-the-result\"><a href=\"#Here-is-the-result\" class=\"headerlink\" title=\"Here is the result\"></a>Here is the result</h2><p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\" alt=\"Reorganized Dichotomies of B(4,3) - 1\"></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"How-to-Render-the-Hyperlink-with-Braces\"><a href=\"#How-to-Render-the-Hyperlink-with-Braces\" class=\"headerlink\" title=\"How to Render the Hyperlink with Braces\"></a>How to Render the Hyperlink with Braces</h1><h2 id=\"Here-is-the-code\"><a href=\"#Here-is-the-code\" class=\"headerlink\" title=\"Here is the code\"></a>Here is the code</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">![Reorganized Dichotomies of B(4,3) - 1][11]</div><div class=\"line\"></div><div class=\"line\">[11]:https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png</div></pre></td></tr></table></figure>\n<h2 id=\"Here-is-the-result\"><a href=\"#Here-is-the-result\" class=\"headerlink\" title=\"Here is the result\"></a>Here is the result</h2><p><img src=\"https://raw.githubusercontent.com/zhichengML/MarkdownPhoto/master/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter5-11%20B(4%2C3)%20-%201.png\" alt=\"Reorganized Dichotomies of B(4,3) - 1\"></p>\n<p><br></p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><br></h2>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjfzzt29x0000rwtjth7kqrys","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2aa0008rwtjgrlcqzq9"},{"post_id":"cjfzzt2da0009rwtjod1qx9sb","category_id":"cjfzzt2dc000brwtjkv2l5qyb","_id":"cjfzzt2di000lrwtjaq2s40lk"},{"post_id":"cjfzzt2dh000jrwtj6ajvr2if","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2dm000qrwtj0yksbvvx"},{"post_id":"cjfzzt2db000arwtjjeaj2bzy","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2dp000urwtjuqzutl5a"},{"post_id":"cjfzzt2di000krwtjgc8uf215","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2dr000xrwtjcfskrmoa"},{"post_id":"cjfzzt2dk000orwtjp508shor","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2dv0012rwtjvtgozd6p"},{"post_id":"cjfzzt2dd000drwtjrredpr2a","category_id":"cjfzzt2dj000mrwtjo6mq3qwi","_id":"cjfzzt2dw0015rwtj8hqmmzos"},{"post_id":"cjfzzt2dl000prwtj7qzom3mz","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2dy0019rwtjzggcn363"},{"post_id":"cjfzzt2do000trwtjqnz3ah9z","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2e8001crwtjoszey4xk"},{"post_id":"cjfzzt2de000erwtj1whgwb7c","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2ea001grwtjdb9qhvfu"},{"post_id":"cjfzzt2dr000wrwtjvhch2302","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2ec001jrwtjot9o7z30"},{"post_id":"cjfzzt2du0011rwtjdk5k7l5i","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2ef001nrwtj9z5glgma"},{"post_id":"cjfzzt2df000frwtjblolojod","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2ei001qrwtj9tbvm8v8"},{"post_id":"cjfzzt2dw0014rwtjcycemgri","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2ek001urwtjc888zj8u"},{"post_id":"cjfzzt2dx0018rwtj9to7o9rc","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2el001xrwtjvh48h6aq"},{"post_id":"cjfzzt2e7001brwtj3kbs3k5c","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2en0021rwtjtdki0nmb"},{"post_id":"cjfzzt2e9001frwtj5s2h83ss","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2eq0024rwtjt9gmexv9"},{"post_id":"cjfzzt2eb001irwtjrrjl2cce","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2er0028rwtjktshpsth"},{"post_id":"cjfzzt2ed001mrwtj4w9464e5","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2et002brwtj0qo2m0iy"},{"post_id":"cjfzzt2eh001prwtjxsaycyxb","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2eu002drwtjbtsj339r"},{"post_id":"cjfzzt2ej001trwtj4a0yclsg","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2eu002frwtjkzywi5mj"},{"post_id":"cjfzzt2el001wrwtj2j3d0n3y","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2ev002hrwtj4fmnb5gp"},{"post_id":"cjfzzt2en0020rwtjs8k824j4","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2ew002jrwtj8f09vrsg"},{"post_id":"cjfzzt2ep0023rwtj18g0l1d0","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2ew002lrwtjv7qnb8ip"},{"post_id":"cjfzzt2er0027rwtj7b6hskyz","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2ex002nrwtjnixnligr"},{"post_id":"cjfzzt2es002arwtjm0rqsidn","category_id":"cjfzzt2a60003rwtjjwkvte4w","_id":"cjfzzt2ex002prwtjefa96eb3"},{"post_id":"cjfzzt2hd0033rwtjga4zrzqz","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2hj003brwtjy6z27kc2"},{"post_id":"cjfzzt2h8002yrwtj3fwdk81i","category_id":"cjfzzt2hb0030rwtj9vzmnilq","_id":"cjfzzt2hl003grwtjzw96jvir"},{"post_id":"cjfzzt2he0034rwtjxbuihxi9","category_id":"cjfzzt2hb0030rwtj9vzmnilq","_id":"cjfzzt2hn003jrwtjvw3422gg"},{"post_id":"cjfzzt2hh0038rwtjz3v4kbgt","category_id":"cjfzzt2hb0030rwtj9vzmnilq","_id":"cjfzzt2hp003orwtj7wud1ntf"},{"post_id":"cjfzzt2ha002zrwtjo3tlfuiz","category_id":"cjfzzt2hb0030rwtj9vzmnilq","_id":"cjfzzt2hr003qrwtjx1rtss9f"},{"post_id":"cjfzzt2hl003frwtj56exjaiq","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2hs003trwtju3doj0lt"},{"post_id":"cjfzzt2hb0032rwtj19qfkzf0","category_id":"cjfzzt2hb0030rwtj9vzmnilq","_id":"cjfzzt2hu003wrwtjxn1rpcr7"},{"post_id":"cjfzzt2hm003irwtjjwa2rku6","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2hx0041rwtjjaghsk6s"},{"post_id":"cjfzzt2hp003nrwtj2w6t44x6","category_id":"cjfzzt2hn003krwtjl7puyvj0","_id":"cjfzzt2hz0043rwtjdg9n0qr2"},{"post_id":"cjfzzt2hi003arwtj7q3dxkam","category_id":"cjfzzt2hn003krwtjl7puyvj0","_id":"cjfzzt2i00047rwtjagcki4rq"},{"post_id":"cjfzzt2hq003prwtj98zbd0x9","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2i10049rwtjluvbyuoo"},{"post_id":"cjfzzt2ht003vrwtjvgqz96p3","category_id":"cjfzzt2dg000grwtjqs6k22x0","_id":"cjfzzt2i2004crwtjwry9l4cj"},{"post_id":"cjfzzt2hs003srwtj9skq1y5z","category_id":"cjfzzt2hv003zrwtjt9w6jq0c","_id":"cjfzzt2i2004erwtjdxf00w8s"},{"post_id":"cjfzzt2hw0040rwtjk5lfaoor","category_id":"cjfzzt2hv003zrwtjt9w6jq0c","_id":"cjfzzt2i3004hrwtj1rgg70ji"}],"PostTag":[{"post_id":"cjfzzt29x0000rwtjth7kqrys","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2a90007rwtjnunzd0n0"},{"post_id":"cjfzzt2da0009rwtjod1qx9sb","tag_id":"cjfzzt2dd000crwtjfbu162br","_id":"cjfzzt2dh000irwtjgnv9pg1t"},{"post_id":"cjfzzt2db000arwtjjeaj2bzy","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2dq000vrwtjar8wh7xo"},{"post_id":"cjfzzt2db000arwtjjeaj2bzy","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2ds000yrwtj4cjavh1a"},{"post_id":"cjfzzt2dl000prwtj7qzom3mz","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2dv0013rwtjihihk1x5"},{"post_id":"cjfzzt2dl000prwtj7qzom3mz","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2dw0016rwtjox96mho4"},{"post_id":"cjfzzt2do000trwtjqnz3ah9z","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2e7001arwtj3m09jnu1"},{"post_id":"cjfzzt2dd000drwtjrredpr2a","tag_id":"cjfzzt2dm000srwtj1tt9g80u","_id":"cjfzzt2e9001drwtj0y2ickay"},{"post_id":"cjfzzt2dr000wrwtjvhch2302","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2eb001hrwtj0qf9grqx"},{"post_id":"cjfzzt2du0011rwtjdk5k7l5i","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ec001krwtjyw8z36yg"},{"post_id":"cjfzzt2dw0014rwtjcycemgri","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2eg001orwtjq7e5xya1"},{"post_id":"cjfzzt2dx0018rwtj9to7o9rc","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ei001rrwtjc2alj3xa"},{"post_id":"cjfzzt2de000erwtj1whgwb7c","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2ek001vrwtjun6ba83y"},{"post_id":"cjfzzt2de000erwtj1whgwb7c","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2em001yrwtjmpakwtra"},{"post_id":"cjfzzt2e7001brwtj3kbs3k5c","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2eo0022rwtjfmk9sojs"},{"post_id":"cjfzzt2e9001frwtj5s2h83ss","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2eq0025rwtjxsuo6ykt"},{"post_id":"cjfzzt2eb001irwtjrrjl2cce","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2es0029rwtjqumqsc3s"},{"post_id":"cjfzzt2ed001mrwtj4w9464e5","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2et002crwtjo8215mim"},{"post_id":"cjfzzt2df000frwtjblolojod","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2eu002erwtjvy91ezcx"},{"post_id":"cjfzzt2df000frwtjblolojod","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2eu002grwtjut87cmxz"},{"post_id":"cjfzzt2eh001prwtjxsaycyxb","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ew002irwtjcm7xxuux"},{"post_id":"cjfzzt2ej001trwtj4a0yclsg","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ew002krwtjfhixqnh3"},{"post_id":"cjfzzt2dh000jrwtj6ajvr2if","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2ew002mrwtjdhe0po4h"},{"post_id":"cjfzzt2dh000jrwtj6ajvr2if","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2ex002orwtjry93hgc0"},{"post_id":"cjfzzt2el001wrwtj2j3d0n3y","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ex002qrwtj968y5v2j"},{"post_id":"cjfzzt2en0020rwtjs8k824j4","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ex002rrwtj2twp26db"},{"post_id":"cjfzzt2di000krwtjgc8uf215","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2ey002srwtj4orz8xdv"},{"post_id":"cjfzzt2di000krwtjgc8uf215","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2ey002trwtjnjh6yaq0"},{"post_id":"cjfzzt2ep0023rwtj18g0l1d0","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ey002urwtjnpfyv9lo"},{"post_id":"cjfzzt2er0027rwtj7b6hskyz","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ey002vrwtj9rezg8mg"},{"post_id":"cjfzzt2es002arwtjm0rqsidn","tag_id":"cjfzzt2a70004rwtjawm6h8wd","_id":"cjfzzt2ey002wrwtjqfiyj7kp"},{"post_id":"cjfzzt2dk000orwtjp508shor","tag_id":"cjfzzt2eq0026rwtjaqrczqk9","_id":"cjfzzt2ey002xrwtjqi4sbfzh"},{"post_id":"cjfzzt2h8002yrwtj3fwdk81i","tag_id":"cjfzzt2hb0031rwtjbmygr49x","_id":"cjfzzt2hg0037rwtja68pwdwo"},{"post_id":"cjfzzt2he0034rwtjxbuihxi9","tag_id":"cjfzzt2hb0031rwtjbmygr49x","_id":"cjfzzt2hi0039rwtjqrpo644v"},{"post_id":"cjfzzt2hh0038rwtjz3v4kbgt","tag_id":"cjfzzt2hb0031rwtjbmygr49x","_id":"cjfzzt2hk003erwtjj68jdz6z"},{"post_id":"cjfzzt2ha002zrwtjo3tlfuiz","tag_id":"cjfzzt2hb0031rwtjbmygr49x","_id":"cjfzzt2hm003hrwtjfchto9j7"},{"post_id":"cjfzzt2hb0032rwtj19qfkzf0","tag_id":"cjfzzt2hb0031rwtjbmygr49x","_id":"cjfzzt2ho003mrwtjehz5zcis"},{"post_id":"cjfzzt2hd0033rwtjga4zrzqz","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2ht003urwtjzmjxtfan"},{"post_id":"cjfzzt2hd0033rwtjga4zrzqz","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2hu003xrwtjwv7g71zd"},{"post_id":"cjfzzt2hd0033rwtjga4zrzqz","tag_id":"cjfzzt2ho003lrwtjulvxa3mr","_id":"cjfzzt2hy0042rwtj3azldeeh"},{"post_id":"cjfzzt2hq003prwtj98zbd0x9","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2hz0044rwtj2ayvu646"},{"post_id":"cjfzzt2hq003prwtj98zbd0x9","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2i00048rwtjcaiap3rq"},{"post_id":"cjfzzt2hq003prwtj98zbd0x9","tag_id":"cjfzzt2ho003lrwtjulvxa3mr","_id":"cjfzzt2i1004arwtjalx5tjhs"},{"post_id":"cjfzzt2hi003arwtj7q3dxkam","tag_id":"cjfzzt2hr003rrwtjp4nxjwww","_id":"cjfzzt2i2004drwtj5xwa5txp"},{"post_id":"cjfzzt2ht003vrwtjvgqz96p3","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2i2004frwtjaor6nvd0"},{"post_id":"cjfzzt2ht003vrwtjvgqz96p3","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2i3004irwtjneyoh1cu"},{"post_id":"cjfzzt2ht003vrwtjvgqz96p3","tag_id":"cjfzzt2ho003lrwtjulvxa3mr","_id":"cjfzzt2i3004jrwtjkgvomz5t"},{"post_id":"cjfzzt2hl003frwtj56exjaiq","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2i4004lrwtjbshrw07o"},{"post_id":"cjfzzt2hl003frwtj56exjaiq","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2i4004mrwtj2zvjci9g"},{"post_id":"cjfzzt2hl003frwtj56exjaiq","tag_id":"cjfzzt2ho003lrwtjulvxa3mr","_id":"cjfzzt2i4004orwtjre9c7jf2"},{"post_id":"cjfzzt2hm003irwtjjwa2rku6","tag_id":"cjfzzt2dg000hrwtj6o52ia3t","_id":"cjfzzt2i5004prwtjg0tyiugp"},{"post_id":"cjfzzt2hm003irwtjjwa2rku6","tag_id":"cjfzzt2dj000nrwtj51bul55h","_id":"cjfzzt2i5004qrwtj5kevv94j"},{"post_id":"cjfzzt2hm003irwtjjwa2rku6","tag_id":"cjfzzt2ho003lrwtjulvxa3mr","_id":"cjfzzt2i5004rrwtjingfaet5"},{"post_id":"cjfzzt2hp003nrwtj2w6t44x6","tag_id":"cjfzzt2hr003rrwtjp4nxjwww","_id":"cjfzzt2i6004srwtjvfmd2t1n"},{"post_id":"cjfzzt2hs003srwtj9skq1y5z","tag_id":"cjfzzt2i3004grwtjecjeo7ie","_id":"cjfzzt2i6004trwtjfecb5wap"},{"post_id":"cjfzzt2hs003srwtj9skq1y5z","tag_id":"cjfzzt2i4004krwtjsvnwsqdi","_id":"cjfzzt2i6004urwtjs0krpggk"},{"post_id":"cjfzzt2hw0040rwtjk5lfaoor","tag_id":"cjfzzt2i3004grwtjecjeo7ie","_id":"cjfzzt2i7004vrwtj96vb6i6m"}],"Tag":[{"name":"ReadNote-Machine-Learning-Foundation","_id":"cjfzzt2a70004rwtjawm6h8wd"},{"name":"Latex","_id":"cjfzzt2dd000crwtjfbu162br"},{"name":"Machine-Learning","_id":"cjfzzt2dg000hrwtj6o52ia3t"},{"name":"Mathematics","_id":"cjfzzt2dj000nrwtj51bul55h"},{"name":"Machine-Learning-System-Tips","_id":"cjfzzt2dm000srwtj1tt9g80u"},{"name":"Machine-Learning-Mathematics","_id":"cjfzzt2eq0026rwtjaqrczqk9"},{"name":"Machine-Learning-Algorithm","_id":"cjfzzt2hb0031rwtjbmygr49x"},{"name":"Calculus","_id":"cjfzzt2ho003lrwtjulvxa3mr"},{"name":"Hexo","_id":"cjfzzt2hr003rrwtjp4nxjwww"},{"name":"Markdown","_id":"cjfzzt2i3004grwtjecjeo7ie"},{"name":"Mathjax","_id":"cjfzzt2i4004krwtjsvnwsqdi"}]}}